# C-1: Neural Network Foundations

1. Neural Networks Fundamentals
    - Basic Structure and Components
    - Forward Propagation Process
    - Activation Functions Overview
    - Neural Network Architectures
    - Input, Hidden, and Output Layers
2. The Perceptron Algorithm
    - Basic Formula and Components
    - Update Rules for Classification
    - Decision Boundaries and Linear Separability
    - Step Function Implementation
    - Limitations of Perceptrons
3. Loss Functions and Error Calculation
    - Log-Loss Error Function
    - Cross-Entropy for Binary Classification
    - Multi-Class Cross-Entropy
    - Mean Squared Error (MSE)
    - Maximum Likelihood Estimation

#### Neural Networks Fundamentals

The study of neural networks begins with understanding their foundational elements and how they work together to process
information. These computational models were originally inspired by the biological neural networks in animal brains,
though they have evolved into sophisticated mathematical systems with properties quite distinct from their biological
counterparts.

##### Basic Structure and Components

Neural networks consist of interconnected processing units called neurons. Each neuron takes one or more inputs,
processes them, and produces an output. These artificial neurons are organized into layers to form a complete network
architecture.

The fundamental components of a neural network include:

1. **Neurons (or nodes)**: The basic computational units that receive inputs, apply a transformation, and produce an
   output.
2. **Weights**: Each connection between neurons has an associated weight that determines the strength of the signal.
   These weights are the primary parameters that the network learns during training.
3. **Bias**: An additional parameter that allows the model to fit the data better by shifting the activation function.
4. **Activation function**: A mathematical function applied to the weighted sum of inputs to introduce non-linearity
   into the network's output.
5. **Layers**: Collections of neurons that process information in parallel. Neural networks typically have:
    - An input layer: Receives the initial data
    - Hidden layer(s): Perform intermediate computations
    - An output layer: Produces the final result
6. **Connections**: The pathways along which information flows between neurons, typically represented as edges in a
   computational graph.

Mathematically, we can represent the computation at a single neuron as:

$$y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)$$

Where:

- $x_i$ are the inputs to the neuron
- $w_i$ are the weights associated with each input
- $b$ is the bias term
- $f$ is the activation function
- $y$ is the output of the neuron

##### Forward Propagation Process

Forward propagation is the process by which input data flows through the network to produce an output. This is the
fundamental computation performed by neural networks both during training and when making predictions.

The forward propagation process follows these steps:

1. **Input Processing**: The input data is fed into the input layer, with each input feature connected to one or more
   neurons in the first hidden layer.
2. **Hidden Layer Computation**: Each neuron in a hidden layer computes a weighted sum of its inputs from the previous
   layer, adds a bias term, and applies an activation function.
3. **Layer-by-Layer Progression**: The outputs from one layer become the inputs to the next layer, with this process
   continuing through all hidden layers.
4. **Output Generation**: The final layer produces the network's output, which could be a continuous value (for
   regression tasks) or a probability distribution (for classification tasks).

For a neural network with $L$ layers, we can represent this mathematically as:

$$a^{(1)} = f^{(1)}(W^{(1)}x + b^{(1)})$$ $$a^{(2)} = f^{(2)}(W^{(2)}a^{(1)} + b^{(2)})$$ $$\vdots$$
$$a^{(L)} = f^{(L)}(W^{(L)}a^{(L-1)} + b^{(L)})$$

Where:

- $a^{(l)}$ represents the activations (outputs) of layer $l$
- $W^{(l)}$ is the weight matrix for layer $l$
- $b^{(l)}$ is the bias vector for layer $l$
- $f^{(l)}$ is the activation function for layer $l$

The final output $a^{(L)}$ represents the network's prediction based on the input $x$.

##### Activation Functions Overview

Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns and
relationships in data. Without activation functions, multiple layers of a neural network would behave like a single
linear layer, severely limiting the network's expressive power.

Common activation functions include:

1. **Sigmoid**: Maps inputs to the range (0,1), historically popular but prone to the vanishing gradient problem.
   $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
2. **Hyperbolic Tangent (tanh)**: Maps inputs to the range (-1,1), addressing some issues with sigmoid but still
   susceptible to vanishing gradients. $$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
3. **Rectified Linear Unit (ReLU)**: Returns the input if positive, otherwise returns zero. ReLU helps mitigate the
   vanishing gradient problem and accelerates convergence. $$\text{ReLU}(x) = \max(0, x)$$
4. **Leaky ReLU**: A variant of ReLU that allows a small gradient when the input is negative, preventing "dying ReLU"
   problems. $$\text{Leaky ReLU}(x) = \max(\alpha x, x) \text{ where } \alpha \text{ is a small constant}$$
5. **Softmax**: Used in the output layer for multi-class classification problems, producing a probability distribution
   across classes. $$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$

The choice of activation function significantly impacts network performance and training dynamics, with different
functions better suited to different tasks and network architectures.

##### Neural Network Architectures

Neural networks come in various architectures, each designed to excel at different types of tasks. The architecture
defines how neurons are arranged and connected, which determines the network's capabilities and limitations.

Major neural network architectures include:

1. **Feedforward Neural Networks (FNN)**: The simplest architecture where information flows in one direction—from input
   to output—without cycles or loops. These are used for basic classification and regression tasks.
2. **Convolutional Neural Networks (CNN)**: Specialized for processing grid-like data such as images. CNNs use
   convolutional layers to automatically detect spatial hierarchies of features.
3. **Recurrent Neural Networks (RNN)**: Designed for sequential data processing, with connections that form cycles,
   allowing the network to maintain an internal state or "memory." These are used for tasks like time series prediction
   or natural language processing.
4. **Long Short-Term Memory (LSTM) Networks**: A sophisticated RNN variant that addresses the vanishing gradient problem
   in standard RNNs, allowing them to learn longer-term dependencies.
5. **Transformer Networks**: Modern architectures that use self-attention mechanisms to process sequential data in
   parallel rather than sequentially, leading to significant advances in natural language processing.
6. **Autoencoders**: Networks designed to learn efficient representations of data by attempting to reconstruct their
   inputs after passing through a bottleneck layer.
7. **Generative Adversarial Networks (GANs)**: A framework involving two neural networks—a generator and a
   discriminator—competing against each other to generate realistic synthetic data.

The complexity and specificity of these architectures continue to evolve as researchers develop novel approaches to
solving increasingly complex problems.

##### Input, Hidden, and Output Layers

A neural network's structure can be understood through its different types of layers, each serving a distinct purpose in
the information processing pipeline.

**Input Layer:**

- Serves as the entry point for data into the network
- The number of neurons typically equals the number of input features
- Performs minimal processing—often just normalization or standardization
- Passes data to the first hidden layer without applying activation functions

**Hidden Layers:**

- Perform the intermediate computations that transform inputs into useful representations
- Extract and combine features at increasing levels of abstraction
- The number and size of hidden layers define the network's capacity and complexity
- Too few hidden units can lead to underfitting; too many can cause overfitting
- Deep networks have multiple hidden layers, enabling them to learn highly complex functions

**Output Layer:**

- Produces the final result of the network
- The structure depends on the task:
    - Regression: Typically one neuron per output variable
    - Binary classification: Often a single neuron with sigmoid activation
    - Multi-class classification: One neuron per class with softmax activation
- The activation function in the output layer is chosen based on the problem type

The design decisions regarding the number, size, and configuration of these layers significantly impact a neural
network's learning capacity and performance. Modern deep learning often involves careful architectural engineering to
balance computational efficiency with model expressiveness.

As neural networks have evolved, the boundaries between these traditional layer categorizations have sometimes blurred,
especially in complex architectures like residual networks (ResNets) where connections can skip across layers, or in
attention-based models where information flows more dynamically between components.

#### The Perceptron Algorithm

The perceptron represents one of the earliest and most fundamental models in machine learning, serving as the foundation
for more complex neural networks. Introduced by Frank Rosenblatt in 1957, this algorithm implements a simple binary
classifier that forms the building block for more sophisticated neural architectures.

##### Basic Formula and Components

At its core, the perceptron is a mathematical model that attempts to separate data points into two categories using a
linear decision boundary. The fundamental components of the perceptron include:

1. **Input Features**: The perceptron takes a set of input features represented as a vector
   $\mathbf{x} = (x_1, x_2, ..., x_n)$.
2. **Weights**: Each input feature is associated with a weight value, forming a weight vector
   $\mathbf{w} = (w_1, w_2, ..., w_n)$. These weights determine the importance of each feature in the classification
   decision.
3. **Bias**: A bias term $b$ (sometimes denoted as $w_0$) shifts the decision boundary away from the origin, allowing
   for more flexible positioning.
4. **Activation Function**: The perceptron uses a step function to convert the weighted sum of inputs into a binary
   output.

The basic formula for the perceptron's prediction (ŷ) is:

$$\hat{y} = \begin{cases} 1 & \text{if } \mathbf{w} \cdot \mathbf{x} + b > 0 \ 0 & \text{otherwise} \end{cases}$$

Alternatively, this can be written as:

$$\hat{y} = \text{step}(\mathbf{w} \cdot \mathbf{x} + b)$$

Where $\mathbf{w} \cdot \mathbf{x}$ represents the dot product of the weight and input feature vectors, and
$\text{step}()$ is the step function that outputs 1 for positive inputs and 0 otherwise.

For simplicity, the bias term can be incorporated into the weight vector by adding a constant input feature $x_0 = 1$
and setting $w_0 = b$. This gives us:

$$\hat{y} = \text{step}(\mathbf{w} \cdot \mathbf{x})$$

Where now $\mathbf{w} = (w_0, w_1, ..., w_n)$ and $\mathbf{x} = (1, x_1, ..., x_n)$.

##### Update Rules for Classification

The learning process for a perceptron involves iteratively updating the weights based on classification errors. This is
done through a simple error-driven update rule.

For a data point with features $(p, q)$ and true label $y$:

1. If the perceptron correctly classifies the point, no update is needed.
2. If the point is classified as positive (1) but is actually negative (0):
    - $w_1 = w_1 - \alpha p$
    - $w_2 = w_2 - \alpha q$
    - $b = b - \alpha$
3. If the point is classified as negative (0) but is actually positive (1):
    - $w_1 = w_1 + \alpha p$
    - $w_2 = w_2 + \alpha q$
    - $b = b + \alpha$

Here, $\alpha$ is the learning rate, a positive constant that controls the size of each update.

The general form of the update rule for the weight vector is:

$$\mathbf{w}*{new} = \mathbf{w}*{old} + \alpha (y - \hat{y}) \mathbf{x}$$

And for the bias:

$$b_{new} = b_{old} + \alpha (y - \hat{y})$$

This update rule has an intuitive interpretation:

- When $y = \hat{y}$ (correct classification), the weights remain unchanged.
- When $y = 1$ and $\hat{y} = 0$ (false negative), weights are increased proportionally to the input features.
- When $y = 0$ and $\hat{y} = 1$ (false positive), weights are decreased proportionally to the input features.

The learning rate $\alpha$ controls how aggressively the weights are updated. A larger learning rate leads to faster
convergence but may cause the algorithm to overshoot the optimal solution, while a smaller learning rate provides more
precise updates but requires more iterations to converge.

##### Decision Boundaries and Linear Separability

A key concept in understanding the perceptron is the notion of a decision boundary, which geometrically separates the
two classes.

In a two-dimensional space, the decision boundary takes the form of a straight line defined by:

$$w_1 x_1 + w_2 x_2 + b = 0$$

For higher-dimensional spaces, this generalizes to a hyperplane:

$$\mathbf{w} \cdot \mathbf{x} + b = 0$$

Points on one side of this hyperplane ($\mathbf{w} \cdot \mathbf{x} + b > 0$) are classified as positive, while points
on the other side ($\mathbf{w} \cdot \mathbf{x} + b < 0$) are classified as negative.

The concept of linear separability is critical to the perceptron's functionality. A dataset is linearly separable if
there exists a hyperplane that can perfectly separate the positive examples from the negative examples. Mathematically,
this means there exists a weight vector $\mathbf{w}$ and bias $b$ such that:

$$\mathbf{w} \cdot \mathbf{x} + b > 0 \text{ for all positive examples}$$
$$\mathbf{w} \cdot \mathbf{x} + b < 0 \text{ for all negative examples}$$

The perceptron convergence theorem, proven by Rosenblatt, states that if a dataset is linearly separable, the perceptron
algorithm will eventually find a separating hyperplane in a finite number of iterations. However, if the data is not
linearly separable, the algorithm may never converge, continually updating weights without reaching a stable solution.

##### Step Function Implementation

The step function (also known as the Heaviside function) is the activation function used in the perceptron. It
transforms the continuous weighted sum of inputs into a binary output.

Mathematically, the step function is defined as:

$$\text{step}(z) = \begin{cases} 1 & \text{if } z > 0 \ 0 & \text{if } z \leq 0 \end{cases}$$

In some implementations, the output values may be {1, -1} instead of {1, 0}, which gives:

$$\text{step}(z) = \begin{cases} 1 & \text{if } z > 0 \ -1 & \text{if } z \leq 0 \end{cases}$$

The implementation of the step function is straightforward, but it introduces a key challenge for training: its
derivative is zero everywhere except at $z = 0$, where it is undefined. This property makes it impossible to use
gradient-based optimization methods directly with the step function, as there is no meaningful gradient to guide the
weight updates.

In practice, the perceptron algorithm circumvents this issue by using the update rule based on the classification error,
rather than attempting to compute gradients of the step function. This rule ensures that weights are adjusted in a
direction that reduces misclassifications, even without explicit gradient information.

##### Limitations of Perceptrons

Despite their historical importance and conceptual simplicity, perceptrons have several significant limitations:

1. **Linear Decision Boundary**: Perceptrons can only learn linear decision boundaries. This means they cannot solve
   problems that require non-linear separation, such as the classic XOR problem (where points at (0,0) and (1,1) belong
   to one class, while points at (0,1) and (1,0) belong to another).
2. **Binary Classification Only**: The basic perceptron is designed for binary classification tasks. Extension to
   multi-class problems requires multiple perceptrons or more advanced architectures.
3. **No Probabilistic Output**: Unlike logistic regression or more advanced neural networks, perceptrons output hard
   classifications (0 or 1) rather than probabilities, limiting their usefulness in probabilistic frameworks.
4. **Convergence Issues**: If the data is not linearly separable, the perceptron algorithm may never converge,
   oscillating between different weight configurations without settling on a stable solution.
5. **Sensitivity to Initial Conditions**: The final solution can depend on the initial weight values and the order in
   which training examples are presented.
6. **Limited Representation Capacity**: Single-layer perceptrons cannot represent complex functions. As shown by Minsky
   and Papert in their 1969 book "Perceptrons," there are many simple functions that single-layer perceptrons cannot
   learn.

These limitations led to the development of more sophisticated neural network architectures, including multi-layer
perceptrons with differentiable activation functions that can approximate any continuous function given enough hidden
units. The introduction of the backpropagation algorithm for training such networks marked a significant advancement
beyond the basic perceptron, enabling the learning of complex, non-linear relationships in data.

Despite these limitations, the perceptron remains a fundamental concept in machine learning, providing insights into the
basic principles of neural computation and serving as a building block for more complex models. Its simple update rule
and geometric interpretation make it an excellent starting point for understanding neural networks and the challenges of
training them.

#### Loss Functions and Error Calculation

Loss functions are fundamental components of neural networks that quantify how well a model performs on given data. They
measure the discrepancy between the model's predictions and the actual target values, providing a signal that guides the
learning process. The choice of loss function significantly influences how a neural network learns and what types of
errors it prioritizes.

##### Log-Loss Error Function

The Log-Loss error function, also known as logarithmic loss or binary cross-entropy, measures the performance of a
classification model whose output is a probability value between 0 and 1. It's essentially a way of measuring how wrong
your predictions are when you're producing probability estimates rather than discrete class labels.

Think of log-loss as a punishment system that severely penalizes confident incorrect predictions while being more
lenient with uncertain predictions. For instance, confidently predicting 0.99 probability for a customer being
fraudulent when they're actually legitimate incurs a much higher penalty than predicting 0.51 probability for the same
scenario.

Mathematically, the log-loss for a single observation is:

$$\text{Log Loss} = -(y \log(p) + (1-y) \log(1-p))$$

Where:

- $y$ is the actual binary outcome (0 or 1)
- $p$ is the predicted probability of the outcome being 1

For a dataset with $N$ observations, the log-loss is the average of individual losses:

$$\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]$$

Key properties of log-loss include:

1. **Always positive**: Log-loss values are always greater than or equal to zero.
2. **Lower is better**: A perfect model has a log-loss of 0.
3. **Unbounded upper value**: As predictions become more wrong, log-loss approaches infinity.
4. **Asymmetric penalty**: Predicting 0 when the true value is 1 gives infinite loss, forcing models to avoid extreme
   certainty.

The log-loss function is differentiable, making it suitable for optimization techniques like gradient descent. Its
gradient provides clear direction for model updates, which is essential for effective learning.

##### Cross-Entropy for Binary Classification

Cross-entropy for binary classification is effectively the same as log-loss, formulated to measure how well a
probability distribution predicts the true distribution of binary outcomes. It stems from information theory, where it
represents the average number of bits needed to identify an event from a set of possibilities if using a coding scheme
optimized for an estimated probability distribution.

For binary classification, cross-entropy is calculated as:

$$\text{Cross-entropy} = -\sum_{i=1}^{m} [y_i \ln(p_i) + (1 - y_i)\ln(1 - p_i)]$$

Where:

- $m$ is the number of training examples
- $y_i$ is the true label (0 or 1)
- $p_i$ is the predicted probability
- $\ln$ is the natural logarithm

This formula can be interpreted as the logarithm of the likelihood function in maximum likelihood estimation. When we
minimize cross-entropy, we're effectively maximizing the likelihood of our observed data given our model's parameters.

The connection between cross-entropy and probability becomes clearer when we view predictions as attempts to approximate
the true distribution of data. Let's examine the components:

1. For a positive example ($y_i = 1$), the term reduces to $-\ln(p_i)$, which becomes smaller as $p_i$ approaches 1.
2. For a negative example ($y_i = 0$), the term becomes $-\ln(1-p_i)$, which is smaller as $p_i$ approaches 0.

The gradient of cross-entropy with respect to the model's logits provides an elegant error signal for backpropagation:

$$\frac{\partial E}{\partial z_i} = p_i - y_i$$

Where $z_i$ is the logit (pre-activation value) that produces probability $p_i$ after applying the sigmoid function.
This simple gradient form makes binary cross-entropy particularly efficient for training neural networks.

##### Multi-Class Cross-Entropy

When dealing with classification problems involving more than two classes, we extend the binary cross-entropy to
multi-class cross-entropy. This measures the dissimilarity between the predicted probability distribution across all
possible classes and the actual distribution (typically a one-hot encoded vector).

The multi-class cross-entropy loss is defined as:

$$H(y, p) = -\sum_{i=1}^{m} \sum_{j=1}^{n} y_{ij} \ln(p_{ij})$$

Where:

- $m$ is the number of examples
- $n$ is the number of classes
- $y_{ij}$ is 1 if example $i$ belongs to class $j$ and 0 otherwise
- $p_{ij}$ is the predicted probability that example $i$ belongs to class $j$

In practice, for each example, only one term in the inner sum is non-zero (where $y_{ij} = 1$), simplifying the
calculation to:

$$H(y, p) = -\sum_{i=1}^{m} \ln(p_{i,c_i})$$

Where $c_i$ is the correct class for example $i$.

When combined with the softmax function in the output layer, this loss function provides an effective mechanism for
training multi-class classification models. The softmax function converts the network's raw outputs (logits) into a
probability distribution:

$$p_{ij} = \frac{e^{z_{ij}}}{\sum_{k=1}^{n} e^{z_{ik}}}$$

Where $z_{ij}$ is the logit for class $j$ on example $i$.

The gradient of multi-class cross-entropy with respect to the logits has the same elegant form as in the binary case:

$$\frac{\partial E}{\partial z_{ij}} = p_{ij} - y_{ij}$$

This gradient efficiently directs the learning process, increasing the predicted probability for the correct class while
decreasing it for incorrect classes.

##### Mean Squared Error (MSE)

Mean Squared Error is a common loss function for regression problems, measuring the average of the squared differences
between predicted and actual values. It's analogous to measuring the straight-line distance between prediction points
and actual data points in a multi-dimensional space.

The MSE for a set of predictions is calculated as:

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

Where:

- $n$ is the number of samples
- $y_i$ is the true value
- $\hat{y}_i$ is the predicted value

Key properties of MSE include:

1. **Non-negative**: MSE is always greater than or equal to zero, with zero indicating perfect predictions.
2. **Punishes large errors**: The squaring of differences heavily penalizes large prediction errors.
3. **Differentiable**: MSE has a smooth gradient everywhere, making it suitable for optimization.
4. **Unit sensitivity**: Since errors are squared, MSE is sensitive to the units of measurement.

The gradient of MSE with respect to the predictions is:

$$\frac{\partial \text{MSE}}{\partial \hat{y}_i} = \frac{2}{n}(\hat{y}_i - y_i)$$

This gradient provides a clear signal for adjusting model parameters. When predictions are too high, the gradient is
positive, pushing predictions downward. When predictions are too low, the gradient is negative, pushing predictions
upward.

While MSE is primarily used for regression tasks, it can also be applied to classification problems by treating the task
as regressing to the class probabilities. However, cross-entropy is generally preferred for classification due to its
better handling of probability distributions.

##### Maximum Likelihood Estimation

Maximum Likelihood Estimation (MLE) is a statistical method that finds the parameter values which make the observed data
most probable. It provides the theoretical foundation for many loss functions used in neural networks, including
cross-entropy.

The core idea is to maximize the likelihood function, which expresses how probable the observed data is given the model
parameters. For a dataset of independent observations, the likelihood is the product of individual probabilities:

$$L(\theta|X) = P(X|\theta) = \prod_{i=1}^{n} P(x_i|\theta)$$

Where:

- $\theta$ represents the model parameters
- $X = {x_1, x_2, ..., x_n}$ is the dataset
- $P(x_i|\theta)$ is the probability of observing $x_i$ given parameters $\theta$

Working with sums is more computationally convenient than products, so we typically maximize the log-likelihood instead:

$$\log L(\theta|X) = \sum_{i=1}^{n} \log P(x_i|\theta)$$

Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood, which connects directly to our
loss functions:

1. For binary classification with a Bernoulli distribution:
   $$-\log L(\theta|X) = -\sum_{i=1}^{n} [y_i \log p_i + (1-y_i) \log (1-p_i)]$$ This is precisely the binary
   cross-entropy loss.
2. For regression with a Gaussian distribution: $$-\log L(\theta|X) \propto \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$ This is
   proportional to the MSE loss.

The connection between MLE and neural network loss functions provides a probabilistic interpretation of the learning
process. When we minimize cross-entropy, we're finding the parameters that make our observed data most probable under a
certain model structure. This perspective helps in understanding why certain loss functions are appropriate for
different types of problems.

For example, MSE corresponds to the assumption that the target variable follows a Gaussian distribution around the
predicted value, while cross-entropy corresponds to the assumption of a Bernoulli or multinomial distribution for
classification problems.

Understanding this connection allows for principled derivation of loss functions for new problems based on the assumed
probability distribution of the target variable, rather than relying on ad-hoc choices.
