{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Neural Network Architecture Diagram\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "   subgraph \"Input Layer\"\n",
    "       A[\"Feature 1\"]\n",
    "       B[\"Feature 2\"]\n",
    "       C[\"Feature 3\"]\n",
    "       D[\"...\"]\n",
    "       E[\"Feature n\"]\n",
    "   end\n",
    "   \n",
    "   subgraph \"Hidden Layer (2 neurons)\"\n",
    "       F[\"Hidden 1<br/>sigmoid\"]\n",
    "       G[\"Hidden 2<br/>sigmoid\"]\n",
    "   end\n",
    "   \n",
    "   subgraph \"Output Layer\"\n",
    "       H[\"Output<br/>sigmoid<br/>(Binary Classification)\"]\n",
    "   end\n",
    "   \n",
    "   A --> F\n",
    "   A --> G\n",
    "   B --> F\n",
    "   B --> G\n",
    "   C --> F\n",
    "   C --> G\n",
    "   D --> F\n",
    "   D --> G\n",
    "   E --> F\n",
    "   E --> G\n",
    "   \n",
    "   F --> H\n",
    "   G --> H\n",
    "   \n",
    "   style A fill:#E3F2FD\n",
    "style B fill:#E1F5FE\n",
    "style C fill:#B3E5FC\n",
    "style D fill:#81D4FA\n",
    "style E fill:#4FC3F7\n",
    "style F fill:#29B6F6\n",
    "style G fill:#03A9F4\n",
    "style H fill:#039BE5\n",
    "```\n",
    "\n",
    "# 2. Numerical Example: Student Grade Prediction\n",
    "\n",
    "**Problem**: Predict if a student passes (1) or fails (0) based on study hours and sleep hours.\n",
    "\n",
    "**Training Data**: One sample\n",
    "- Student: study_hours = 6, sleep_hours = 8, actual_grade = 1 (pass)\n",
    "\n",
    "**Initial Parameters**:\n",
    "```\n",
    "Input features: x = [6, 8]\n",
    "Target: y = 1\n",
    "\n",
    "Initial Weights:\n",
    "weights_input_hidden = [[0.2, 0.3],    # From input 1 to [hidden 1, hidden 2]\n",
    "                        [0.4, 0.1]]     # From input 2 to [hidden 1, hidden 2]\n",
    "\n",
    "weights_hidden_output = [0.5, -0.2]    # From [hidden 1, hidden 2] to output\n",
    "\n",
    "Learning rate = 0.1\n",
    "```\n",
    "\n",
    "Different weights are essential for the neural network to learn effectively. Here's why:\n",
    "\n",
    "###### Breaking Symmetry\n",
    "\n",
    "**If all weights were identical:**\n",
    "```\n",
    "weights_input_hidden = [[0.2, 0.2],    # Same weights\n",
    "                        [0.2, 0.2]]     # Same weights\n",
    "```\n",
    "\n",
    "**Problem**: Both hidden neurons would:\n",
    "- Receive identical inputs: `0.2×input1 + 0.2×input2`\n",
    "- Produce identical outputs\n",
    "- Receive identical error gradients during backpropagation\n",
    "- Update by identical amounts\n",
    "\n",
    "**Result**: The two hidden neurons would remain functionally identical throughout training, effectively reducing your network to having only one hidden neuron.\n",
    "\n",
    "###### Mathematical Proof of the Problem\n",
    "\n",
    "**Forward pass with identical weights:**\n",
    "```\n",
    "hidden_input = [6, 8] · [[0.2, 0.2],\n",
    "                         [0.2, 0.2]]\n",
    "hidden_input = [4.8, 4.8]  # Identical values\n",
    "hidden_output = [sigmoid(4.8), sigmoid(4.8)] = [0.992, 0.992]  # Identical\n",
    "```\n",
    "\n",
    "**Backpropagation with identical weights:**\n",
    "Both neurons receive the same error signal and update identically, maintaining the symmetry forever.\n",
    "\n",
    "###### Why Different Weights Enable Learning\n",
    "\n",
    "**With different weights:**\n",
    "```\n",
    "weights_input_hidden = [[0.2, 0.3],    # Different weights\n",
    "                        [0.4, 0.1]]     # Different weights\n",
    "```\n",
    "\n",
    "**Each neuron specializes:**\n",
    "- **Hidden neuron 1**: Emphasizes input2 (weight 0.4 vs 0.2)\n",
    "- **Hidden neuron 2**: Emphasizes input1 (weight 0.3 vs 0.1)\n",
    "\n",
    "**This allows:**\n",
    "- **Neuron 1** might learn to detect \"high sleep hours\" patterns\n",
    "- **Neuron 2** might learn to detect \"high study hours\" patterns\n",
    "- **Combined** they can capture complex relationships between both inputs\n",
    "\n",
    "###### The Diversity Principle\n",
    "\n",
    "Different initial weights create **functional diversity**:\n",
    "\n",
    "**Weight Matrix Analysis:**\n",
    "```\n",
    "From input 1: [0.2, 0.3] → Different influence on each hidden neuron\n",
    "From input 2: [0.4, 0.1] → Different influence on each hidden neuron\n",
    "```\n",
    "\n",
    "This creates two distinct \"feature detectors\" in the hidden layer, each capable of learning different aspects of the input patterns.\n",
    "\n",
    "###### Random Initialization Strategy\n",
    "\n",
    "The code uses random initialization specifically to ensure diversity:\n",
    "```python\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "```\n",
    "\n",
    "This guarantees that each connection starts with a different value, breaking symmetry from the beginning and enabling each neuron to develop its own specialized function during training.\n",
    "\n",
    "Without different weights, you lose the representational power that comes from having multiple neurons work together to capture different aspects of the data patterns.\n",
    "\n",
    "###### Forward Propagation\n",
    "\n",
    "**Step 1: Input to Hidden Layer**\n",
    "```\n",
    "hidden_input = x · weights_input_hidden\n",
    "hidden_input = [6, 8] · [[0.2, 0.3],\n",
    "                         [0.4, 0.1]]\n",
    "hidden_input = [6×0.2 + 8×0.4, 6×0.3 + 8×0.1]\n",
    "hidden_input = [1.2 + 3.2, 1.8 + 0.8]\n",
    "hidden_input = [4.4, 2.6]\n",
    "```\n",
    "\n",
    "**Step 2: Apply Sigmoid to Hidden Layer**\n",
    "```\n",
    "hidden_output = sigmoid([4.4, 2.6])\n",
    "hidden_output = [1/(1+e^(-4.4)), 1/(1+e^(-2.6))]\n",
    "hidden_output = [0.988, 0.931]\n",
    "```\n",
    "\n",
    "**Step 3: Hidden to Output Layer**\n",
    "```\n",
    "output_input = hidden_output · weights_hidden_output\n",
    "output_input = [0.988, 0.931] · [0.5, -0.2]\n",
    "output_input = 0.988×0.5 + 0.931×(-0.2)\n",
    "output_input = 0.494 - 0.186 = 0.308\n",
    "```\n",
    "\n",
    "**Step 4: Apply Sigmoid to Output**\n",
    "```\n",
    "output = sigmoid(0.308) = 1/(1+e^(-0.308)) = 0.576\n",
    "```\n",
    "\n",
    "**Prediction**: 0.576 (57.6% chance of passing)\n",
    "\n",
    "###### Backpropagation\n",
    "\n",
    "**Step 1: Calculate Output Error**\n",
    "```\n",
    "error = y - output = 1 - 0.576 = 0.424\n",
    "output_error_term = error × output × (1 - output)\n",
    "output_error_term = 0.424 × 0.576 × (1 - 0.576)\n",
    "output_error_term = 0.424 × 0.576 × 0.424 = 0.104\n",
    "```\n",
    "\n",
    "**Step 2: Propagate Error to Hidden Layer**\n",
    "```\n",
    "hidden_error = output_error_term × weights_hidden_output\n",
    "hidden_error = 0.104 × [0.5, -0.2] = [0.052, -0.021]\n",
    "\n",
    "hidden_error_term = hidden_error × hidden_output × (1 - hidden_output)\n",
    "For hidden neuron 1: 0.052 × 0.988 × (1 - 0.988) = 0.052 × 0.988 × 0.012 = 0.0006\n",
    "For hidden neuron 2: -0.021 × 0.931 × (1 - 0.931) = -0.021 × 0.931 × 0.069 = -0.001\n",
    "\n",
    "hidden_error_term = [0.0006, -0.001]\n",
    "```\n",
    "\n",
    "**Step 3: Calculate Weight Updates**\n",
    "\n",
    "**Update weights_hidden_output**:\n",
    "```\n",
    "Δweights_hidden_output = learning_rate × output_error_term × hidden_output\n",
    "Δweights_hidden_output = 0.1 × 0.104 × [0.988, 0.931]\n",
    "Δweights_hidden_output = [0.0103, 0.0097]\n",
    "\n",
    "New weights_hidden_output = [0.5, -0.2] + [0.0103, 0.0097] = [0.5103, -0.1903]\n",
    "```\n",
    "\n",
    "**Update weights_input_hidden**:\n",
    "```\n",
    "Δweights_input_hidden = learning_rate × hidden_error_term × x[:, None]\n",
    "Δweights_input_hidden = 0.1 × [0.0006, -0.001] × [[6], [8]]\n",
    "\n",
    "For input feature 1: 0.1 × [0.0006, -0.001] × 6 = [0.00036, -0.0006]\n",
    "For input feature 2: 0.1 × [0.0006, -0.001] × 8 = [0.00048, -0.0008]\n",
    "\n",
    "New weights_input_hidden = [[0.2, 0.3],     + [[0.00036, -0.0006],\n",
    "                           [0.4, 0.1]]        [0.00048, -0.0008]]\n",
    "                         = [[0.20036, 0.2994],\n",
    "                            [0.40048, 0.0992]]\n",
    "```\n",
    "\n",
    "###### Learning Progress\n",
    "\n",
    "**After Weight Update**:\n",
    "- **Hidden layer weights** slightly adjusted to better capture input patterns\n",
    "- **Output weights** increased for hidden neuron 1 (positive contribution) and decreased for hidden neuron 2 (negative contribution)\n",
    "- **Next iteration** would use these updated weights for improved prediction\n",
    "\n",
    "**Key Learning**: The network learned that the current prediction (57.6%) was too low for a passing student, so it adjusted weights to increase future predictions for similar input patterns.\n",
    "\n",
    "This process repeats for all training samples and multiple epochs until the network learns to accurately distinguish between passing and failing students based on study and sleep hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "grader_id": "mwqthqbrju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2513572524259881\n",
      "Train loss: 0.24996540718842905\n",
      "Train loss: 0.24862005218904504\n",
      "Train loss: 0.2473199321717981\n",
      "Train loss: 0.24606380465584854\n",
      "Train loss: 0.24485044179257037\n",
      "Train loss: 0.243678632018683\n",
      "Train loss: 0.24254718151769472\n",
      "Train loss: 0.24145491550165454\n",
      "Train loss: 0.24040067932493334\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\"\"\"\n",
    "Neural Network Implementation: Two-Layer Feedforward Network for Binary Classification\n",
    "\n",
    "This implementation demonstrates a complete neural network training process including:\n",
    "- Forward propagation through hidden and output layers\n",
    "- Backpropagation with gradient computation\n",
    "- Weight updates using batch gradient descent\n",
    "- Training loss monitoring and test accuracy evaluation\n",
    "\n",
    "Network Architecture:\n",
    "- Input layer: n_features neurons (determined by data)\n",
    "- Hidden layer: 2 neurons with sigmoid activation\n",
    "- Output layer: 1 neuron with sigmoid activation (binary classification)\n",
    "\n",
    "Training Process:\n",
    "- Uses batch gradient descent (accumulates gradients over all samples)\n",
    "- Updates weights after processing entire training set each epoch\n",
    "- Monitors training loss every 10% of epochs for convergence tracking\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "   \"\"\"\n",
    "   Sigmoid activation function with numerical stability improvements.\n",
    "   \n",
    "   Applies element-wise sigmoid transformation: f(x) = 1 / (1 + e^(-x))\n",
    "   Includes clipping to prevent overflow for extreme input values.\n",
    "   \n",
    "   Args:\n",
    "       x: Input values (scalar, array, or tensor)\n",
    "       \n",
    "   Returns:\n",
    "       Sigmoid-transformed values in range (0, 1)\n",
    "   \"\"\"\n",
    "   x = np.asarray(x, dtype=np.float64)  # Force conversion and dtype\n",
    "   return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Prevent overflow\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights using Xavier/Glorot initialization\n",
    "# Scales initial weights by 1/sqrt(n_features) for stable training\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                       size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=n_hidden)\n",
    "\n",
    "# Training Loop: Batch Gradient Descent\n",
    "for e in range(epochs):\n",
    "   # Initialize gradient accumulation arrays\n",
    "   del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "   del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "   \n",
    "   # Process each training sample and accumulate gradients\n",
    "   for x, y in zip(features, targets):\n",
    "       ## Forward Propagation ##\n",
    "       # Hidden layer computation: input → hidden\n",
    "       hidden_input = np.dot(x, weights_input_hidden)\n",
    "       hidden_output = sigmoid(np.array(hidden_input, dtype=np.float64))\n",
    "       \n",
    "       # Output layer computation: hidden → output\n",
    "       output = sigmoid(np.array(np.dot(hidden_output, weights_hidden_output), dtype=np.float64))                        \n",
    "\n",
    "       ## Backpropagation ##\n",
    "       # Calculate prediction error\n",
    "       error = y - output\n",
    "       \n",
    "       # Output layer error term (gradient of loss w.r.t. output weights)\n",
    "       # For sigmoid: derivative = output * (1 - output)\n",
    "       output_error_term = error * output * (1 - output)\n",
    "\n",
    "       # Propagate error back to hidden layer\n",
    "       # Hidden layer's contribution to output error\n",
    "       hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "       \n",
    "       # Hidden layer error term (gradient of loss w.r.t. hidden weights)\n",
    "       hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "       \n",
    "       # Accumulate weight gradients (sum over all samples in batch)\n",
    "       del_w_hidden_output += output_error_term * hidden_output\n",
    "       del_w_input_hidden += hidden_error_term * np.array(x[:, None], dtype=np.float64)\n",
    "\n",
    "   # Update weights using accumulated gradients\n",
    "   # Divide by n_records to get average gradient over batch\n",
    "   weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "   weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "   # Training Progress Monitoring\n",
    "   # Print loss every 10% of total epochs\n",
    "   if e % (epochs / 10) == 0:\n",
    "       # Calculate current training loss on entire dataset\n",
    "       hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "       out = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
    "       loss = np.mean((out - targets) ** 2)\n",
    "       \n",
    "       # Warning if loss is increasing (possible overfitting or high learning rate)\n",
    "       if last_loss and last_loss < loss:\n",
    "           print(\"Train loss:\", loss, \"WARNING - Loss Increasing\")\n",
    "       else:\n",
    "           print(\"Train loss:\", loss)\n",
    "       last_loss = loss\n",
    "\n",
    "# Model Evaluation on Test Set\n",
    "# Forward pass through trained network\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5)\n",
    "predictions = out > 0.5\n",
    "\n",
    "# Calculate classification accuracy\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
