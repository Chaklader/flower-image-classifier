{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "grader_id": "0zbarnk7zt6c"
   },
   "source": [
    "## What are the weights and bias for the AND perceptron?\n",
    "\n",
    "Set the weights (`and_weight1`, `and_weight2`) and bias (`and_bias`) to values that will correctly determine the AND operation as shown above.\n",
    "More than one set of values will work!\n",
    "\n",
    "\n",
    "##### Understanding the AND Perceptron Problem\n",
    "\n",
    "This code is asking you to implement a perceptron for the logical AND operation. Let me explain what's happening step by step, and how to solve it properly.\n",
    "\n",
    "##### The Logical AND Operation\n",
    "\n",
    "First, let's remember what the AND operation does:\n",
    "- 0 AND 0 = 0 (False)\n",
    "- 0 AND 1 = 0 (False)\n",
    "- 1 AND 0 = 0 (False)\n",
    "- 1 AND 1 = 1 (True)\n",
    "\n",
    "Only when both inputs are 1 (True) will the output be 1 (True).\n",
    "\n",
    "##### How Perceptrons Work\n",
    "\n",
    "A perceptron is the simplest form of a neural network. It takes inputs, multiplies them by weights, adds a bias, and then applies an activation function. Here, the activation function is simply checking if the result is greater than or equal to 0.\n",
    "\n",
    "The mathematical formula is:\n",
    "output = (weight1 × input1 + weight2 × input2 + bias) ≥ 0\n",
    "\n",
    "For our AND perceptron to work correctly, we need to find values for `and_weight1`, `and_weight2`, and `and_bias` that will produce:\n",
    "- Negative values for input combinations (0,0), (0,1), and (1,0)\n",
    "- Positive value for input combination (1,1)\n",
    "\n",
    "##### Working Through the Solution\n",
    "\n",
    "Let's think about this systematically:\n",
    "\n",
    "1. For input (0,0):\n",
    "   - Result = weight1 × 0 + weight2 × 0 + bias = bias\n",
    "   - We need bias < 0 for this to be negative\n",
    "\n",
    "2. For input (0,1):\n",
    "   - Result = weight1 × 0 + weight2 × 1 + bias = weight2 + bias\n",
    "   - We need weight2 + bias < 0\n",
    "\n",
    "3. For input (1,0):\n",
    "   - Result = weight1 × 1 + weight2 × 0 + bias = weight1 + bias\n",
    "   - We need weight1 + bias < 0\n",
    "\n",
    "4. For input (1,1):\n",
    "   - Result = weight1 × 1 + weight2 × 1 + bias = weight1 + weight2 + bias\n",
    "   - We need weight1 + weight2 + bias ≥ 0\n",
    "\n",
    "From these constraints, we can deduce:\n",
    "- bias must be negative\n",
    "- weight1 and weight2 must be positive\n",
    "- The positive weights must overcome the negative bias when both inputs are 1\n",
    "\n",
    "##### One Valid Solution\n",
    "\n",
    "A typical solution would be:\n",
    "```python\n",
    "and_weight1 = 1.0\n",
    "and_weight2 = 1.0\n",
    "and_bias = -1.5\n",
    "```\n",
    "\n",
    "Let's verify this works:\n",
    "- (0,0): 0×1.0 + 0×1.0 + (-1.5) = -1.5 < 0 ✓\n",
    "- (0,1): 0×1.0 + 1×1.0 + (-1.5) = -0.5 < 0 ✓\n",
    "- (1,0): 1×1.0 + 0×1.0 + (-1.5) = -0.5 < 0 ✓\n",
    "- (1,1): 1×1.0 + 1×1.0 + (-1.5) = 0.5 > 0 ✓\n",
    "\n",
    "This solution works because the weights are large enough that when both inputs are 1, their sum exceeds the negative bias, but when only one input is 1, it's not enough to overcome the bias.\n",
    "\n",
    "##### Visualizing the Solution\n",
    "\n",
    "Geometrically, this creates a decision boundary that separates the point (1,1) from the other three points. The weights determine the direction of the boundary, and the bias determines its position.\n",
    "\n",
    "In the 2D input space, our solution creates a line that puts (1,1) on one side and the other points on the other side. This is how the perceptron \"learns\" to classify inputs according to the AND function.\n",
    "\n",
    "##### Other Possible Solutions\n",
    "\n",
    "There are many other solutions. For example:\n",
    "- weights = [0.7, 0.7] and bias = -1.0\n",
    "- weights = [2.0, 2.0] and bias = -3.0\n",
    "\n",
    "As long as both weights are positive, their sum is greater than the absolute value of the negative bias, and individually each weight is less than the absolute value of the bias, the perceptron will correctly implement the AND function.\n",
    "\n",
    "This type of problem forms the foundation of neural networks, where more complex combinations of these simple units can learn much more complicated patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "grader_id": "6ffohlffms4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n"
     ]
    }
   ],
   "source": [
    "and_weight1 = 1.0\n",
    "and_weight2 = 1.0\n",
    "and_bias = -1.5\n",
    "\n",
    "# Inputs and outputs (only 1 AND 1 should result in True)\n",
    "and_test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "and_correct_outputs = [False, False, False, True]\n",
    "and_outputs = []\n",
    "\n",
    "for test_input in and_test_inputs:\n",
    "    linear_combination = and_weight1 * test_input[0] + and_weight2 * test_input[1] + and_bias\n",
    "    output = linear_combination >= 0\n",
    "    and_outputs.append(output)\n",
    "\n",
    "# Check output correctness\n",
    "if and_outputs == and_correct_outputs:\n",
    "    print('Nice!  You got it all correct.')\n",
    "else:\n",
    "    for index in range(len(and_outputs)):\n",
    "        if and_outputs[index] != and_correct_outputs[index]:\n",
    "            print(\"For the input {} your weights and bias produced an output of {}. The correct output is {}.\".format(\n",
    "                and_test_inputs[index],\n",
    "                and_outputs[index],\n",
    "                and_correct_outputs[index]\n",
    "            ))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "grader_id": "jn9f5eqgdv"
   },
   "source": [
    "## What are the weights and bias for the NOT perceptron?\n",
    "\n",
    "Set the weights (`not_weight1`, `not_weight2`)  and bias `not_bias` to the values that calculate the NOT operation on the second input and ignores the first input.\n",
    "\n",
    "##### Understanding the NOT Perceptron Problem\n",
    "\n",
    "This code is asking you to implement a perceptron for a logical operation that appears to be a NOT gate on the second input. Let me walk through this systematically to help you understand what's needed.\n",
    "\n",
    "##### The Logical Operation\n",
    "\n",
    "Looking at the expected outputs:\n",
    "- (0,0) → True\n",
    "- (0,1) → False\n",
    "- (1,0) → True\n",
    "- (1,1) → False\n",
    "\n",
    "The pattern here is that the output is True if and only if the second input is 0. This is essentially a NOT operation on the second input, while ignoring the first input completely.\n",
    "\n",
    "In logical terms, this is equivalent to: NOT(input2)\n",
    "\n",
    "##### How to Approach This Perceptron\n",
    "\n",
    "Just like the AND perceptron, we need to find values for `not_weight1`, `not_weight2`, and `not_bias` that will create the right decision boundary. The perceptron will output True when:\n",
    "\n",
    "(not_weight1 × input1 + not_weight2 × input2 + not_bias) ≥ 0\n",
    "\n",
    "##### Analyzing the Constraints\n",
    "\n",
    "Let's analyze what happens for each input combination:\n",
    "\n",
    "1. For input (0,0):\n",
    "   - Result = not_weight1 × 0 + not_weight2 × 0 + not_bias = not_bias\n",
    "   - We need not_bias ≥ 0 for this to output True\n",
    "\n",
    "2. For input (0,1):\n",
    "   - Result = not_weight1 × 0 + not_weight2 × 1 + not_bias = not_weight2 + not_bias\n",
    "   - We need not_weight2 + not_bias < 0 for this to output False\n",
    "\n",
    "3. For input (1,0):\n",
    "   - Result = not_weight1 × 1 + not_weight2 × 0 + not_bias = not_weight1 + not_bias\n",
    "   - We need not_weight1 + not_bias ≥ 0 for this to output True\n",
    "\n",
    "4. For input (1,1):\n",
    "   - Result = not_weight1 × 1 + not_weight2 × 1 + not_bias = not_weight1 + not_weight2 + not_bias\n",
    "   - We need not_weight1 + not_weight2 + not_bias < 0 for this to output False\n",
    "\n",
    "##### Deriving the Solution\n",
    "\n",
    "From these constraints, we can conclude:\n",
    "\n",
    "1. not_bias must be positive (from constraint 1)\n",
    "2. not_weight2 must be negative and its absolute value must be greater than not_bias (from constraint 2)\n",
    "3. not_weight1 must be zero or positive to maintain constraint 3 while satisfying constraint 4\n",
    "\n",
    "The simplest solution is to make not_weight1 = 0 (meaning the first input has no effect), not_weight2 = negative, and not_bias = positive.\n",
    "\n",
    "##### A Valid Solution\n",
    "\n",
    "A typical solution would be:\n",
    "```python\n",
    "not_weight1 = 0.0\n",
    "not_weight2 = -1.0\n",
    "not_bias = 0.5\n",
    "```\n",
    "\n",
    "Let's verify this works:\n",
    "- (0,0): 0×0.0 + 0×(-1.0) + 0.5 = 0.5 ≥ 0 → True ✓\n",
    "- (0,1): 0×0.0 + 1×(-1.0) + 0.5 = -0.5 < 0 → False ✓\n",
    "- (1,0): 1×0.0 + 0×(-1.0) + 0.5 = 0.5 ≥ 0 → True ✓\n",
    "- (1,1): 1×0.0 + 1×(-1.0) + 0.5 = -0.5 < 0 → False ✓\n",
    "\n",
    "This works perfectly! The perceptron correctly implements NOT(input2).\n",
    "\n",
    "##### Visualization of the Solution\n",
    "\n",
    "In this case, the decision boundary is a vertical line in the input space. All points to the left of the line (where input2 = 0) result in True, while all points to the right (where input2 = 1) result in False.\n",
    "\n",
    "The first input has no influence on the decision because its weight is zero.\n",
    "\n",
    "##### Generalized Solutions\n",
    "\n",
    "Many other solutions exist as well. Any values that satisfy:\n",
    "- not_weight1 = any number (typically 0 for simplicity)\n",
    "- not_weight2 < 0 (negative)\n",
    "- not_bias > 0 (positive)\n",
    "- |not_weight2| > not_bias (the absolute value of the negative weight must exceed the positive bias)\n",
    "\n",
    "For example, alternative solutions could be:\n",
    "- not_weight1 = 0.0, not_weight2 = -2.0, not_bias = 1.0\n",
    "- not_weight1 = 0.5, not_weight2 = -1.5, not_bias = 0.7\n",
    "\n",
    "As long as the solution satisfies the constraints derived from the truth table, it will correctly implement the NOT function on the second input.\n",
    "\n",
    "##### Why This Matters\n",
    "\n",
    "This example illustrates an important point: perceptrons can learn to ignore irrelevant inputs by assigning them weights of zero. This is a form of feature selection that occurs naturally during training when some inputs don't contribute to predicting the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "grader_id": "b0wtcwiwk79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n"
     ]
    }
   ],
   "source": [
    "not_weight1 = 0.0\n",
    "not_weight2 = -1.0\n",
    "not_bias = 0.5\n",
    "\n",
    "# Inputs and outputs (True only if the second value is 0)\n",
    "not_test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "not_correct_outputs = [True, False, True, False]\n",
    "not_outputs = []\n",
    "\n",
    "# Generate output\n",
    "for test_input in not_test_inputs:\n",
    "    linear_combination = not_weight1 * test_input[0] \\\n",
    "                            + not_weight2 * test_input[1] \\\n",
    "                            + not_bias\n",
    "    output = linear_combination >= 0\n",
    "    not_outputs.append(output)\n",
    "\n",
    "# Check output correctness\n",
    "if not_outputs == not_correct_outputs:\n",
    "    print('Nice!  You got it all correct.')\n",
    "else:\n",
    "    for index in range(len(not_outputs)):\n",
    "        if not_outputs[index] != not_correct_outputs[index]:\n",
    "            print(\"For the input {} your weights and bias \\\n",
    "produced an output of {}. The correct output is {}.\".format(\n",
    "                not_test_inputs[index],\n",
    "                not_outputs[index],\n",
    "                not_correct_outputs[index]\n",
    "            ))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aipnd)",
   "language": "python",
   "name": "aipnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
