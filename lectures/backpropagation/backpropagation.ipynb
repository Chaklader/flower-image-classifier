{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-entropy\n",
    "\n",
    "Cross-entropy is a loss (cost) function that measures how well a set of predicted probabilities P matches the true labels Y.\n",
    "\n",
    "For binary classification (labels 0 or 1) the formula is\n",
    "\n",
    "L(Y,P) = - Σ [ Y · log P + (1-Y) · log (1-P) ]\n",
    "\n",
    "where\n",
    "· Y is the true label (1 = positive, 0 = negative)\n",
    "· P is the model's predicted probability that the label is 1\n",
    "· The sum Σ runs over all samples.\n",
    "\n",
    "Key points\n",
    "· If the prediction is perfect (P ≈ 1 when Y = 1, or P ≈ 0 when Y = 0) the loss is near 0.\n",
    "· Confident wrong predictions (e.g. P ≈ 1 when Y = 0) incur a very large loss because log(0) → -∞.\n",
    "· Cross-entropy is convex for logistic regression, giving smooth gradients for optimisation.\n",
    "\n",
    "The code you posted implements this exactly:\n",
    "\n",
    "```python\n",
    "return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n",
    "```\n",
    "\n",
    "It converts Y and P to floats, takes element-wise logs, multiplies by the appropriate label term, sums over all samples, and returns the negative of that sum (so lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y  *np.log(P) + (1 - Y)*  np.log(1 - P))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph \"Input Layer\"\n",
    "        I0[\"Input 0 | (0.5)\"]\n",
    "        I1[\"Input 1 | (0.1)\"]\n",
    "        I2[\"Input 2 | (-0.2)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Hidden Layer\"\n",
    "        H0[\"Hidden 0 | (σ = 0.632)\"]\n",
    "        H1[\"Hidden 1 | (σ = 0.456)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Output Layer\"\n",
    "        O0[\"Output | (σ = 0.540) | Target = 0.6\"]\n",
    "    end\n",
    "    \n",
    "    I0 -->|0.5| H0\n",
    "    I0 -->|-0.6| H1\n",
    "    I1 -->|0.1| H0\n",
    "    I1 -->|-0.2| H1\n",
    "    I2 -->|0.1| H0\n",
    "    I2 -->|0.7| H1\n",
    "    \n",
    "    H0 -->|0.1| O0\n",
    "    H1 -->|-0.3| O0\n",
    "    \n",
    "    \n",
    "    style I0 fill:#9AE4F5\n",
    "    style I1 fill:#9AE4F5\n",
    "    style I2 fill:#9AE4F5\n",
    "    style H0 fill:#BCFB89\n",
    "    style H1 fill:#BCFB89\n",
    "    style O0 fill:#FA756A\n",
    "```\n",
    "\n",
    "This diagram shows:\n",
    "\n",
    "1. The input layer with 3 nodes (blue)\n",
    "2. The hidden layer with 2 nodes (green)\n",
    "3. The output layer with 1 node (coral)\n",
    "\n",
    "The connections between the input and hidden layers show the weights from your weight matrix:\n",
    "- Input 0 connects to Hidden 0 with weight 0.5\n",
    "- Input 0 connects to Hidden 1 with weight -0.6\n",
    "- Input 1 connects to Hidden 0 with weight 0.1\n",
    "- Input 1 connects to Hidden 1 with weight -0.2\n",
    "- Input 2 connects to Hidden 0 with weight 0.1\n",
    "- Input 2 connects to Hidden 1 with weight 0.7\n",
    "\n",
    "The connections between the hidden layer and output layer are shown as w1 and w2, as these weren't specified in your example.\n",
    "\n",
    "This network structure performs the matrix multiplication you described: when a 3-element input vector is multiplied by the 3×2 weight matrix, it produces a 2-element vector that serves as input to the hidden layer neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "grader_id": "tpan1eblwvo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Error: 0.11502656915007464\n"
     ]
    }
   ],
   "source": [
    "# x is a single feature vector for the training that feeds into the network’s input layer. \n",
    "# It has three features—[0.5, 0.1, -0.2]—matching the three input nodes used in the notebook’s \n",
    "# weight matrices.\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "\n",
    "# target is the desired (ground-truth) value for that training example.\n",
    "target = 0.6\n",
    "\n",
    "# learnrate is the learning rate, a hyperparameter that controls how much we adjust the weights in each iteration.\n",
    "learnrate = 0.5\n",
    "\n",
    "# weights_input_hidden is the weight matrix connecting the input layer to the hidden layer.\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "\n",
    "# Calculate output error\n",
    "error = target - output      \n",
    "print(\"Output Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate error term for output layer\n",
    "\n",
    "output * (1 - output) is the derivative of the sigmoid activation\n",
    "\n",
    "```\n",
    "σ'(z) = σ(z)(1 - σ(z))\n",
    "```\n",
    "\n",
    "In back-propagation you multiply that derivative by the error coming from\n",
    "the loss:\n",
    "\n",
    "```\n",
    "error = (y_true - output) # ∂L/∂output \n",
    "output_error_term = error * output * (1 - output) # ∂L/∂z\n",
    "```\n",
    "\n",
    "So `output_error_term` is not the derivative of the sigmoid by itself; it is the\n",
    "gradient of the loss with respect to the neuron's pre-activation input z.\n",
    "In other words, it's the product of:\n",
    "\n",
    "1. the derivative of the loss w.r.t. the neuron's output (error), and\n",
    "2. the derivative of the sigmoid w.r.t. its input (output * (1-output))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Error Term: 0.028730669543515015\n"
     ]
    }
   ],
   "source": [
    "gradient_out = output * (1 - output) # ∂L/∂z\n",
    "output_error_term = error * gradient_out\n",
    "\n",
    "print(\"Output Error Term:\", output_error_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Error Term: [ 0.00070802 -0.00204471]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculates the error term for the hidden layer in backpropagation.\n",
    "\n",
    "This code block performs a crucial step in training a neural network:\n",
    "it determines how much the hidden layer contributed to the overall error.\n",
    "This is achieved by propagating the error signal backward from the output\n",
    "layer.\n",
    "\n",
    "The process uses the chain rule from calculus. The error for the hidden\n",
    "layer (delta_h) is the error from the output layer (delta_o) weighted by\n",
    "the connections (W_ho), multiplied by the gradient of the hidden layer's\n",
    "activation function (g'(z_h)).\n",
    "\n",
    "\n",
    "Line-by-line Breakdown:\n",
    "\n",
    "1. `gradient_hidden = hidden_layer_output * (1 - hidden_layer_output)`\n",
    "   - This calculates the derivative of the sigmoid activation function.\n",
    "   - The derivative of sigmoid(x) is sigmoid(x) * (1 - sigmoid(x)).\n",
    "   - `hidden_layer_output` is the result of the sigmoid function, so this\n",
    "     line computes the gradient needed for the chain rule.\n",
    "\n",
    "2. `hidden_error_term = np.dot(output_error_term, weights_hidden_output) * gradient_hidden`\n",
    "   - `np.dot(...)`: This part propagates the error from the output layer\n",
    "     back to the hidden layer. It computes the weighted sum of output errors\n",
    "     that each hidden neuron is responsible for.\n",
    "   - `* gradient_hidden`: The propagated error is then multiplied by the\n",
    "     gradient of the activation function to get the final error term for\n",
    "     the hidden layer.\n",
    "\n",
    "This `hidden_error_term` is then used to calculate the weight updates for\n",
    "the weights connecting the input layer to the hidden layer.\n",
    "\"\"\"\n",
    "\n",
    "# Calculate error term for hidden layer\n",
    "gradient_hidden = hidden_layer_output * (1 - hidden_layer_output) # ∂L/∂z\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * gradient_hidden\n",
    "            \n",
    "print(\"Hidden Error Term:\", hidden_error_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta W H O: [0.00804047 0.00555918]\n"
     ]
    }
   ],
   "source": [
    "# Calculate change in weights for hidden layer to output layer\n",
    "\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output \n",
    "print(\"Delta W H O:\", delta_w_h_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
