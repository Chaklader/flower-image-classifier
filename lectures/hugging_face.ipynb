{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization\n",
    "\n",
    "First, to use the high-level API from Hugging Face, we need to import the pipeline function from the transformers library. Let's see how we can summarize some text. To do this, we first need to create a pipeline and specify the summarization task. When we create it, this function will return a summarizer that we can then use to summarize our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "grader_id": "tpan1eblwvo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Text to be summarized\n",
    "text = \"\"\"\n",
    "The Hub is home to over 5,000 datasets in more than 100 languages that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. The Hub makes it simple to find, download, and upload datasets.\n",
    "Datasets are accompanied by extensive documentation in the form of Dataset Cards and Dataset Preview to let you explore the data directly in your browser.\n",
    "While many datasets are public, organizations and individuals can create private datasets to comply with licensing or privacy issues. You can learn more about Datasets here on Hugging Face Hub documentation.\n",
    "\n",
    "The ðŸ¤— datasets library allows you to programmatically interact with the datasets, so you can easily use datasets from the Hub in your projects.\n",
    "With a single line of code, you can access the datasets; even if they are so large they donâ€™t fit in your computer, you can use streaming to efficiently access the data.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(text, max_length=130, min_length=30)\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "Next, let's see how we can generate some text. The process here is very similar. We create a pipeline, provide a prompt that will be used as a starting point for the model to generate follow-up text, and we can also provide additional parameters like max_length and the number of sequences to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text generation pipeline\n",
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "# Define the prompt for the text generation\n",
    "prompt = \"Scientists from University of California, Berkeley has announced that\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "generated_text = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Classification\n",
    "\n",
    "Another thing we will try is to classify text using so-called zero-shot classification. In this context, \"shot\" means how many examples we provide in a prompt for our model to understand what it should do. \"Zero-shot\" means we don't provide any example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Text to classify\n",
    "text = \"Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a modelâ€™s life; train a model in three lines of code in one framework, and load it for inference in another.\"\n",
    "\n",
    "labels = [\"technology\", \"biology\", \"space\", \"transport\"]\n",
    "\n",
    "results = classifier(text, candidate_labels=labels, hypothesis_template=\"This text is about {}.\")\n",
    "for label, score in zip(results['labels'], results['scores']):\n",
    "  print(f\"{label} -> {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Captioning\n",
    "\n",
    "Hugging Face is not limited to working just with text; we can also use it with images. For example, if we have an image, we can create a captioning model that will write a text caption for this image, describing what is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Replace the URL below with the URL of your image\n",
    "image_url = 'https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png'\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(Image(url=image_url))\n",
    "\n",
    "captioner = pipeline(model=\"ydshieh/vit-gpt2-coco-en\")\n",
    "\n",
    "result = captioner(image_url)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References, Resources, and Further Study\n",
    "\n",
    "1. Hugging Face Transformers Documentation - Official documentation for the Transformers library, including usage, tutorials, and API references.\n",
    "2. Hugging Face Model Hub - A repository of pre-trained models for various NLP tasks, including text generation, summarization, and classification.\n",
    "3. Introduction to Hugging Face - A comprehensive guide to getting started with Hugging Face, including tutorials on using the high-level and low-level APIs.\n",
    "4. Pipeline API in Transformers - Detailed explanation of the pipeline API, its components, and usage examples.\n",
    "5. Zero-Shot Classification - Overview of zero-shot classification, including how to set up and use the task with different models.\n",
    "6. Image Processing with Transformers - Documentation on using Transformers for image-related tasks such as classification and captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
