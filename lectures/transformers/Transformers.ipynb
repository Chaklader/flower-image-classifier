{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z2ea-5SaF59N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from get_device import get_device\n",
        "\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DSc0ludHGE1o"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "text = Path('../../data/tiny-shakespeare.txt').read_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7-9Nk7OoGGHc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[0:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Character-Level Tokenizer for Language Models\n",
        "\n",
        "###### Purpose and Overview\n",
        "\n",
        "The `CharTokenizer` class implements a character-level tokenization system that converts text into numerical representations for neural language models. Unlike word-based tokenizers, this approach treats each individual character as a token, making it suitable for character-level language modeling tasks.\n",
        "\n",
        "###### Core Functionality\n",
        "\n",
        "**Bidirectional Mapping System:**\n",
        "The tokenizer maintains two dictionaries for efficient conversion:\n",
        "- `token_id_for_char`: Maps characters to unique integer IDs\n",
        "- `char_for_token_id`: Maps integer IDs back to characters\n",
        "\n",
        "**Key Operations:**\n",
        "- **Encoding**: Text → Tensor of integer IDs\n",
        "- **Decoding**: Tensor of integer IDs → Text\n",
        "- **Vocabulary Management**: Builds and maintains character vocabulary\n",
        "\n",
        "###### Detailed Method Analysis\n",
        "\n",
        "**Initialization Process:**\n",
        "```python\n",
        "tokenizer = CharTokenizer(['a', 'b', 'c', ' ', '!'])\n",
        "# Creates mappings:\n",
        "# token_id_for_char = {'a': 0, 'b': 1, 'c': 2, ' ': 3, '!': 4}\n",
        "# char_for_token_id = {0: 'a', 1: 'b', 2: 'c', 3: ' ', 4: '!'}\n",
        "```\n",
        "\n",
        "**Training from Text:**\n",
        "```python\n",
        "text = \"hello world\"\n",
        "tokenizer = CharTokenizer.train_from_text(text)\n",
        "# Vocabulary: [' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']  # Sorted unique characters\n",
        "```\n",
        "\n",
        "**Encoding Example:**\n",
        "```python\n",
        "text = \"hello\"\n",
        "encoded = tokenizer.encode(text)\n",
        "# Result: tensor([3, 4, 5, 5, 6])  # Each character mapped to its ID\n",
        "```\n",
        "\n",
        "**Decoding Example:**\n",
        "```python\n",
        "token_ids = torch.tensor([3, 4, 5, 5, 6])\n",
        "decoded = tokenizer.decode(token_ids)\n",
        "# Result: \"hello\"  # IDs mapped back to characters\n",
        "```\n",
        "\n",
        "The IDs are generated automatically by the `enumerate()` function in the `__init__` method. Here's exactly how it works:\n",
        "\n",
        "###### ID Assignment Process\n",
        "\n",
        "```python\n",
        "def __init__(self, vocabulary):\n",
        "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "```\n",
        "\n",
        "###### Step-by-Step Breakdown\n",
        "\n",
        "**Input vocabulary:** `['a', 'b', 'c', ' ', '!']`\n",
        "\n",
        "**enumerate() function creates pairs:**\n",
        "```python\n",
        "list(enumerate(['a', 'b', 'c', ' ', '!']))\n",
        "# Result: [(0, 'a'), (1, 'b'), (2, 'c'), (3, ' '), (4, '!')]\n",
        "```\n",
        "\n",
        "**Dictionary comprehension assigns IDs:**\n",
        "```python\n",
        "# For token_id_for_char:\n",
        "{char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "# Becomes: {'a': 0, 'b': 1, 'c': 2, ' ': 3, '!': 4}\n",
        "\n",
        "# For char_for_token_id:\n",
        "{token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "# Becomes: {0: 'a', 1: 'b', 2: 'c', 3: ' ', 4: '!'}\n",
        "```\n",
        "\n",
        "###### The enumerate() Function\n",
        "\n",
        "`enumerate(sequence)` returns pairs of `(index, item)` where the index starts at 0:\n",
        "- Position 0: 'a' gets ID 0\n",
        "- Position 1: 'b' gets ID 1\n",
        "- Position 2: 'c' gets ID 2\n",
        "- Position 3: ' ' gets ID 3\n",
        "- Position 4: '!' gets ID 4\n",
        "\n",
        "The IDs are simply the sequential positions of characters in the vocabulary list, starting from 0. This ensures each character has a unique integer identifier that can be used as an index in neural network embedding layers.\n",
        "\n",
        "\n",
        "\n",
        "###### Practical Application Workflow\n",
        "\n",
        "**Step 1: Vocabulary Creation**\n",
        "The tokenizer scans input text to identify all unique characters and creates a sorted vocabulary ensuring consistent ordering across different runs.\n",
        "\n",
        "**Step 2: Text Processing**\n",
        "During encoding, each character in the input text is looked up in the vocabulary and replaced with its corresponding integer ID, creating a sequence of numbers suitable for neural network processing.\n",
        "\n",
        "**Step 3: Neural Network Integration**\n",
        "The encoded tensors can be fed directly into embedding layers of neural networks, where each character ID is mapped to a learned vector representation.\n",
        "\n",
        "**Step 4: Output Decoding**\n",
        "Model predictions (sequences of token IDs) are converted back to readable text using the reverse mapping.\n",
        "\n",
        "###### Use Cases and Applications\n",
        "\n",
        "**Character-Level Language Models:**\n",
        "- Text generation at character granularity\n",
        "- Handling out-of-vocabulary words naturally\n",
        "- Working with any language or script without preprocessing\n",
        "\n",
        "**Advantages:**\n",
        "- No unknown token issues (every character is in vocabulary)\n",
        "- Handles misspellings and novel words\n",
        "- Language-agnostic approach\n",
        "- Simple implementation and debugging\n",
        "\n",
        "**Limitations:**\n",
        "- Longer sequences than word-based tokenization\n",
        "- May struggle with long-range dependencies\n",
        "- Computationally more intensive for long texts\n",
        "\n",
        "###### Technical Implementation Details\n",
        "\n",
        "**Data Types:**\n",
        "The tokenizer uses `torch.long` dtype for token IDs, which is standard for indexing operations in PyTorch embeddings and ensures compatibility with neural network layers.\n",
        "\n",
        "**Memory Efficiency:**\n",
        "The bidirectional mapping approach provides O(1) lookup time for both encoding and decoding operations, making it efficient for processing large texts.\n",
        "\n",
        "**Vocabulary Consistency:**\n",
        "The sorting of unique characters ensures deterministic vocabulary creation, crucial for model reproducibility across different training runs.\n",
        "\n",
        "This tokenizer serves as a foundational component for character-level natural language processing tasks, providing the essential interface between raw text and numerical representations required by neural networks.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You're absolutely right to question this! The current `__init__` method has a **serious flaw** - it doesn't handle duplicates properly.\n",
        "\n",
        "###### The Problem with Duplicates\n",
        "\n",
        "If the vocabulary contains duplicates, the dictionary comprehension will overwrite previous mappings:\n",
        "\n",
        "```python\n",
        "# Problematic input with duplicates:\n",
        "vocabulary = ['a', 'b', 'a', 'c', 'a']\n",
        "\n",
        "# enumerate() produces:\n",
        "[(0, 'a'), (1, 'b'), (2, 'a'), (3, 'c'), (4, 'a')]\n",
        "\n",
        "# Dictionary comprehension overwrites:\n",
        "token_id_for_char = {'a': 0, 'b': 1, 'a': 2, 'c': 3, 'a': 4}\n",
        "# Final result: {'a': 4, 'b': 1, 'c': 3}  # 'a' only maps to ID 4!\n",
        "\n",
        "char_for_token_id = {0: 'a', 1: 'b', 2: 'a', 3: 'c', 4: 'a'}\n",
        "# Final result: {0: 'a', 1: 'b', 2: 'a', 3: 'c', 4: 'a'}  # Multiple IDs for 'a'\n",
        "```\n",
        "\n",
        "###### The Broken Behavior\n",
        "\n",
        "This creates inconsistent mappings:\n",
        "- `encode('a')` would return ID 4 (the last occurrence)\n",
        "- `decode([0])` would return 'a' \n",
        "- But `encode(decode([0]))` wouldn't equal `[0]` - it would be `[4]`!\n",
        "\n",
        "###### How the Code Should Handle This\n",
        "\n",
        "The code currently **doesn't** handle duplicates properly. A robust implementation should either:\n",
        "\n",
        "**Option 1: Remove duplicates before processing:**\n",
        "```python\n",
        "def __init__(self, vocabulary):\n",
        "    unique_vocab = list(dict.fromkeys(vocabulary))  # Preserves order, removes duplicates\n",
        "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(unique_vocab)}\n",
        "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(unique_vocab)}\n",
        "```\n",
        "\n",
        "**Option 2: Raise an error for duplicates:**\n",
        "```python\n",
        "def __init__(self, vocabulary):\n",
        "    if len(set(vocabulary)) != len(vocabulary):\n",
        "        raise ValueError(\"Vocabulary contains duplicate characters\")\n",
        "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "```\n",
        "\n",
        "###### Why `train_from_text()` Works\n",
        "\n",
        "The `train_from_text()` method avoids this issue by explicitly removing duplicates:\n",
        "```python\n",
        "vocabulary = sorted(list(set(text)))  # set() removes duplicates\n",
        "```\n",
        "\n",
        "So the class works correctly when using `train_from_text()`, but the `__init__` method itself is vulnerable to duplicate inputs. This is a design flaw that should be addressed for robustness.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qg-yHMXPGHYq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class CharTokenizer:\n",
        "    \"\"\"\n",
        "    A simple character-level tokenizer for converting text to and from numerical IDs.\n",
        "\n",
        "    This tokenizer builds a vocabulary from a given text and provides methods\n",
        "    to encode strings into integer tensors and decode them back into strings.\n",
        "    It is a basic but essential component for character-level language models.\n",
        "\n",
        "    Attributes:\n",
        "        token_id_for_char (dict): A mapping from each character in the vocabulary\n",
        "            to its unique integer ID.\n",
        "        char_for_token_id (dict): A reverse mapping from each integer ID back\n",
        "            to its corresponding character.\n",
        "    \"\"\"\n",
        "  \n",
        "    # def __init__(self, vocabulary):\n",
        "    #     \"\"\"\n",
        "    #     Initializes the CharTokenizer with a predefined vocabulary.\n",
        "\n",
        "    #     Args:\n",
        "    #         vocabulary (list or str): An ordered list or string of unique\n",
        "    #             characters that will form the tokenizer's vocabulary.\n",
        "    #     \"\"\"\n",
        "    #     self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "    #     self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "\n",
        "    def __init__(self, vocabulary):\n",
        "        \"\"\"\n",
        "        Initializes the CharTokenizer with a predefined vocabulary.\n",
        "\n",
        "        Args:\n",
        "            vocabulary (list or str): An ordered list or string of unique\n",
        "                characters that will form the tokenizer's vocabulary.\n",
        "        \"\"\"        \n",
        "        \n",
        "        unique_vocab = list(dict.fromkeys(vocabulary))  # Preserves order, removes duplicates\n",
        "        self.token_id_for_char = {char: token_id for token_id, char in enumerate(unique_vocab)}\n",
        "        self.char_for_token_id = {token_id: char for token_id, char in enumerate(unique_vocab)}\n",
        "\n",
        "    @staticmethod\n",
        "    def train_from_text(text):\n",
        "        \"\"\"\n",
        "        Creates a new CharTokenizer instance by building a vocabulary from text.\n",
        "\n",
        "        This static method scans the input text, finds all unique characters,\n",
        "        sorts them to ensure a consistent vocabulary order, and then creates\n",
        "        a new tokenizer instance based on this vocabulary.\n",
        "\n",
        "        Args:\n",
        "            text (str): The corpus of text from which to build the vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            CharTokenizer: A new instance of the tokenizer trained on the text.\n",
        "        \"\"\"\n",
        "        vocabulary = sorted(list(set(text)))\n",
        "        return CharTokenizer(vocabulary)\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a string of text into a tensor of token IDs.\n",
        "\n",
        "        Each character in the input string is mapped to its corresponding integer\n",
        "        ID from the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            text (str): The string to encode.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A 1D tensor of dtype torch.long containing the sequence\n",
        "                of token IDs.\n",
        "        \"\"\"\n",
        "        token_ids = []\n",
        "        for char in text:\n",
        "            token_ids.append(self.token_id_for_char[char])\n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"\n",
        "        Decodes a tensor of token IDs back into a string of text.\n",
        "\n",
        "        Each integer ID in the input tensor is mapped back to its corresponding\n",
        "        character from the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            token_ids (torch.Tensor): A 1D tensor of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded string.\n",
        "        \"\"\"\n",
        "        chars = []\n",
        "        # .tolist() converts the tensor to a standard Python list for iteration.\n",
        "        for token_id in token_ids.tolist():\n",
        "            chars.append(self.char_for_token_id[token_id])\n",
        "        return ''.join(chars)\n",
        "\n",
        "    def vocabulary_size(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of unique characters in the vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            int: The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        return len(self.token_id_for_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pAYBFds4GNAb"
      },
      "outputs": [],
      "source": [
        "tokenizer = CharTokenizer.train_from_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LKq-R9xvJ3RX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n",
            "\n",
            "\n",
            "tensor([20, 43, 50, 50, 43, 53, 53])\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.encode(\"Hello world\"))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(tokenizer.encode(\"Helleoo\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tWpR_hr9GOKs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello world\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3a7qPM-mGPur"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocabulary_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UbtX7_JtHFXL"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(depth=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2I3m9KI6HM-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: '\\n',\n",
            " 1: ' ',\n",
            " 2: '!',\n",
            " 3: '$',\n",
            " 4: '&',\n",
            " 5: \"'\",\n",
            " 6: ',',\n",
            " 7: '-',\n",
            " 8: '.',\n",
            " 9: '3',\n",
            " 10: ':',\n",
            " 11: ';',\n",
            " 12: '?',\n",
            " 13: 'A',\n",
            " 14: 'B',\n",
            " 15: 'C',\n",
            " 16: 'D',\n",
            " 17: 'E',\n",
            " 18: 'F',\n",
            " 19: 'G',\n",
            " 20: 'H',\n",
            " 21: 'I',\n",
            " 22: 'J',\n",
            " 23: 'K',\n",
            " 24: 'L',\n",
            " 25: 'M',\n",
            " 26: 'N',\n",
            " 27: 'O',\n",
            " 28: 'P',\n",
            " 29: 'Q',\n",
            " 30: 'R',\n",
            " 31: 'S',\n",
            " 32: 'T',\n",
            " 33: 'U',\n",
            " 34: 'V',\n",
            " 35: 'W',\n",
            " 36: 'X',\n",
            " 37: 'Y',\n",
            " 38: 'Z',\n",
            " 39: 'a',\n",
            " 40: 'b',\n",
            " 41: 'c',\n",
            " 42: 'd',\n",
            " 43: 'e',\n",
            " 44: 'f',\n",
            " 45: 'g',\n",
            " 46: 'h',\n",
            " 47: 'i',\n",
            " 48: 'j',\n",
            " 49: 'k',\n",
            " 50: 'l',\n",
            " 51: 'm',\n",
            " 52: 'n',\n",
            " 53: 'o',\n",
            " 54: 'p',\n",
            " 55: 'q',\n",
            " 56: 'r',\n",
            " 57: 's',\n",
            " 58: 't',\n",
            " 59: 'u',\n",
            " 60: 'v',\n",
            " 61: 'w',\n",
            " 62: 'x',\n",
            " 63: 'y',\n",
            " 64: 'z'}\n"
          ]
        }
      ],
      "source": [
        "pp.pprint(tokenizer.char_for_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "olX9rHjxHQih"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'\\n': 0,\n",
            " ' ': 1,\n",
            " '!': 2,\n",
            " '$': 3,\n",
            " '&': 4,\n",
            " \"'\": 5,\n",
            " ',': 6,\n",
            " '-': 7,\n",
            " '.': 8,\n",
            " '3': 9,\n",
            " ':': 10,\n",
            " ';': 11,\n",
            " '?': 12,\n",
            " 'A': 13,\n",
            " 'B': 14,\n",
            " 'C': 15,\n",
            " 'D': 16,\n",
            " 'E': 17,\n",
            " 'F': 18,\n",
            " 'G': 19,\n",
            " 'H': 20,\n",
            " 'I': 21,\n",
            " 'J': 22,\n",
            " 'K': 23,\n",
            " 'L': 24,\n",
            " 'M': 25,\n",
            " 'N': 26,\n",
            " 'O': 27,\n",
            " 'P': 28,\n",
            " 'Q': 29,\n",
            " 'R': 30,\n",
            " 'S': 31,\n",
            " 'T': 32,\n",
            " 'U': 33,\n",
            " 'V': 34,\n",
            " 'W': 35,\n",
            " 'X': 36,\n",
            " 'Y': 37,\n",
            " 'Z': 38,\n",
            " 'a': 39,\n",
            " 'b': 40,\n",
            " 'c': 41,\n",
            " 'd': 42,\n",
            " 'e': 43,\n",
            " 'f': 44,\n",
            " 'g': 45,\n",
            " 'h': 46,\n",
            " 'i': 47,\n",
            " 'j': 48,\n",
            " 'k': 49,\n",
            " 'l': 50,\n",
            " 'm': 51,\n",
            " 'n': 52,\n",
            " 'o': 53,\n",
            " 'p': 54,\n",
            " 'q': 55,\n",
            " 'r': 56,\n",
            " 's': 57,\n",
            " 't': 58,\n",
            " 'u': 59,\n",
            " 'v': 60,\n",
            " 'w': 61,\n",
            " 'x': 62,\n",
            " 'y': 63,\n",
            " 'z': 64}\n"
          ]
        }
      ],
      "source": [
        "pp.pprint(tokenizer.token_id_for_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Language Model Dataset: Token Sequence Generation\n",
        "\n",
        "###### Purpose and Functionality\n",
        "\n",
        "The `TokenIdsDataset` class creates training data for autoregressive language models by converting a sequence of tokens into input-target pairs. This dataset implements the standard \"next token prediction\" training paradigm where the model learns to predict the subsequent token given a context window.\n",
        "\n",
        "###### Core Concept: Shifted Sequences\n",
        "\n",
        "**Training Objective:** Given a sequence of tokens, predict the next token\n",
        "**Implementation:** For each position, create pairs where the target is the input shifted by one position\n",
        "\n",
        "###### Detailed Example Walkthrough\n",
        "\n",
        "**Setup:**\n",
        "```python\n",
        "data = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])  # Token sequence\n",
        "block_size = 4  # Context window size\n",
        "dataset = TokenIdsDataset(data, block_size)\n",
        "```\n",
        "\n",
        "**Dataset Length Calculation:**\n",
        "```python\n",
        "len(dataset) = len(data) - block_size = 9 - 4 = 5\n",
        "# 5 possible starting positions for complete sequences\n",
        "```\n",
        "\n",
        "**Sample Generation:**\n",
        "```python\n",
        "# Position 0: x = [1, 2, 3, 4], y = [2, 3, 4, 5]\n",
        "# Position 1: x = [2, 3, 4, 5], y = [3, 4, 5, 6]  \n",
        "# Position 2: x = [3, 4, 5, 6], y = [4, 5, 6, 7]\n",
        "# Position 3: x = [4, 5, 6, 7], y = [5, 6, 7, 8]\n",
        "# Position 4: x = [5, 6, 7, 8], y = [6, 7, 8, 9]\n",
        "```\n",
        "\n",
        "###### Token-by-Token Prediction Logic\n",
        "\n",
        "For each input-target pair, the model learns multiple next-token predictions simultaneously:\n",
        "\n",
        "**Example with Position 0:**\n",
        "```python\n",
        "Input:  [1, 2, 3, 4]\n",
        "Target: [2, 3, 4, 5]\n",
        "\n",
        "# Training signals:\n",
        "# Given context [1] → predict 2\n",
        "# Given context [1, 2] → predict 3  \n",
        "# Given context [1, 2, 3] → predict 4\n",
        "# Given context [1, 2, 3, 4] → predict 5\n",
        "```\n",
        "\n",
        "###### Implementation Analysis\n",
        "\n",
        "**Memory Efficiency:**\n",
        "The dataset doesn't store all possible sequences but generates them on-demand using tensor slicing, making it memory-efficient for large corpora.\n",
        "\n",
        "**Boundary Handling:**\n",
        "```python\n",
        "def __len__(self):\n",
        "    return len(self.data) - self.block_size\n",
        "```\n",
        "This ensures every generated sequence has exactly `block_size` input tokens and `block_size` target tokens, preventing index overflow.\n",
        "\n",
        "**Tensor Slicing:**\n",
        "```python\n",
        "x = self.data[pos:pos + block_size]        # Input: 4 tokens\n",
        "y = self.data[pos + 1:pos + 1 + block_size] # Target: 4 tokens (shifted)\n",
        "```\n",
        "\n",
        "###### Integration with Training Loop\n",
        "\n",
        "**DataLoader Usage:**\n",
        "```python\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "for batch_x, batch_y in dataloader:\n",
        "    # batch_x shape: (32, block_size)  \n",
        "    # batch_y shape: (32, block_size)\n",
        "    predictions = model(batch_x)\n",
        "    loss = criterion(predictions.view(-1, vocab_size), batch_y.view(-1))\n",
        "```\n",
        "\n",
        "The `-1` in `.view(-1, vocab_size)` tells PyTorch to automatically calculate that dimension based on the tensor's total size. Here's why this is necessary:\n",
        "\n",
        "###### Tensor Shape Problem\n",
        "\n",
        "**Before reshaping:**\n",
        "```python\n",
        "# predictions shape: (batch_size, block_size, vocab_size)\n",
        "# batch_y shape: (batch_size, block_size)\n",
        "\n",
        "# Example with batch_size=32, block_size=4, vocab_size=1000:\n",
        "predictions.shape = torch.Size([32, 4, 1000])\n",
        "batch_y.shape = torch.Size([32, 4])\n",
        "```\n",
        "\n",
        "**Loss function requirement:**\n",
        "Most loss functions expect:\n",
        "- Predictions: 2D tensor `(num_samples, num_classes)`\n",
        "- Targets: 1D tensor `(num_samples,)`\n",
        "\n",
        "###### What `.view(-1, vocab_size)` Does\n",
        "\n",
        "**Automatic dimension calculation:**\n",
        "```python\n",
        "predictions.view(-1, vocab_size)\n",
        "# PyTorch calculates: total_elements / vocab_size = first_dimension\n",
        "# (32 * 4 * 1000) / 1000 = 128\n",
        "# Result shape: (128, 1000)\n",
        "\n",
        "batch_y.view(-1)  \n",
        "# Flattens to: (128,)\n",
        "```\n",
        "\n",
        "**Step-by-step breakdown:**\n",
        "```python\n",
        "# Original: (32, 4, 1000) - 32 batches, 4 tokens each, 1000 vocab probabilities\n",
        "# Reshaped: (128, 1000) - 128 individual token predictions, 1000 vocab probabilities\n",
        "\n",
        "# Original targets: (32, 4) - 32 batches, 4 target tokens each  \n",
        "# Reshaped: (128,) - 128 individual target tokens\n",
        "```\n",
        "\n",
        "###### Why Use -1 Instead of Hard-coding?\n",
        "\n",
        "**Flexibility:**\n",
        "```python\n",
        "# Hard-coded (brittle):\n",
        "predictions.view(32 * 4, vocab_size)  # Breaks if batch_size changes\n",
        "\n",
        "# Auto-calculated (robust):  \n",
        "predictions.view(-1, vocab_size)  # Works with any batch_size\n",
        "```\n",
        "\n",
        "**Real example:**\n",
        "```python\n",
        "batch_size = 32\n",
        "block_size = 4\n",
        "vocab_size = 1000\n",
        "\n",
        "# Before reshaping\n",
        "print(predictions.shape)     # torch.Size([32, 4, 1000])\n",
        "print(batch_y.shape)         # torch.Size([32, 4])\n",
        "\n",
        "# After reshaping  \n",
        "print(predictions.view(-1, vocab_size).shape)  # torch.Size([128, 1000])\n",
        "print(batch_y.view(-1).shape)                  # torch.Size([128])\n",
        "```\n",
        "\n",
        "###### What This Accomplishes\n",
        "\n",
        "The reshaping converts from \"batch-of-sequences\" format to \"individual-predictions\" format, where each token prediction is treated as a separate classification problem. This allows the loss function to compute the cross-entropy between each predicted token distribution and its corresponding target token.\n",
        "\n",
        "The `-1` makes the code robust to different batch sizes without requiring manual calculation of the flattened dimension.\n",
        "\n",
        "\n",
        "###### Practical Considerations\n",
        "\n",
        "**Context Length Trade-offs:**\n",
        "- Larger `block_size`: Better long-range dependencies, more memory usage\n",
        "- Smaller `block_size`: Less memory, limited context understanding\n",
        "\n",
        "**Data Utilization:**\n",
        "From a sequence of length N with block size B, the dataset generates N-B training examples, maximizing data utilization through overlapping windows.\n",
        "\n",
        "**Computational Efficiency:**\n",
        "The sliding window approach creates multiple training examples from a single sequence, effectively augmenting the training data without additional storage requirements.\n",
        "\n",
        "This dataset design is fundamental to training transformer-based language models, providing the structured input-target pairs necessary for learning autoregressive text generation through next-token prediction.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TokenIdsDataset(Dataset):\n",
        "  \n",
        "  \"\"\"\n",
        "  A PyTorch Dataset for creating input-target pairs for language model training.\n",
        "\n",
        "  This dataset takes a long sequence of token IDs and a specified block size\n",
        "  (context length) to generate pairs of (input, target) tensors. The input `x`\n",
        "  is a chunk of the data, and the target `y` is the same chunk shifted by one\n",
        "  position to the right. This setup is standard for training a model to predict\n",
        "  the next token in a sequence.\n",
        "\n",
        "  For example, if the data is [1, 2, 3, 4, 5] and block_size is 3:\n",
        "  - A possible `x` would be [1, 2, 3].\n",
        "  - The corresponding `y` would be [2, 3, 4].\n",
        "  \"\"\"\n",
        "  def __init__(self, data, block_size):\n",
        "    \"\"\"\n",
        "    Initializes the dataset.\n",
        "\n",
        "    Args:\n",
        "        data (torch.Tensor): A 1D tensor containing the entire sequence of\n",
        "            token IDs for the text corpus.\n",
        "        block_size (int): The context length or the size of the input\n",
        "            sequences to be generated.\n",
        "    \"\"\"\n",
        "    self.data = data\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Returns the total number of possible sequences that can be generated.\n",
        "\n",
        "    The length is the total number of tokens minus the block size, as this\n",
        "    represents the number of possible starting positions for a full sequence.\n",
        "\n",
        "    Returns:\n",
        "        int: The total number of samples in the dataset.\n",
        "    \"\"\"\n",
        "    return len(self.data) - self.block_size\n",
        "\n",
        "  def __getitem__(self, pos):\n",
        "    \"\"\"\n",
        "    Retrieves a single input-target pair at a given position.\n",
        "\n",
        "    Args:\n",
        "        pos (int): The starting index in the data tensor from which to\n",
        "            create the sequence.\n",
        "\n",
        "    Returns:\n",
        "        tuple[torch.Tensor, torch.Tensor]: A tuple containing the input\n",
        "            tensor `x` and the target tensor `y`.\n",
        "    \"\"\"\n",
        "    # Ensure the requested position is valid.\n",
        "    assert pos < len(self.data) - self.block_size\n",
        "\n",
        "    # The input sequence starts at `pos` and has length `block_size`.\n",
        "    x = self.data[pos:pos + self.block_size]\n",
        "    # The target sequence is shifted by one token to the right.\n",
        "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "The `//` operator performs **floor division** (integer division), which is crucial for ensuring the head size is always an integer.\n",
        "\n",
        "###### Floor Division vs Regular Division\n",
        "\n",
        "**Regular division (`/`):**\n",
        "```python\n",
        "embedding_dim = 768\n",
        "heads_num = 12\n",
        "\n",
        "head_size = embedding_dim / heads_num\n",
        "# Result: 64.0 (float)\n",
        "```\n",
        "\n",
        "**Floor division (`//`):**\n",
        "```python\n",
        "head_size = embedding_dim // heads_num  \n",
        "# Result: 64 (integer)\n",
        "```\n",
        "\n",
        "###### Why This Matters for Neural Networks\n",
        "\n",
        "**Integer requirement:**\n",
        "Neural network dimensions must be integers. You cannot have 64.5 neurons or create a tensor with fractional dimensions.\n",
        "\n",
        "**Example where it makes a difference:**\n",
        "```python\n",
        "embedding_dim = 770  # Not perfectly divisible\n",
        "heads_num = 12\n",
        "\n",
        "regular_division = embedding_dim / heads_num  # 64.16666... (float)\n",
        "floor_division = embedding_dim // heads_num   # 64 (integer)\n",
        "```\n",
        "\n",
        "###### Multi-Head Attention Context\n",
        "\n",
        "In transformer architecture, the embedding dimension must be evenly divided among attention heads:\n",
        "```python\n",
        "# Each attention head gets head_size dimensions\n",
        "# Total: heads_num × head_size = embedding_dim\n",
        "\n",
        "12 heads × 64 dimensions = 768 total embedding dimensions\n",
        "```\n",
        "\n",
        "**Configuration validation:**\n",
        "```python\n",
        "assert embedding_dim % heads_num == 0, \"embedding_dim must be divisible by heads_num\"\n",
        "head_size = embedding_dim // heads_num\n",
        "```\n",
        "\n",
        "The `//` operator ensures you get a clean integer division result, which is essential for creating properly sized tensor operations in the attention mechanism. Using regular division would produce floats that would cause errors when used as tensor dimension specifications.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
        "  \"context_size\": 256,\n",
        "  \"embedding_dim\": 768,\n",
        "  \"heads_num\": 12,\n",
        "  \"layers_num\": 10,\n",
        "  \"dropout_rate\": 0.1,\n",
        "  \"use_bias\": False,\n",
        "}\n",
        "\n",
        "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Single-Head Attention Mechanism Implementation\n",
        "\n",
        "###### Core Purpose\n",
        "\n",
        "The `AttentionHead` class implements a single attention head from the multi-head attention mechanism used in transformer architectures. It performs the fundamental attention operation: allowing each token to attend to (focus on) relevant tokens in the sequence while respecting causal constraints for language modeling.\n",
        "\n",
        "###### Architecture Components\n",
        "\n",
        "**Linear Projection Layers:**\n",
        "```python\n",
        "self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
        "self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])  \n",
        "self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
        "```\n",
        "\n",
        "These layers project the input embeddings into three different subspaces:\n",
        "- **Query (Q)**: What the current token is looking for\n",
        "- **Key (K)**: What each token represents/offers  \n",
        "- **Value (V)**: The actual content each token contributes\n",
        "\n",
        "###### Causal Attention Mask\n",
        "\n",
        "```python\n",
        "casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
        "self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
        "```\n",
        "\n",
        "**Purpose:** Ensures tokens can only attend to previous tokens (including themselves), not future tokens. This is crucial for autoregressive language modeling.\n",
        "\n",
        "**Structure:** Lower triangular matrix where:\n",
        "- 1 = allowed attention (current and previous positions)\n",
        "- 0 = blocked attention (future positions)\n",
        "\n",
        "**Example for context_size=4:**\n",
        "```\n",
        "[[1, 0, 0, 0],\n",
        " [1, 1, 0, 0], \n",
        " [1, 1, 1, 0],\n",
        " [1, 1, 1, 1]]\n",
        "```\n",
        "\n",
        "###### Forward Pass Breakdown\n",
        "\n",
        "**Step 1: Linear Projections**\n",
        "```python\n",
        "Q = self.Q_weights(input) # (B, C, head_size)\n",
        "K = self.K_weights(input) # (B, C, head_size)  \n",
        "V = self.V_weights(input) # (B, C, head_size)\n",
        "```\n",
        "Input shape: `(batch_size, sequence_length, embedding_dim)`\n",
        "Output shapes: `(batch_size, sequence_length, head_size)`\n",
        "\n",
        "**Step 2: Attention Score Computation**\n",
        "```python\n",
        "attention_scores = Q @ K.transpose(1, 2)  # (B, C, C)\n",
        "```\n",
        "Computes similarity between queries and keys using dot product. Result is a `(batch_size, sequence_length, sequence_length)` matrix where `attention_scores[i,j]` represents how much token `i` should attend to token `j`.\n",
        "\n",
        "**Step 3: Causal Masking**\n",
        "```python\n",
        "attention_scores = attention_scores.masked_fill(\n",
        "    self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
        "    -torch.inf\n",
        ")\n",
        "```\n",
        "Sets attention scores for future positions to negative infinity, ensuring they become zero after softmax.\n",
        "\n",
        "**Step 4: Scaled Dot-Product Attention**\n",
        "```python\n",
        "attention_scores = attention_scores / (K.shape[-1] ** 0.5)\n",
        "```\n",
        "Scales by `√(head_size)` to prevent softmax saturation with large dot products.\n",
        "\n",
        "**Step 5: Attention Probabilities**\n",
        "```python\n",
        "attention_scores = torch.softmax(attention_scores, dim=-1)\n",
        "attention_scores = self.dropout(attention_scores)\n",
        "```\n",
        "Converts scores to probabilities that sum to 1 for each query position, then applies dropout for regularization.\n",
        "\n",
        "**Step 6: Weighted Value Aggregation**\n",
        "```python\n",
        "return attention_scores @ V # (B, C, head_size)\n",
        "```\n",
        "Multiplies attention probabilities with values to get the final attended representation.\n",
        "\n",
        "###### Mathematical Formulation\n",
        "\n",
        "The complete attention operation can be expressed as:\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "- $M$ is the causal mask (0 for allowed, $-\\infty$ for blocked)\n",
        "- $d_k$ is the head size (key dimension)\n",
        "\n",
        "###### Key Design Decisions\n",
        "\n",
        "**Head Size Calculation:** `head_size = embedding_dim // heads_num` ensures the total dimension is preserved when multiple heads are concatenated.\n",
        "\n",
        "**Buffer Registration:** Using `register_buffer` for the mask ensures it moves with the model to GPU/CPU without being treated as a trainable parameter.\n",
        "\n",
        "**Dropout Placement:** Applied to attention weights rather than the final output, providing regularization on the attention patterns themselves.\n",
        "\n",
        "This implementation forms the building block for multi-head attention, where multiple such heads operate in parallel and their outputs are concatenated to capture different types of relationships in the data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "attention_scores = Q @ K.transpose(1, 2)  # Shape: (B, T, T)\n",
        "```\n",
        "\n",
        "The `@` symbol is the **matrix multiplication** operator in Python.\n",
        "\n",
        "In the context of PyTorch and NumPy, it's used to perform matrix multiplication on tensors or arrays. It's a more readable, infix alternative to calling a function like `torch.matmul()`.\n",
        "\n",
        "##### In Your Code: `attention_scores = Q @ K.transpose(1, 2)`\n",
        "\n",
        "Let's break down this specific line:\n",
        "\n",
        "1.  **`Q`**: This is the \"Query\" tensor, with a shape of `(Batch, Tokens, Head_size)`.\n",
        "2.  **`K`**: This is the \"Key\" tensor, also with a shape of `(Batch, Tokens, Head_size)`.\n",
        "3.  **`K.transpose(1, 2)`**: This transposes the Key tensor, swapping its last two dimensions. Its new shape becomes `(Batch, Head_size, Tokens)`. This is done to make the dimensions compatible for matrix multiplication.\n",
        "4.  **`Q @ ...`**: This performs the matrix multiplication:\n",
        "    *   `Q` shape: `(B, T, H)`\n",
        "    *   `K.transpose` shape: `(B, H, T)`\n",
        "    *   Resulting `attention_scores` shape: `(B, T, T)`\n",
        "\n",
        "The resulting `(B, T, T)` tensor holds the attention scores, where each token in the sequence has a score indicating how much it should \"attend to\" every other token.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A single head of self-attention for a transformer model.\n",
        "\n",
        "    This module implements the scaled dot-product attention mechanism. It takes\n",
        "    a sequence of token embeddings and computes a new representation for each\n",
        "    token by attending to all other tokens in the sequence. It learns three\n",
        "    linear projections (Query, Key, Value) to transform the input embeddings.\n",
        "\n",
        "    The key components are:\n",
        "    - Q, K, V linear layers to project the input.\n",
        "    - A causal mask to prevent tokens from attending to future tokens.\n",
        "    - Scaled dot-product attention calculation.\n",
        "    - Dropout for regularization.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initializes the AttentionHead module.\n",
        "\n",
        "        Args:\n",
        "            config (dict): A configuration dictionary containing the following keys:\n",
        "                - \"embedding_dim\" (int): The dimensionality of the input token embeddings.\n",
        "                - \"head_size\" (int): The dimensionality of the Query, Key, and Value projections.\n",
        "                - \"use_bias\" (bool): Whether to use a bias term in the linear layers.\n",
        "                - \"dropout_rate\" (float): The dropout rate to apply to the attention scores.\n",
        "                - \"context_size\" (int): The maximum sequence length (block size).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear layers to project input embeddings into Query, Key, and Value spaces.\n",
        "        self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
        "        self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
        "        self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
        "\n",
        "        # Dropout layer to regularize attention scores.\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "\n",
        "        # Create a lower triangular matrix for the causal attention mask.\n",
        "        # This prevents tokens from attending to future tokens in the sequence.\n",
        "        casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
        "        \n",
        "        # `register_buffer` makes the mask a part of the module's state, but not\n",
        "        # a parameter to be trained. This ensures it's moved to the correct\n",
        "        # device (e.g., GPU) along with the model.\n",
        "        self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
        "\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the attention mechanism.\n",
        "\n",
        "        Args:\n",
        "            input_embeddings (torch.Tensor): A tensor of shape (B, T, E) where\n",
        "                B is the batch size, T is the sequence length (tokens_num), and\n",
        "                E is the embedding dimension.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor of shape (B, T, H) where H is the\n",
        "                head size. This is the weighted aggregation of the Value vectors.\n",
        "        \"\"\"\n",
        "        batch_size, tokens_num, embedding_dim = input_embeddings.shape\n",
        "        \n",
        "        # 1. Project input into Query, Key, and Value tensors.\n",
        "        Q = self.Q_weights(input_embeddings) # Shape: (B, T, H)\n",
        "        K = self.K_weights(input_embeddings) # Shape: (B, T, H)\n",
        "        V = self.V_weights(input_embeddings) # Shape: (B, T, H)\n",
        "\n",
        "        # 2. Calculate attention scores by taking the dot product of Q and K.\n",
        "        # K is transposed to align dimensions for matrix multiplication.\n",
        "        attention_scores = Q @ K.transpose(1, 2)  # Shape: (B, T, T)\n",
        "\n",
        "        # 3. Apply the causal mask to prevent future-peeking.\n",
        "        # We set the scores for future positions to negative infinity so that\n",
        "        # they become zero after the softmax operation.\n",
        "        attention_scores = attention_scores.masked_fill(\n",
        "            self.casual_attention_mask[:tokens_num, :tokens_num] == 0,\n",
        "            -torch.inf\n",
        "        )\n",
        "        \n",
        "        # 4. Scale the attention scores to stabilize gradients.\n",
        "        # This is divided by the square root of the Key dimension (head_size).\n",
        "        attention_scores = attention_scores / (K.shape[-1] ** 0.5)\n",
        "        \n",
        "        # 5. Apply softmax to convert scores into probability distributions (weights).\n",
        "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
        "        \n",
        "        # 6. Apply dropout for regularization.\n",
        "        attention_scores = self.dropout(attention_scores)\n",
        "\n",
        "        # 7. Compute the final output by taking a weighted sum of the Value vectors.\n",
        "        return attention_scores @ V # Shape: (B, T, H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"]) \n",
        "\n",
        "ah = AttentionHead(config)\n",
        "output = ah(input)\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Multi-Head Attention Implementation\n",
        "\n",
        "###### Core Purpose\n",
        "\n",
        "The `MultiHeadAttention` class implements the complete multi-head attention mechanism by combining multiple parallel attention heads and processing their combined output. This allows the model to attend to different types of relationships and patterns simultaneously across multiple representation subspaces.\n",
        "\n",
        "###### Architecture Overview\n",
        "\n",
        "**Parallel Head Structure:**\n",
        "```python\n",
        "heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
        "self.heads = nn.ModuleList(heads_list)\n",
        "```\n",
        "\n",
        "Creates multiple independent attention heads (typically 12) that operate in parallel, each focusing on different aspects of the input relationships.\n",
        "\n",
        "**Output Processing:**\n",
        "```python\n",
        "self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
        "self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "```\n",
        "\n",
        "A linear projection layer that processes the concatenated head outputs, followed by dropout for regularization.\n",
        "\n",
        "###### Forward Pass Breakdown\n",
        "\n",
        "**Step 1: Parallel Head Computation**\n",
        "```python\n",
        "heads_outputs = [head(input) for head in self.heads]\n",
        "```\n",
        "\n",
        "Each attention head processes the input independently:\n",
        "- Input shape: `(batch_size, sequence_length, embedding_dim)`\n",
        "- Each head output shape: `(batch_size, sequence_length, head_size)`\n",
        "- Number of outputs: `heads_num` (e.g., 12)\n",
        "\n",
        "**Step 2: Concatenation**\n",
        "```python\n",
        "scores_change = torch.cat(heads_outputs, dim=-1)\n",
        "```\n",
        "\n",
        "Concatenates all head outputs along the feature dimension:\n",
        "- Individual head: `(B, C, head_size)` where `head_size = embedding_dim // heads_num`\n",
        "- After concatenation: `(B, C, heads_num × head_size) = (B, C, embedding_dim)`\n",
        "\n",
        "**Numerical Example:**\n",
        "```python\n",
        "# Config: embedding_dim=768, heads_num=12, head_size=64\n",
        "# Input: (32, 256, 768)  # batch_size=32, sequence_length=256\n",
        "\n",
        "# Each head output: (32, 256, 64)\n",
        "# After concatenation: (32, 256, 768)  # 12 × 64 = 768\n",
        "```\n",
        "\n",
        "**Step 3: Linear Projection**\n",
        "```python\n",
        "scores_change = self.linear(scores_change)\n",
        "```\n",
        "\n",
        "Applies a learned linear transformation to the concatenated outputs:\n",
        "- Weight matrix: `(embedding_dim, embedding_dim)` = `(768, 768)`\n",
        "- Allows heads to interact and combine their representations\n",
        "- Maintains the original embedding dimension\n",
        "\n",
        "**Step 4: Regularization**\n",
        "```python\n",
        "return self.dropout(scores_change)\n",
        "```\n",
        "\n",
        "Applies dropout to prevent overfitting on the attention patterns.\n",
        "\n",
        "###### Why Multiple Heads?\n",
        "\n",
        "**Representation Diversity:**\n",
        "Each head can specialize in different types of relationships:\n",
        "- Head 1: Syntactic dependencies (subject-verb relationships)\n",
        "- Head 2: Semantic similarity (related concepts)\n",
        "- Head 3: Positional patterns (sequential ordering)\n",
        "- Head 4: Long-range dependencies (paragraph-level connections)\n",
        "\n",
        "**Parallel Processing:**\n",
        "All heads compute simultaneously, making the operation efficient while capturing multiple perspectives on the same input.\n",
        "\n",
        "###### Mathematical Formulation\n",
        "\n",
        "The complete multi-head attention can be expressed as:\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "\n",
        "Where:\n",
        "- $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
        "- $W^O$ is the output projection matrix (`self.linear`)\n",
        "- $h$ is the number of heads\n",
        "\n",
        "###### Dimension Preservation\n",
        "\n",
        "**Key insight:** The total computational cost remains similar to single-head attention:\n",
        "- Single head: `(B, C, embedding_dim)` → `(B, C, embedding_dim)`\n",
        "- Multi-head: `heads_num × (B, C, head_size)` → `(B, C, embedding_dim)`\n",
        "- Total parameters: Similar due to dimension splitting\n",
        "\n",
        "**Head Size Relationship:**\n",
        "```python\n",
        "head_size = embedding_dim // heads_num  # 768 // 12 = 64\n",
        "total_dim = heads_num × head_size       # 12 × 64 = 768\n",
        "```\n",
        "\n",
        "###### Integration Benefits\n",
        "\n",
        "**Enhanced Representation:** Captures multiple types of attention patterns simultaneously rather than learning a single averaged attention pattern.\n",
        "\n",
        "**Computational Efficiency:** Parallel computation across heads with dimension splitting maintains reasonable computational cost.\n",
        "\n",
        "**Learning Flexibility:** Different heads can specialize during training without interfering with each other's learning process.\n",
        "\n",
        "This multi-head structure is fundamental to transformer architectures, enabling the model to build rich, multi-faceted representations of sequence relationships that single-head attention cannot achieve.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Multi-Head Attention mechanism for a transformer model.\n",
        "\n",
        "    This module runs multiple self-attention \"heads\" in parallel and then\n",
        "    concatenates their outputs. This allows the model to jointly attend to\n",
        "    information from different representation subspaces at different positions.\n",
        "    A final linear layer is applied to the concatenated outputs to produce\n",
        "    the final result.\n",
        "\n",
        "    This architecture is a core component of the Transformer model, enabling it\n",
        "    to capture a richer variety of relationships within the input sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initializes the MultiHeadAttention module.\n",
        "\n",
        "        Args:\n",
        "            config (dict): A configuration dictionary containing the following keys:\n",
        "                - \"heads_num\" (int): The number of parallel attention heads to use.\n",
        "                - \"embedding_dim\" (int): The dimensionality of the input and output.\n",
        "                - \"dropout_rate\" (float): The dropout rate for the final output.\n",
        "                - Other keys required by `AttentionHead` (head_size, use_bias, etc.).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a list of `AttentionHead` modules, one for each head.\n",
        "        # `nn.ModuleList` is used to properly register all the heads as sub-modules.\n",
        "        heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
        "        self.heads = nn.ModuleList(heads_list)\n",
        "\n",
        "        # A final linear layer to project the concatenated head outputs back\n",
        "        # to the original embedding dimension.\n",
        "        self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
        "        \n",
        "        # A dropout layer for regularization on the final output.\n",
        "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        \"\"\"\n",
        "        Performs the forward pass for Multi-Head Attention.\n",
        "\n",
        "        Args:\n",
        "            input_embeddings (torch.Tensor): A tensor of shape (B, T, E) where\n",
        "                B is the batch size, T is the sequence length, and E is the\n",
        "                embedding dimension.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The final output tensor of shape (B, T, E).\n",
        "        \"\"\"\n",
        "        # 1. Run each attention head in parallel on the same input.\n",
        "        # This results in a list of output tensors, each of shape (B, T, H).\n",
        "        heads_outputs = [head(input_embeddings) for head in self.heads]\n",
        "\n",
        "        # 2. Concatenate the outputs of all heads along the last dimension.\n",
        "        # If we have N heads, the shape becomes (B, T, N * H).\n",
        "        # Note: For this to work, N * H must equal the embedding_dim.\n",
        "        concatenated_heads = torch.cat(heads_outputs, dim=-1)\n",
        "\n",
        "        # 3. Pass the concatenated output through a final linear layer.\n",
        "        # This projects the combined attention information back to the original\n",
        "        # embedding dimension, shape (B, T, E).\n",
        "        projected_output = self.linear(concatenated_heads)\n",
        "        \n",
        "        # 4. Apply dropout for regularization.\n",
        "        return self.dropout(projected_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mha = MultiHeadAttention(config)\n",
        "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
        "\n",
        "output = mha(input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "#### Feed-Forward Network in Transformer Architecture\n",
        "\n",
        "```python\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise feed-forward network used in transformer blocks.\n",
        "    \n",
        "    This module implements a two-layer fully connected network with GELU activation\n",
        "    that processes each position in the sequence independently. It expands the \n",
        "    embedding dimension by a factor of 4, applies non-linear activation, then\n",
        "    projects back to the original dimension.\n",
        "    \n",
        "    The architecture follows the standard transformer design:\n",
        "    embedding_dim → 4×embedding_dim → embedding_dim\n",
        "    \n",
        "    Args:\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - embedding_dim (int): The input/output dimension size\n",
        "            - dropout_rate (float): Dropout probability for regularization\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "##### Architecture Overview\n",
        "\n",
        "**Two-Layer Structure:**\n",
        "```python\n",
        "nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),  # Expansion\n",
        "nn.GELU(),                                                        # Activation\n",
        "nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]), # Contraction\n",
        "nn.Dropout(config[\"dropout_rate\"])                               # Regularization\n",
        "```\n",
        "\n",
        "**Dimension Transformation:**\n",
        "- Input: `(batch_size, sequence_length, embedding_dim)`\n",
        "- Hidden: `(batch_size, sequence_length, embedding_dim × 4)`\n",
        "- Output: `(batch_size, sequence_length, embedding_dim)`\n",
        "\n",
        "##### GELU Activation Function\n",
        "\n",
        "**GELU (Gaussian Error Linear Unit)** is defined mathematically as:\n",
        "$$\\text{GELU}(x) = x \\cdot \\Phi(x)$$\n",
        "where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution.\n",
        "\n",
        "**Approximation used in practice:**\n",
        "$$\\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\right)\\right)$$\n",
        "\n",
        "**Properties of GELU:**\n",
        "- **Smooth**: Unlike ReLU, GELU is differentiable everywhere\n",
        "- **Non-monotonic**: Has a slight negative region for small negative inputs\n",
        "- **Probabilistic**: Based on probability theory rather than hard thresholding\n",
        "- **Better gradients**: Smoother gradients compared to ReLU variants\n",
        "\n",
        "**Comparison with other activations:**\n",
        "```python\n",
        "# ReLU: max(0, x) - hard cutoff at zero\n",
        "# GELU: x * Φ(x) - smooth probabilistic gating\n",
        "# Swish: x * sigmoid(x) - similar smooth properties\n",
        "```\n",
        "\n",
        "##### Why 4x Expansion?\n",
        "\n",
        "**Computational Capacity:**\n",
        "The 4x expansion provides sufficient representational capacity for complex transformations while maintaining computational efficiency.\n",
        "\n",
        "**Parameter Distribution:**\n",
        "```python\n",
        "# Example with embedding_dim = 768\n",
        "# Layer 1: 768 → 3072 (768 × 4) = 2,359,296 parameters\n",
        "# Layer 2: 3072 → 768 = 2,359,296 parameters  \n",
        "# Total: ~4.7M parameters per feed-forward block\n",
        "```\n",
        "\n",
        "**Information Processing:**\n",
        "- **Expansion phase**: Projects to higher-dimensional space for complex pattern recognition\n",
        "- **Contraction phase**: Compresses back to original dimension while preserving learned features\n",
        "\n",
        "##### Position-wise Processing\n",
        "\n",
        "**Independent Processing:**\n",
        "Each position in the sequence is processed independently - the same transformation is applied to every token position without interaction between positions.\n",
        "\n",
        "**Mathematical representation:**\n",
        "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
        "(using ReLU in original paper, but GELU in modern implementations)\n",
        "\n",
        "##### Role in Transformer Architecture\n",
        "\n",
        "**Complementary to Attention:**\n",
        "- **Attention**: Models relationships between positions\n",
        "- **Feed-forward**: Processes individual positions with non-linear transformations\n",
        "\n",
        "**Residual Connection Context:**\n",
        "The feed-forward output is typically added to its input via residual connections:\n",
        "```python\n",
        "# In transformer block:\n",
        "x = x + attention(x)\n",
        "x = x + feedforward(x)  # This module\n",
        "```\n",
        "\n",
        "**Learning Capacity:**\n",
        "The feed-forward network often contains the majority of parameters in a transformer model, providing substantial learning capacity for pattern recognition and feature transformation.\n",
        "\n",
        "This component is essential for transformers to learn complex non-linear transformations while maintaining the ability to process variable-length sequences efficiently.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\"\n",
        "  Position-wise feed-forward network used in transformer blocks.\n",
        "  \n",
        "  This module implements a two-layer fully connected network with GELU activation\n",
        "  that processes each position in the sequence independently. It expands the \n",
        "  embedding dimension by a factor of 4, applies non-linear activation, then\n",
        "  projects back to the original dimension.\n",
        "  \n",
        "  The architecture follows the standard transformer design:\n",
        "  embedding_dim → 4×embedding_dim → embedding_dim\n",
        "  \n",
        "  Args:\n",
        "      config (dict): Configuration dictionary containing:\n",
        "          - embedding_dim (int): The input/output dimension size\n",
        "          - dropout_rate (float): Dropout probability for regularization\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_layers = nn.Sequential(\n",
        "        nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
        "        nn.Dropout(config[\"dropout_rate\"])\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.linear_layers(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ff = FeedForward(config)\n",
        "\n",
        "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
        "output = ff(input)\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Transformer Block: Complete Processing Unit\n",
        "\n",
        "###### Core Architecture\n",
        "\n",
        "The `Block` class implements a complete transformer layer that combines self-attention and feed-forward processing with residual connections and layer normalization. This represents one layer of a multi-layer transformer architecture.\n",
        "\n",
        "###### Component Structure\n",
        "\n",
        "**Two Main Sub-modules:**\n",
        "```python\n",
        "self.multi_head = MultiHeadAttention(config)    # Attention mechanism\n",
        "self.feed_forward = FeedForward(config)         # Position-wise processing\n",
        "```\n",
        "\n",
        "**Normalization Layers:**\n",
        "```python\n",
        "self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])  # Pre-attention norm\n",
        "self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])  # Pre-feedforward norm\n",
        "```\n",
        "\n",
        "###### Pre-Norm Architecture\n",
        "\n",
        "The implementation uses **Pre-LayerNorm** architecture (also called Pre-LN), where normalization is applied before each sub-module rather than after:\n",
        "\n",
        "**Pre-Norm Flow:**\n",
        "```python\n",
        "# Attention sub-layer\n",
        "residual = input\n",
        "x = self.multi_head(self.layer_norm_1(input))  # Norm → Attention\n",
        "x = x + residual                               # Add residual\n",
        "\n",
        "# Feed-forward sub-layer  \n",
        "residual = x\n",
        "x = self.feed_forward(self.layer_norm_2(x))    # Norm → FFN\n",
        "return x + residual                            # Add residual\n",
        "```\n",
        "\n",
        "**vs Post-Norm (original transformer):**\n",
        "```python\n",
        "# Would be: x = self.layer_norm_1(x + self.multi_head(x))\n",
        "# Would be: x = self.layer_norm_2(x + self.feed_forward(x))\n",
        "```\n",
        "\n",
        "###### Forward Pass Breakdown\n",
        "\n",
        "**Step 1: Attention Processing**\n",
        "```python\n",
        "residual = input                                    # Store original input\n",
        "x = self.multi_head(self.layer_norm_1(input))     # Normalize → Attention\n",
        "x = x + residual                                   # Add residual connection\n",
        "```\n",
        "\n",
        "- Input shape: `(batch_size, sequence_length, embedding_dim)`\n",
        "- LayerNorm normalizes across embedding dimension for each token\n",
        "- Multi-head attention processes normalized input\n",
        "- Residual connection preserves original information\n",
        "\n",
        "**Step 2: Feed-Forward Processing**\n",
        "```python\n",
        "residual = x                                       # Store attention output\n",
        "x = self.feed_forward(self.layer_norm_2(x))      # Normalize → FFN\n",
        "return x + residual                               # Add residual connection\n",
        "```\n",
        "\n",
        "- Takes attention output as input\n",
        "- Applies second normalization layer\n",
        "- Feed-forward network processes each position independently\n",
        "- Final residual connection completes the block\n",
        "\n",
        "###### Layer Normalization Details\n",
        "\n",
        "**LayerNorm Operation:**\n",
        "For each token position, normalizes across the embedding dimension:\n",
        "$$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta$$\n",
        "\n",
        "Where:\n",
        "- $\\mu$ = mean across embedding dimension\n",
        "- $\\sigma$ = standard deviation across embedding dimension\n",
        "- $\\gamma, \\beta$ = learnable scale and shift parameters\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# For one token with embedding_dim=768\n",
        "token_embedding = [0.1, 0.5, -0.2, ..., 0.3]  # 768 values\n",
        "# LayerNorm computes mean and std of these 768 values\n",
        "# Then normalizes: (value - mean) / std\n",
        "```\n",
        "\n",
        "###### Residual Connections\n",
        "\n",
        "**Purpose:**\n",
        "- Enables gradient flow through deep networks\n",
        "- Allows model to learn identity mappings when needed\n",
        "- Provides multiple paths for information flow\n",
        "\n",
        "**Mathematical Effect:**\n",
        "Each sub-layer computes: $\\text{output} = x + \\text{SubLayer}(x)$\n",
        "\n",
        "This means the sub-layer only needs to learn the \"change\" or \"refinement\" to apply to the input, rather than reconstructing the entire representation.\n",
        "\n",
        "###### Why Pre-Norm Architecture?\n",
        "\n",
        "**Training Stability:**\n",
        "- Better gradient flow during training\n",
        "- Reduced gradient exploding/vanishing problems\n",
        "- More stable training in deeper models\n",
        "\n",
        "**Normalization Benefits:**\n",
        "- Normalizes inputs to sub-layers rather than outputs\n",
        "- Ensures sub-layers receive well-conditioned inputs\n",
        "- Often converges faster than Post-Norm\n",
        "\n",
        "###### Information Flow Through Block\n",
        "\n",
        "**Multi-Path Processing:**\n",
        "1. **Attention path**: Models relationships between tokens\n",
        "2. **Feed-forward path**: Processes individual token representations\n",
        "3. **Residual paths**: Preserve original information at each step\n",
        "\n",
        "**Cumulative Effect:**\n",
        "Each block refines the representation while preserving previous learning through residual connections, allowing the model to build increasingly sophisticated representations layer by layer.\n",
        "\n",
        "This block design is the fundamental building unit of transformer architectures, with models typically stacking 6-96 such blocks to create powerful language models.\n",
        "\n",
        "\n",
        "##### Data Flow Diagram\n",
        "\n",
        "The data flow for the Pre-LayerNorm architecture implemented in the code. Here's the diagram:\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[\"Input\"] --> B[\"LayerNorm 1\"]\n",
        "    B --> C[\"MultiHeadAttention\"]\n",
        "    C --> D[\"Add (Residual 1)\"]\n",
        "    A --> D\n",
        "    D --> E[\"LayerNorm 2\"]\n",
        "    E --> F[\"FeedForward\"]\n",
        "    F --> G[\"Add (Residual 2)\"]\n",
        "    D --> G\n",
        "    G --> H[\"Output\"]\n",
        "    \n",
        "    style A fill:#F1F8E9\n",
        "    style B fill:#DCEDC8\n",
        "    style C fill:#C5E1A5\n",
        "    style D fill:#AED581\n",
        "    style E fill:#9CCC65\n",
        "    style F fill:#8BC34A\n",
        "    style G fill:#7CB342\n",
        "    style H fill:#CDDC39\n",
        "```\n",
        "\n",
        "**Data Flow Verification:**\n",
        "\n",
        "1. **Input** → **LayerNorm 1** → **MultiHeadAttention** → **Add (with input)** \n",
        "2. **Result** → **LayerNorm 2** → **FeedForward** → **Add (with previous result)** → **Output**\n",
        "\n",
        "This matches exactly with the code implementation:\n",
        "\n",
        "```python\n",
        "# First sub-layer\n",
        "residual = input\n",
        "x = self.multi_head(self.layer_norm_1(input))  # LayerNorm → Attention\n",
        "x = x + residual                               # Add residual\n",
        "\n",
        "# Second sub-layer  \n",
        "residual = x\n",
        "x = self.feed_forward(self.layer_norm_2(x))    # LayerNorm → FFN\n",
        "return x + residual                            # Add residual\n",
        "```\n",
        "\n",
        "The diagram shows the Pre-LayerNorm architecture where normalization occurs before each sub-module, with residual connections bypassing the normalized paths.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    A single Transformer block, which is a fundamental building block of a Transformer model.\n",
        "\n",
        "    This module encapsulates two main sub-layers:\n",
        "    1. A Multi-Head Self-Attention mechanism.\n",
        "    2. A position-wise Feed-Forward Network.\n",
        "\n",
        "    Each sub-layer is followed by a residual connection and layer normalization,\n",
        "    a technique often referred to as \"Pre-LN\" (pre-layer normalization). This\n",
        "    structure helps stabilize training and allows for deeper models.\n",
        "\n",
        "    The data flow is as follows:\n",
        "    input -> LayerNorm -> MultiHeadAttention -> Add -> LayerNorm -> FeedForward -> Add -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initializes the Transformer Block.\n",
        "\n",
        "        Args:\n",
        "            config (dict): A configuration dictionary containing parameters for\n",
        "                the sub-modules, such as \"embedding_dim\", \"heads_num\", etc.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # The first sub-layer: Multi-Head Attention.\n",
        "        self.multi_head = MultiHeadAttention(config)\n",
        "        # Layer normalization applied *before* the attention mechanism.\n",
        "        self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
        "\n",
        "        # The second sub-layer: a simple Feed-Forward Network.\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        # Layer normalization applied *before* the feed-forward network.\n",
        "        self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the Transformer block.\n",
        "\n",
        "        Args:\n",
        "            input_tensor (torch.Tensor): The input tensor of shape (B, T, E),\n",
        "                where B is batch size, T is sequence length, and E is embedding dim.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor with the same shape as the input.\n",
        "        \"\"\"\n",
        "        # --- First Sub-layer: Multi-Head Attention with Add & Norm ---\n",
        "        \n",
        "        # Store the original input for the first residual connection.\n",
        "        residual = input_tensor\n",
        "        \n",
        "        # Apply layer normalization, then the multi-head attention.\n",
        "        x = self.multi_head(self.layer_norm_1(input_tensor))\n",
        "        \n",
        "        # Add the residual connection. This allows the model to bypass the\n",
        "        # sub-layer if needed, aiding gradient flow.\n",
        "        x = x + residual\n",
        "\n",
        "        # --- Second Sub-layer: Feed-Forward Network with Add & Norm ---\n",
        "        \n",
        "        # Store the output of the first sub-layer for the second residual connection.\n",
        "        residual = x\n",
        "        \n",
        "        # Apply layer normalization, then the feed-forward network.\n",
        "        x = self.feed_forward(self.layer_norm_2(x))\n",
        "        \n",
        "        # Add the second residual connection.\n",
        "        return x + residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b = Block(config)\n",
        "\n",
        "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
        "ouptut = b(input)\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Complete GPT Model Architecture\n",
        "\n",
        "###### Model Overview\n",
        "\n",
        "The `DemoGPT` class implements a complete transformer-based language model following the GPT (Generative Pre-trained Transformer) architecture. It combines token embeddings, positional embeddings, multiple transformer blocks, and an output projection to perform next-token prediction.\n",
        "\n",
        "###### Component Breakdown\n",
        "\n",
        "**Embedding Layers:**\n",
        "```python\n",
        "self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
        "self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
        "```\n",
        "\n",
        "- **Token embeddings**: Map each token ID to a dense vector representation\n",
        "- **Positional embeddings**: Add position-specific information to distinguish token order\n",
        "\n",
        "**Transformer Stack:**\n",
        "```python\n",
        "blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
        "self.layers = nn.Sequential(*blocks)\n",
        "```\n",
        "\n",
        "Creates a stack of transformer blocks (typically 10-96 layers) for deep processing.\n",
        "\n",
        "**Output Processing:**\n",
        "```python\n",
        "self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
        "self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
        "```\n",
        "\n",
        "- **Final layer norm**: Stabilizes the output representations\n",
        "- **Unembedding**: Projects back to vocabulary space for token prediction\n",
        "\n",
        "###### Forward Pass Analysis\n",
        "\n",
        "**Step 1: Token Embedding**\n",
        "```python\n",
        "x = self.token_embedding_layer(token_ids)\n",
        "```\n",
        "- Input: `(batch_size, sequence_length)` of token IDs\n",
        "- Output: `(batch_size, sequence_length, embedding_dim)` of dense vectors\n",
        "\n",
        "**Step 2: Positional Embedding Addition**\n",
        "```python\n",
        "sequence = torch.arange(tokens_num, device=device)\n",
        "x = x + self.positional_embedding_layer(sequence)\n",
        "```\n",
        "\n",
        "**Issue Alert**: The code has a bug - `device` is not defined in the method scope. Should be:\n",
        "```python\n",
        "sequence = torch.arange(tokens_num, device=token_ids.device)\n",
        "```\n",
        "\n",
        "**Mathematical Operation:**\n",
        "Each position gets both content and positional information:\n",
        "$$\\text{input\\_to\\_blocks} = \\text{TokenEmb}(\\text{token\\_ids}) + \\text{PosEmb}(\\text{positions})$$\n",
        "\n",
        "**Step 3: Transformer Processing**\n",
        "```python\n",
        "x = self.layers(x)\n",
        "```\n",
        "Passes through all transformer blocks sequentially, with each block applying attention and feed-forward transformations.\n",
        "\n",
        "**Step 4: Output Normalization**\n",
        "```python\n",
        "x = self.layer_norm(x)\n",
        "```\n",
        "Final layer normalization ensures stable representations before output projection.\n",
        "\n",
        "**Step 5: Vocabulary Projection**\n",
        "```python\n",
        "x = self.unembedding(x)\n",
        "```\n",
        "- Maps from embedding space back to vocabulary space\n",
        "- Output: `(batch_size, sequence_length, vocabulary_size)`\n",
        "- Each position gets a probability distribution over all possible next tokens\n",
        "\n",
        "###### Key Architecture Decisions\n",
        "\n",
        "**Parameter Sharing:**\n",
        "The model uses separate embedding matrices for tokens and positions, allowing independent learning of semantic and positional representations.\n",
        "\n",
        "**No Bias in Final Layer:**\n",
        "```python\n",
        "bias=False\n",
        "```\n",
        "Common practice in modern language models to reduce parameters and improve training dynamics.\n",
        "\n",
        "**Additive Positional Encoding:**\n",
        "Uses learned positional embeddings added to token embeddings, rather than the sinusoidal encodings from the original transformer paper.\n",
        "\n",
        "###### Dimensional Flow Example\n",
        "\n",
        "```python\n",
        "# Example with config: vocabulary_size=50000, embedding_dim=768, context_size=1024\n",
        "# Input token_ids: (32, 512)  # batch_size=32, sequence_length=512\n",
        "\n",
        "# After token embedding: (32, 512, 768)\n",
        "# After positional embedding addition: (32, 512, 768)\n",
        "# After transformer blocks: (32, 512, 768)\n",
        "# After final layer norm: (32, 512, 768)  \n",
        "# After unembedding: (32, 512, 50000)  # Logits for each vocabulary token\n",
        "```\n",
        "\n",
        "###### Training Usage\n",
        "\n",
        "During training, the output logits are used with cross-entropy loss:\n",
        "```python\n",
        "# model output: (batch_size, sequence_length, vocab_size)\n",
        "# targets: (batch_size, sequence_length)\n",
        "loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
        "```\n",
        "\n",
        "###### Generation Usage\n",
        "\n",
        "For text generation, the model predicts one token at a time:\n",
        "```python\n",
        "# Get logits for last position\n",
        "next_token_logits = model(input_ids)[:, -1, :]  # (batch_size, vocab_size)\n",
        "# Sample or take argmax to get next token\n",
        "next_token = torch.multinomial(torch.softmax(next_token_logits, dim=-1), 1)\n",
        "```\n",
        "\n",
        "This architecture represents a complete autoregressive language model capable of learning complex language patterns and generating coherent text through next-token prediction.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DemoGPT(nn.Module):\n",
        "  \"\"\"\n",
        "  A complete, simplified implementation of a GPT-style transformer model.\n",
        "\n",
        "  This class brings together all the necessary components:\n",
        "  1. Token and Positional Embeddings to create the initial input representation.\n",
        "  2. A stack of Transformer `Block`s to perform the core processing.\n",
        "  3. A final Layer Normalization and a linear layer (unembedding) to project\n",
        "     the output back into the vocabulary space to get logits for the next token.\n",
        "\n",
        "  The model is designed for auto-regressive language generation, predicting the\n",
        "  next token in a sequence given the previous ones.\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    \"\"\"\n",
        "    Initializes the DemoGPT model architecture.\n",
        "\n",
        "    Args:\n",
        "        config (dict): A configuration dictionary containing model hyperparameters:\n",
        "            - \"vocabulary_size\" (int): The number of unique tokens in the tokenizer.\n",
        "            - \"embedding_dim\" (int): The dimensionality of the token and positional embeddings.\n",
        "            - \"context_size\" (int): The maximum sequence length the model can handle.\n",
        "            - \"layers_num\" (int): The number of Transformer `Block`s to stack.\n",
        "            - Other keys required by the `Block` class.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Token embedding layer: maps each token ID to a dense vector.\n",
        "    self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
        "    # Positional embedding layer: maps each position index (0 to context_size-1) to a vector.\n",
        "    self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
        "\n",
        "    # Create a stack of Transformer Blocks.\n",
        "    # `nn.Sequential` chains the blocks together, so the output of one is the input to the next.\n",
        "    blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
        "    self.layers = nn.Sequential(*blocks)\n",
        "\n",
        "    # A final layer normalization applied after the transformer blocks.\n",
        "    self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
        "    # The final linear layer (unembedding) that projects the model's output\n",
        "    # back to the vocabulary size to get the logits for each token.\n",
        "    self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
        "\n",
        "  def forward(self, token_ids):\n",
        "    \"\"\"\n",
        "    Performs the forward pass of the DemoGPT model.\n",
        "\n",
        "    Args:\n",
        "        token_ids (torch.Tensor): A tensor of shape (B, T) containing the\n",
        "            input token IDs, where B is batch size and T is sequence length.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The output logits tensor of shape (B, T, V), where V is\n",
        "            the vocabulary size.\n",
        "    \"\"\"\n",
        "    batch_size, tokens_num = token_ids.shape\n",
        "\n",
        "    # 1. Get token embeddings for the input IDs. Shape: (B, T, E)\n",
        "    token_embeddings = self.token_embedding_layer(token_ids)\n",
        "    \n",
        "    # 2. Get positional embeddings for each position in the sequence.\n",
        "    # `torch.arange` creates a sequence of position indices [0, 1, ..., T-1].\n",
        "    sequence = torch.arange(tokens_num, device=token_ids.device)\n",
        "    positional_embeddings = self.positional_embedding_layer(sequence) # Shape: (T, E)\n",
        "    \n",
        "    # 3. Add token and positional embeddings. Broadcasting adds the positional\n",
        "    # embeddings to each sequence in the batch. Shape: (B, T, E)\n",
        "    x = token_embeddings + positional_embeddings\n",
        "\n",
        "    # 4. Pass the combined embeddings through the stack of Transformer blocks.\n",
        "    x = self.layers(x)\n",
        "    \n",
        "    # 5. Apply the final layer normalization.\n",
        "    x = self.layer_norm(x)\n",
        "    \n",
        "    # 6. Project the final hidden states to logits over the vocabulary.\n",
        "    logits = self.unembedding(x) # Shape: (B, T, V)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = DemoGPT(config).to(device)\n",
        "\n",
        "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))\n",
        "\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Autoregressive Text Generation Function\n",
        "\n",
        "###### Purpose and Functionality\n",
        "\n",
        "The `generate` function implements autoregressive text generation for language models, producing new tokens one at a time by sampling from the model's predicted probability distributions. This is the standard approach for generating coherent text sequences from transformer-based language models.\n",
        "\n",
        "###### Function Parameters\n",
        "\n",
        "```python\n",
        "def generate(model, prompt_ids, max_tokens):\n",
        "```\n",
        "\n",
        "- **model**: The trained GPT model instance\n",
        "- **prompt_ids**: Initial token sequence to start generation (shape: `(1, prompt_length)`)\n",
        "- **max_tokens**: Maximum number of new tokens to generate\n",
        "\n",
        "###### Step-by-Step Generation Process\n",
        "\n",
        "**Initialization:**\n",
        "```python\n",
        "output_ids = prompt_ids\n",
        "```\n",
        "Starts with the provided prompt as the foundation for generation.\n",
        "\n",
        "**Generation Loop:**\n",
        "```python\n",
        "for _ in range(max_tokens):\n",
        "```\n",
        "Iteratively generates tokens up to the specified maximum.\n",
        "\n",
        "**Context Length Check:**\n",
        "```python\n",
        "if output_ids.shape[1] >= config[\"context_size\"]:\n",
        "    break\n",
        "```\n",
        "Prevents exceeding the model's maximum context window (e.g., 1024 tokens).\n",
        "\n",
        "**Forward Pass (No Gradients):**\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    logits = model(output_ids)\n",
        "```\n",
        "- Disables gradient computation for efficiency during inference\n",
        "- Gets model predictions for all positions in the sequence\n",
        "- Output shape: `(batch_size, sequence_length, vocabulary_size)`\n",
        "\n",
        "**Next Token Prediction:**\n",
        "```python\n",
        "logits = logits[:, -1, :]  # Extract last position logits\n",
        "probs = F.softmax(logits, dim=-1)  # Convert to probabilities\n",
        "```\n",
        "- Extracts logits for the last position only (next token prediction)\n",
        "- Applies softmax to convert raw logits to probability distribution\n",
        "- Result shape: `(batch_size, vocabulary_size)`\n",
        "\n",
        "**Sampling Strategy:**\n",
        "```python\n",
        "next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "```\n",
        "Uses multinomial sampling to select next token based on probability distribution rather than always choosing the highest probability token (greedy decoding).\n",
        "\n",
        "**Sequence Extension:**\n",
        "```python\n",
        "output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
        "```\n",
        "Appends the newly generated token to the existing sequence for the next iteration.\n",
        "\n",
        "###### Sampling vs. Greedy Decoding\n",
        "\n",
        "**Multinomial Sampling (Used Here):**\n",
        "- Introduces randomness and diversity in generation\n",
        "- Tokens with higher probability are more likely to be selected\n",
        "- Produces more creative and varied outputs\n",
        "- Can occasionally select lower-probability but contextually interesting tokens\n",
        "\n",
        "**Greedy Decoding (Alternative):**\n",
        "```python\n",
        "next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "```\n",
        "- Always selects the highest probability token\n",
        "- Deterministic output (same input always produces same output)\n",
        "- Often leads to repetitive or predictable text\n",
        "\n",
        "###### Generation Example Flow\n",
        "\n",
        "```python\n",
        "# Initial prompt: \"The cat sat on the\"\n",
        "# prompt_ids: [464, 3857, 3332, 319, 262]  # Token IDs\n",
        "\n",
        "# Iteration 1:\n",
        "# Model predicts probabilities: [mat: 0.4, chair: 0.3, floor: 0.2, ...]\n",
        "# Sample → \"mat\" (token_id: 2603)\n",
        "# output_ids: [464, 3857, 3332, 319, 262, 2603]\n",
        "\n",
        "# Iteration 2:\n",
        "# Model sees \"The cat sat on the mat\"\n",
        "# Predicts next token probabilities: [and: 0.5, while: 0.2, .: 0.15, ...]\n",
        "# Sample → \"and\" (token_id: 290)\n",
        "# output_ids: [464, 3857, 3332, 319, 262, 2603, 290]\n",
        "```\n",
        "\n",
        "###### Key Design Considerations\n",
        "\n",
        "**Memory Efficiency:**\n",
        "Using `torch.no_grad()` prevents unnecessary gradient computation and memory allocation during inference.\n",
        "\n",
        "**Context Management:**\n",
        "The function respects the model's context length limit, preventing out-of-bounds errors.\n",
        "\n",
        "**Stochastic Generation:**\n",
        "Multinomial sampling introduces controlled randomness, balancing coherence with creativity.\n",
        "\n",
        "**Incremental Processing:**\n",
        "Each iteration processes the entire sequence, allowing the model to consider full context when predicting the next token.\n",
        "\n",
        "This generation approach forms the foundation for interactive chatbots, creative writing assistants, and other applications requiring coherent text continuation from language models.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(model, prompt_ids, max_tokens_to_generate, config):\n",
        "    \"\"\"\n",
        "    Generates a sequence of tokens auto-regressively from a given prompt.\n",
        "\n",
        "    This function takes a trained model and a starting sequence of token IDs\n",
        "    (the prompt) and generates new tokens one by one. In each step, it uses\n",
        "    the model to predict the next token, samples from the resulting probability\n",
        "    distribution, and appends the new token to the sequence, which then becomes\n",
        "    the input for the next step.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained DemoGPT transformer model.\n",
        "        prompt_ids (torch.Tensor): A tensor of shape (B, T) containing the\n",
        "            initial token IDs to start generation from. B is the batch size\n",
        "            (usually 1 for generation) and T is the length of the prompt.\n",
        "        max_tokens_to_generate (int): The maximum number of new tokens to generate after the prompt.\n",
        "        config (dict): The model's configuration dictionary, used to access\n",
        "            the `context_size`.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of shape (B, T + generated_tokens) containing\n",
        "            the original prompt plus the newly generated tokens.\n",
        "    \"\"\"\n",
        "    # Start with the initial prompt.\n",
        "    output_ids = prompt_ids\n",
        "    \n",
        "    # Loop to generate tokens one by one.\n",
        "    for _ in range(max_tokens_to_generate):\n",
        "      # Stop if the context window is full.\n",
        "      if output_ids.shape[1] >= config[\"context_size\"]:\n",
        "        break\n",
        "        \n",
        "      # Use torch.no_grad() to disable gradient calculations, as we are only\n",
        "      # doing inference, which saves memory and computation.\n",
        "      with torch.no_grad():\n",
        "        # Get the model's predictions (logits) for the current sequence.\n",
        "        logits = model(output_ids)\n",
        "\n",
        "      # Focus only on the logits for the very last token in the sequence,\n",
        "      # as that's the prediction for the *next* token.\n",
        "      last_token_logits = logits[:, -1, :]\n",
        "      \n",
        "      # Apply softmax to convert the logits into a probability distribution.\n",
        "      probs = F.softmax(last_token_logits, dim=-1)\n",
        "      \n",
        "      # Sample one token from the probability distribution.\n",
        "      # `torch.multinomial` treats the input as a set of weights for sampling.\n",
        "      next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "      \n",
        "      # Append the newly sampled token ID to our sequence.\n",
        "      output_ids = torch.cat([output_ids, next_token_id], dim=1)\n",
        "      \n",
        "    return output_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
        "#   model.eval()\n",
        "\n",
        "#   prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
        "\n",
        "#   return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])\n",
        "\n",
        "\n",
        "def generate_with_prompt(model, tokenizer, config, prompt, max_tokens_to_generate=100):\n",
        "  \"\"\"\n",
        "  Generates text from a prompt using the specified model and tokenizer.\n",
        "\n",
        "  This function sets the model to evaluation mode, encodes the prompt, calls\n",
        "  the `generate` function to produce token IDs, and decodes them back into\n",
        "  human-readable text.\n",
        "\n",
        "  Args:\n",
        "      model (nn.Module): The trained transformer model.\n",
        "      tokenizer (CharTokenizer): The tokenizer for encoding/decoding text.\n",
        "      config (dict): The model's configuration dictionary.\n",
        "      prompt (str): The initial text to start generation from.\n",
        "      max_tokens_to_generate (int): The maximum number of new tokens to create.\n",
        "\n",
        "  Returns:\n",
        "      str: The generated text, including the original prompt.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "\n",
        "  prompt_ids = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
        "\n",
        "  # Call the generate function with the correct arguments\n",
        "  generated_ids = generate(\n",
        "      model,\n",
        "      prompt_ids,\n",
        "      max_tokens_to_generate=max_tokens_to_generate,\n",
        "      config=config\n",
        "  )\n",
        "\n",
        "  return tokenizer.decode(generated_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")\n",
        "\n",
        "generate_with_prompt(model, tokenizer, config, \"First Citizen:\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# train_iterations = 5000\n",
        "train_iterations = 20\n",
        "evaluation_interval = 100\n",
        "learning_rate=4e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = tokenizer.encode(text).to(device)\n",
        "\n",
        "train_dataset = TokenIdsDataset(train_data, config[\"context_size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "\n",
        "train_sampler = RandomSampler(train_dataset, num_samples=batch_size * train_iterations, replacement=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "validation_sampler = RandomSampler(validation_dataset, replacement=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, sampler=validation_sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute validation loss for the model using \"batches_num\" batches\n",
        "# from the validation data loader\n",
        "@torch.no_grad()\n",
        "def calculate_validation_loss(model, batches_num):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  validation_iter = iter(validation_dataloader)\n",
        "\n",
        "  for _ in range(batches_num):\n",
        "    input, targets = next(validation_iter)\n",
        "    logits = model(input)\n",
        "\n",
        "    logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
        "    targets_view = targets.view(batch_size * config[\"context_size\"])\n",
        "      \n",
        "    loss = F.cross_entropy(logits_view, targets_view)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  average_loss = total_loss / batches_num\n",
        "\n",
        "  return average_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import display, clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "%matplotlib inline\n",
        "\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "display(plot_output)\n",
        "\n",
        "def update_plot(train_losses, train_steps, validation_losses, validation_steps):\n",
        "\n",
        "  with plot_output:\n",
        "    clear_output(wait=True)  # Clear only the plot output, not the text\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.plot(train_steps, train_losses, label='Training Loss')\n",
        "    plt.plot(validation_steps, validation_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(loc='center left')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for step_num, sample in enumerate(train_dataloader):\n",
        "\n",
        "#     model.train()\n",
        "#     input, targets = sample\n",
        "#     logits = model(input)\n",
        "\n",
        "#     logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
        "#     targets_view = targets.view(batch_size * config[\"context_size\"])\n",
        "    \n",
        "#     loss = F.cross_entropy(logits_view, targets_view)\n",
        "#     # Backward propagation\n",
        "#     loss.backward()\n",
        "#     # Update model parameters\n",
        "#     optimizer.step()\n",
        "#     # Set to None to reduce memory usage\n",
        "#     optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "#     print(f\"Step {step_num}. Loss {loss.item():.3f}\")\n",
        "\n",
        "#     if step_num % evaluation_interval == 0:\n",
        "#       print(\"Demo GPT:\\n\" + generate_with_prompt(model, tokenizer, config, \"\\n\"))\n",
        "\n",
        "\n",
        "# Set up lists to store losses for plotting\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "eval_losses = []\n",
        "eval_steps = []\n",
        "\n",
        "\n",
        "for step_num, sample in enumerate(train_dataloader):\n",
        "\n",
        "  model.train()\n",
        "  input, targets = sample\n",
        "  logits = model(input)\n",
        "\n",
        "  logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
        "  targets_view = targets.view(batch_size * config[\"context_size\"])\n",
        "  \n",
        "  loss = F.cross_entropy(logits_view, targets_view)\n",
        "  # Backward propagation\n",
        "  loss.backward()\n",
        "  # Update model parameters\n",
        "  optimizer.step()\n",
        "  # Set to None to reduce memory usage\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  train_losses.append(loss.item())\n",
        "  train_steps.append(step_num)\n",
        "\n",
        "  print(f\"Step {step_num}. Loss {loss.item():.3f}\")\n",
        "\n",
        "  if step_num % evaluation_interval == 0:\n",
        "    print(\"Demo GPT:\\n\" + generate_with_prompt(model, tokenizer, config, \"\\n\"))\n",
        "\n",
        "    validation_loss = calculate_validation_loss(model, batches_num=10)\n",
        "    eval_losses.append(validation_loss)\n",
        "    eval_steps.append(step_num)\n",
        "\n",
        "    print(f\"Step {step_num}. Validation loss: {validation_loss:.3f}\")\n",
        "\n",
        "\n",
        "  update_plot(train_losses, train_steps, eval_losses, eval_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "In this exercise, your goal is to implement a `Dataset` class in PyTorch, which is used to handle and process the data for training a transformer model. You'll also use a `DataLoader` to load batches of data from this dataset. Follow the steps below to complete the implementation based on the provided starter code.\n",
        "\n",
        "The starter code includes the tokenizer that we've implemented in previous videos. You will need to create an instance of this tokenizer using the training dataset.\n",
        "\n",
        "To implement the exercise go over the code and implement the missing parts marked with the `TODO` comment.\n",
        "\n",
        "**1. Define the `TokenIdsDataset` Class:**\n",
        "\n",
        "For the first step, you would need to implement the missing methods in the `TokenIdsDataset` class.\n",
        "\n",
        "- **`__init__` Method:**\n",
        "    - Initialize the class with `data` and `block_size`.\n",
        "    - Save `data` and `block_size` as instance variables.\n",
        "\n",
        "- **`__len__` Method:**\n",
        "    - Compute the size of the dataset. If every position in the data can be the start of an item, the length of the dataset should be less than the `len(data)`.\n",
        "\n",
        "- **`__getitem__` Method:**\n",
        "    - Validate the input position to ensure it is within a valid range.\n",
        "    - Retrieve an item starting from position `pos` up to `pos + block_size`.\n",
        "    - Retrieve a target item that is the same as the item but has been shifted by one position.\n",
        "    - Return both the input item and the target item.\n",
        "\n",
        "**2. Tokenize the Text:**\n",
        "\n",
        "Create a tokenizer and encode data from the training dataset. Then, create an instance of the `TokenIdsDataset` for the training data.\n",
        "\n",
        "**3. Retrieve the First Item from the Dataset**\n",
        "\n",
        "Get the first item from the dataset and decode it using the tokenizer. If everything is implemented correctly, you should get the first 64 characters of the training dataset.\n",
        "\n",
        "**4. Use a DataLoader:**\n",
        "\n",
        "Now, try using the `DataLoader` with the training dataset we've created. The `DataLoader` here is created with a `RandomSampler` to randomize the items we get from the training dataset.\n",
        "\n",
        "For this exercise, first, get a single training batch from the `DataLoader` we've created.\n",
        "\n",
        "Then, we decode input and target tokens using the tokenizer we've created. The input and target should be from the same part of the training dataset but shifted by one character.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (ml)",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
