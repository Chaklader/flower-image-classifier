{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from get_device import get_device\n",
    "\n",
    "# Use CUDA if available\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('../../data/tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 256,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "    self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, tokens_num, embedding_dim = input.shape\n",
    "    Q = self.Q_weights(input)\n",
    "    K = self.K_weights(input)\n",
    "    V = self.V_weights(input)\n",
    "\n",
    "    attention_scores = Q @ K.transpose(1, 2)\n",
    "    attention_scores = attention_scores.masked_fill(\n",
    "        self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "        -torch.inf\n",
    "    )\n",
    "    attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = AttentionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ah(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "    self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "    self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    # print(f\"Input shape: {input.shape}\")\n",
    "    heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "    scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "    # print(f\"heads shape: {scores_change.shape}\")\n",
    "\n",
    "    scores_change = self.linear(scores_change)\n",
    "    return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
    "        nn.Dropout(config[\"dropout_rate\"])\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ff(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multi_head = MultiHeadAttention(config)\n",
    "    self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    self.feed_forward = FeedForward(config)\n",
    "    self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    residual = input\n",
    "    x = self.multi_head(self.layer_norm_1(input))\n",
    "    x = x + residual\n",
    "\n",
    "    residual = x\n",
    "    x = self.feed_forward(self.layer_norm_2(x))\n",
    "    return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = b(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Complete GPT Model Architecture\n",
    "\n",
    "###### Model Overview\n",
    "\n",
    "The `DemoGPT` class implements a complete transformer-based language model following the GPT (Generative Pre-trained Transformer) architecture. It combines token embeddings, positional embeddings, multiple transformer blocks, and an output projection to perform next-token prediction.\n",
    "\n",
    "###### Component Breakdown\n",
    "\n",
    "**Embedding Layers:**\n",
    "```python\n",
    "self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
    "self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
    "```\n",
    "\n",
    "- **Token embeddings**: Map each token ID to a dense vector representation\n",
    "- **Positional embeddings**: Add position-specific information to distinguish token order\n",
    "\n",
    "**Transformer Stack:**\n",
    "```python\n",
    "blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "self.layers = nn.Sequential(*blocks)\n",
    "```\n",
    "\n",
    "Creates a stack of transformer blocks (typically 10-96 layers) for deep processing.\n",
    "\n",
    "**Output Processing:**\n",
    "```python\n",
    "self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
    "```\n",
    "\n",
    "- **Final layer norm**: Stabilizes the output representations\n",
    "- **Unembedding**: Projects back to vocabulary space for token prediction\n",
    "\n",
    "###### Forward Pass Analysis\n",
    "\n",
    "**Step 1: Token Embedding**\n",
    "```python\n",
    "x = self.token_embedding_layer(token_ids)\n",
    "```\n",
    "- Input: `(batch_size, sequence_length)` of token IDs\n",
    "- Output: `(batch_size, sequence_length, embedding_dim)` of dense vectors\n",
    "\n",
    "**Step 2: Positional Embedding Addition**\n",
    "```python\n",
    "sequence = torch.arange(tokens_num, device=device)\n",
    "x = x + self.positional_embedding_layer(sequence)\n",
    "```\n",
    "\n",
    "**Issue Alert**: The code has a bug - `device` is not defined in the method scope. Should be:\n",
    "```python\n",
    "sequence = torch.arange(tokens_num, device=token_ids.device)\n",
    "```\n",
    "\n",
    "**Mathematical Operation:**\n",
    "Each position gets both content and positional information:\n",
    "$$\\text{input\\_to\\_blocks} = \\text{TokenEmb}(\\text{token\\_ids}) + \\text{PosEmb}(\\text{positions})$$\n",
    "\n",
    "**Step 3: Transformer Processing**\n",
    "```python\n",
    "x = self.layers(x)\n",
    "```\n",
    "Passes through all transformer blocks sequentially, with each block applying attention and feed-forward transformations.\n",
    "\n",
    "**Step 4: Output Normalization**\n",
    "```python\n",
    "x = self.layer_norm(x)\n",
    "```\n",
    "Final layer normalization ensures stable representations before output projection.\n",
    "\n",
    "**Step 5: Vocabulary Projection**\n",
    "```python\n",
    "x = self.unembedding(x)\n",
    "```\n",
    "- Maps from embedding space back to vocabulary space\n",
    "- Output: `(batch_size, sequence_length, vocabulary_size)`\n",
    "- Each position gets a probability distribution over all possible next tokens\n",
    "\n",
    "###### Key Architecture Decisions\n",
    "\n",
    "**Parameter Sharing:**\n",
    "The model uses separate embedding matrices for tokens and positions, allowing independent learning of semantic and positional representations.\n",
    "\n",
    "**No Bias in Final Layer:**\n",
    "```python\n",
    "bias=False\n",
    "```\n",
    "Common practice in modern language models to reduce parameters and improve training dynamics.\n",
    "\n",
    "**Additive Positional Encoding:**\n",
    "Uses learned positional embeddings added to token embeddings, rather than the sinusoidal encodings from the original transformer paper.\n",
    "\n",
    "###### Dimensional Flow Example\n",
    "\n",
    "```python\n",
    "# Example with config: vocabulary_size=50000, embedding_dim=768, context_size=1024\n",
    "# Input token_ids: (32, 512)  # batch_size=32, sequence_length=512\n",
    "\n",
    "# After token embedding: (32, 512, 768)\n",
    "# After positional embedding addition: (32, 512, 768)\n",
    "# After transformer blocks: (32, 512, 768)\n",
    "# After final layer norm: (32, 512, 768)  \n",
    "# After unembedding: (32, 512, 50000)  # Logits for each vocabulary token\n",
    "```\n",
    "\n",
    "###### Training Usage\n",
    "\n",
    "During training, the output logits are used with cross-entropy loss:\n",
    "```python\n",
    "# model output: (batch_size, sequence_length, vocab_size)\n",
    "# targets: (batch_size, sequence_length)\n",
    "loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "```\n",
    "\n",
    "###### Generation Usage\n",
    "\n",
    "For text generation, the model predicts one token at a time:\n",
    "```python\n",
    "# Get logits for last position\n",
    "next_token_logits = model(input_ids)[:, -1, :]  # (batch_size, vocab_size)\n",
    "# Sample or take argmax to get next token\n",
    "next_token = torch.multinomial(torch.softmax(next_token_logits, dim=-1), 1)\n",
    "```\n",
    "\n",
    "This architecture represents a complete autoregressive language model capable of learning complex language patterns and generating coherent text through next-token prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DemoGPT(nn.Module):\n",
    "  \"\"\"\n",
    "  A complete, simplified implementation of a GPT-style transformer model.\n",
    "\n",
    "  This class brings together all the necessary components:\n",
    "  1. Token and Positional Embeddings to create the initial input representation.\n",
    "  2. A stack of Transformer `Block`s to perform the core processing.\n",
    "  3. A final Layer Normalization and a linear layer (unembedding) to project\n",
    "     the output back into the vocabulary space to get logits for the next token.\n",
    "\n",
    "  The model is designed for auto-regressive language generation, predicting the\n",
    "  next token in a sequence given the previous ones.\n",
    "  \"\"\"\n",
    "  def __init__(self, config):\n",
    "    \"\"\"\n",
    "    Initializes the DemoGPT model architecture.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A configuration dictionary containing model hyperparameters:\n",
    "            - \"vocabulary_size\" (int): The number of unique tokens in the tokenizer.\n",
    "            - \"embedding_dim\" (int): The dimensionality of the token and positional embeddings.\n",
    "            - \"context_size\" (int): The maximum sequence length the model can handle.\n",
    "            - \"layers_num\" (int): The number of Transformer `Block`s to stack.\n",
    "            - Other keys required by the `Block` class.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # Token embedding layer: maps each token ID to a dense vector.\n",
    "    self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
    "    # Positional embedding layer: maps each position index (0 to context_size-1) to a vector.\n",
    "    self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
    "\n",
    "    # Create a stack of Transformer Blocks.\n",
    "    # `nn.Sequential` chains the blocks together, so the output of one is the input to the next.\n",
    "    blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "    self.layers = nn.Sequential(*blocks)\n",
    "\n",
    "    # A final layer normalization applied after the transformer blocks.\n",
    "    self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "    # The final linear layer (unembedding) that projects the model's output\n",
    "    # back to the vocabulary size to get the logits for each token.\n",
    "    self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
    "\n",
    "  def forward(self, token_ids):\n",
    "    \"\"\"\n",
    "    Performs the forward pass of the DemoGPT model.\n",
    "\n",
    "    Args:\n",
    "        token_ids (torch.Tensor): A tensor of shape (B, T) containing the\n",
    "            input token IDs, where B is batch size and T is sequence length.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output logits tensor of shape (B, T, V), where V is\n",
    "            the vocabulary size.\n",
    "    \"\"\"\n",
    "    batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "    # 1. Get token embeddings for the input IDs. Shape: (B, T, E)\n",
    "    token_embeddings = self.token_embedding_layer(token_ids)\n",
    "    \n",
    "    # 2. Get positional embeddings for each position in the sequence.\n",
    "    # `torch.arange` creates a sequence of position indices [0, 1, ..., T-1].\n",
    "    sequence = torch.arange(tokens_num, device=token_ids.device)\n",
    "    positional_embeddings = self.positional_embedding_layer(sequence) # Shape: (T, E)\n",
    "    \n",
    "    # 3. Add token and positional embeddings. Broadcasting adds the positional\n",
    "    # embeddings to each sequence in the batch. Shape: (B, T, E)\n",
    "    x = token_embeddings + positional_embeddings\n",
    "\n",
    "    # 4. Pass the combined embeddings through the stack of Transformer blocks.\n",
    "    x = self.layers(x)\n",
    "    \n",
    "    # 5. Apply the final layer normalization.\n",
    "    x = self.layer_norm(x)\n",
    "    \n",
    "    # 6. Project the final hidden states to logits over the vocabulary.\n",
    "    logits = self.unembedding(x) # Shape: (B, T, V)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 65])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Autoregressive Text Generation Function\n",
    "\n",
    "###### Purpose and Functionality\n",
    "\n",
    "The `generate` function implements autoregressive text generation for language models, producing new tokens one at a time by sampling from the model's predicted probability distributions. This is the standard approach for generating coherent text sequences from transformer-based language models.\n",
    "\n",
    "###### Function Parameters\n",
    "\n",
    "```python\n",
    "def generate(model, prompt_ids, max_tokens):\n",
    "```\n",
    "\n",
    "- **model**: The trained GPT model instance\n",
    "- **prompt_ids**: Initial token sequence to start generation (shape: `(1, prompt_length)`)\n",
    "- **max_tokens**: Maximum number of new tokens to generate\n",
    "\n",
    "###### Step-by-Step Generation Process\n",
    "\n",
    "**Initialization:**\n",
    "```python\n",
    "output_ids = prompt_ids\n",
    "```\n",
    "Starts with the provided prompt as the foundation for generation.\n",
    "\n",
    "**Generation Loop:**\n",
    "```python\n",
    "for _ in range(max_tokens):\n",
    "```\n",
    "Iteratively generates tokens up to the specified maximum.\n",
    "\n",
    "**Context Length Check:**\n",
    "```python\n",
    "if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "    break\n",
    "```\n",
    "Prevents exceeding the model's maximum context window (e.g., 1024 tokens).\n",
    "\n",
    "**Forward Pass (No Gradients):**\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    logits = model(output_ids)\n",
    "```\n",
    "- Disables gradient computation for efficiency during inference\n",
    "- Gets model predictions for all positions in the sequence\n",
    "- Output shape: `(batch_size, sequence_length, vocabulary_size)`\n",
    "\n",
    "**Next Token Prediction:**\n",
    "```python\n",
    "logits = logits[:, -1, :]  # Extract last position logits\n",
    "probs = F.softmax(logits, dim=-1)  # Convert to probabilities\n",
    "```\n",
    "- Extracts logits for the last position only (next token prediction)\n",
    "- Applies softmax to convert raw logits to probability distribution\n",
    "- Result shape: `(batch_size, vocabulary_size)`\n",
    "\n",
    "**Sampling Strategy:**\n",
    "```python\n",
    "next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "Uses multinomial sampling to select next token based on probability distribution rather than always choosing the highest probability token (greedy decoding).\n",
    "\n",
    "**Sequence Extension:**\n",
    "```python\n",
    "output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "```\n",
    "Appends the newly generated token to the existing sequence for the next iteration.\n",
    "\n",
    "###### Sampling vs. Greedy Decoding\n",
    "\n",
    "**Multinomial Sampling (Used Here):**\n",
    "- Introduces randomness and diversity in generation\n",
    "- Tokens with higher probability are more likely to be selected\n",
    "- Produces more creative and varied outputs\n",
    "- Can occasionally select lower-probability but contextually interesting tokens\n",
    "\n",
    "**Greedy Decoding (Alternative):**\n",
    "```python\n",
    "next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "```\n",
    "- Always selects the highest probability token\n",
    "- Deterministic output (same input always produces same output)\n",
    "- Often leads to repetitive or predictable text\n",
    "\n",
    "###### Generation Example Flow\n",
    "\n",
    "```python\n",
    "# Initial prompt: \"The cat sat on the\"\n",
    "# prompt_ids: [464, 3857, 3332, 319, 262]  # Token IDs\n",
    "\n",
    "# Iteration 1:\n",
    "# Model predicts probabilities: [mat: 0.4, chair: 0.3, floor: 0.2, ...]\n",
    "# Sample → \"mat\" (token_id: 2603)\n",
    "# output_ids: [464, 3857, 3332, 319, 262, 2603]\n",
    "\n",
    "# Iteration 2:\n",
    "# Model sees \"The cat sat on the mat\"\n",
    "# Predicts next token probabilities: [and: 0.5, while: 0.2, .: 0.15, ...]\n",
    "# Sample → \"and\" (token_id: 290)\n",
    "# output_ids: [464, 3857, 3332, 319, 262, 2603, 290]\n",
    "```\n",
    "\n",
    "###### Key Design Considerations\n",
    "\n",
    "**Memory Efficiency:**\n",
    "Using `torch.no_grad()` prevents unnecessary gradient computation and memory allocation during inference.\n",
    "\n",
    "**Context Management:**\n",
    "The function respects the model's context length limit, preventing out-of-bounds errors.\n",
    "\n",
    "**Stochastic Generation:**\n",
    "Multinomial sampling introduces controlled randomness, balancing coherence with creativity.\n",
    "\n",
    "**Incremental Processing:**\n",
    "Each iteration processes the entire sequence, allowing the model to consider full context when predicting the next token.\n",
    "\n",
    "This generation approach forms the foundation for interactive chatbots, creative writing assistants, and other applications requiring coherent text continuation from language models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, prompt_ids, max_tokens_to_generate, config):\n",
    "    \"\"\"\n",
    "    Generates a sequence of tokens auto-regressively from a given prompt.\n",
    "\n",
    "    This function takes a trained model and a starting sequence of token IDs\n",
    "    (the prompt) and generates new tokens one by one. In each step, it uses\n",
    "    the model to predict the next token, samples from the resulting probability\n",
    "    distribution, and appends the new token to the sequence, which then becomes\n",
    "    the input for the next step.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained DemoGPT transformer model.\n",
    "        prompt_ids (torch.Tensor): A tensor of shape (B, T) containing the\n",
    "            initial token IDs to start generation from. B is the batch size\n",
    "            (usually 1 for generation) and T is the length of the prompt.\n",
    "        max_tokens_to_generate (int): The maximum number of new tokens to generate after the prompt.\n",
    "        config (dict): The model's configuration dictionary, used to access\n",
    "            the `context_size`.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (B, T + generated_tokens) containing\n",
    "            the original prompt plus the newly generated tokens.\n",
    "    \"\"\"\n",
    "    # Start with the initial prompt.\n",
    "    output_ids = prompt_ids\n",
    "    \n",
    "    # Loop to generate tokens one by one.\n",
    "    for _ in range(max_tokens_to_generate):\n",
    "      # Stop if the context window is full.\n",
    "      if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "        break\n",
    "        \n",
    "      # Use torch.no_grad() to disable gradient calculations, as we are only\n",
    "      # doing inference, which saves memory and computation.\n",
    "      with torch.no_grad():\n",
    "        # Get the model's predictions (logits) for the current sequence.\n",
    "        logits = model(output_ids)\n",
    "\n",
    "      # Focus only on the logits for the very last token in the sequence,\n",
    "      # as that's the prediction for the *next* token.\n",
    "      last_token_logits = logits[:, -1, :]\n",
    "      \n",
    "      # Apply softmax to convert the logits into a probability distribution.\n",
    "      probs = F.softmax(last_token_logits, dim=-1)\n",
    "      \n",
    "      # Sample one token from the probability distribution.\n",
    "      # `torch.multinomial` treats the input as a set of weights for sampling.\n",
    "      next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "      \n",
    "      # Append the newly sampled token ID to our sequence.\n",
    "      output_ids = torch.cat([output_ids, next_token_id], dim=1)\n",
    "      \n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "#   model.eval()\n",
    "\n",
    "#   prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "#   return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])\n",
    "\n",
    "\n",
    "def generate_with_prompt(model, tokenizer, config, prompt, max_tokens_to_generate=100):\n",
    "  \"\"\"\n",
    "  Generates text from a prompt using the specified model and tokenizer.\n",
    "\n",
    "  This function sets the model to evaluation mode, encodes the prompt, calls\n",
    "  the `generate` function to produce token IDs, and decodes them back into\n",
    "  human-readable text.\n",
    "\n",
    "  Args:\n",
    "      model (nn.Module): The trained transformer model.\n",
    "      tokenizer (CharTokenizer): The tokenizer for encoding/decoding text.\n",
    "      config (dict): The model's configuration dictionary.\n",
    "      prompt (str): The initial text to start generation from.\n",
    "      max_tokens_to_generate (int): The maximum number of new tokens to create.\n",
    "\n",
    "  Returns:\n",
    "      str: The generated text, including the original prompt.\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "\n",
    "  prompt_ids = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "  # Call the generate function with the correct arguments\n",
    "  generated_ids = generate(\n",
    "      model,\n",
    "      prompt_ids,\n",
    "      max_tokens_to_generate=max_tokens_to_generate,\n",
    "      config=config\n",
    "  )\n",
    "\n",
    "  return tokenizer.decode(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\n.Mnr\\nS'cW-FyGGuvZ: LMkGRvPGRarz eu;cdrkUt FcwNa! XVStOhgg!!Yw-ForftaIqIvv,zn;hAitYBYxiRkBEOqn-MC:tc\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")\n",
    "\n",
    "generate_with_prompt(model, tokenizer, config, \"First Citizen:\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
