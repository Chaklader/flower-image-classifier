{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ulgnkbbvEt6u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from get_device import get_device\n",
        "\n",
        "device = get_device()\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# cuda_is_available = torch.cuda.is_available()\n",
        "\n",
        "# if cuda_is_available:\n",
        "#   print(\"All good!\")\n",
        "# else:\n",
        "#   print(\"CUDA is NOT available!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R6RnqQu0-P9"
      },
      "outputs": [],
      "source": [
        "prompt = \"During the latest presentation OpenAI\"\n",
        "model = \"openai-community/gpt2-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkVKzDegAItu"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the text generation pipeline\n",
        "text_generator = pipeline(\"text-generation\", model=model, device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xUAq3XiAN1z"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "print(f\"Generated text:\\n{generated_texts[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGcDDPcz1Ln5"
      },
      "outputs": [],
      "source": [
        "for do_sample in [\n",
        "    False, # Greedy Search\n",
        "    True   # Multinomial sampling\n",
        "  ]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=do_sample, num_beams=1)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"do_sample={do_sample}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo3CC3yk4QmC"
      },
      "outputs": [],
      "source": [
        "# Beam-search strategy\n",
        "\n",
        "for beams in [1, 2, 4, 8]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=False, num_beams=beams)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"num_beams={beams}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K96hVRPC6MUR"
      },
      "outputs": [],
      "source": [
        "# Beam-search multinomial sampling\n",
        "\n",
        "for beams in [1, 2, 4, 8]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=beams)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"num_beams={beams}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5R7GeD3AUgp"
      },
      "outputs": [],
      "source": [
        "# Change the top-k parameter\n",
        "\n",
        "for k in [1, 5, 10, 50, 100, 500]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, top_k=k)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"top_k={k}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXOuaxe0RJwf"
      },
      "outputs": [],
      "source": [
        "# Change temperature\n",
        "\n",
        "for temp in [0.1, 1.0, 2.0, 3.0]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, temperature=temp)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"temperature={temp}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (ml)",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
