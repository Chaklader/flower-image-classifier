{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z2ea-5SaF59N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set the device for computation\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DSc0ludHGE1o"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "text = Path('../../data/tiny-shakespeare.txt').read_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7-9Nk7OoGGHc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[0:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Character-Level Tokenizer for Language Models\n",
        "\n",
        "###### Purpose and Overview\n",
        "\n",
        "The `CharTokenizer` class implements a character-level tokenization system that converts text into numerical representations for neural language models. Unlike word-based tokenizers, this approach treats each individual character as a token, making it suitable for character-level language modeling tasks.\n",
        "\n",
        "###### Core Functionality\n",
        "\n",
        "**Bidirectional Mapping System:**\n",
        "The tokenizer maintains two dictionaries for efficient conversion:\n",
        "- `token_id_for_char`: Maps characters to unique integer IDs\n",
        "- `char_for_token_id`: Maps integer IDs back to characters\n",
        "\n",
        "**Key Operations:**\n",
        "- **Encoding**: Text → Tensor of integer IDs\n",
        "- **Decoding**: Tensor of integer IDs → Text\n",
        "- **Vocabulary Management**: Builds and maintains character vocabulary\n",
        "\n",
        "###### Detailed Method Analysis\n",
        "\n",
        "**Initialization Process:**\n",
        "```python\n",
        "tokenizer = CharTokenizer(['a', 'b', 'c', ' ', '!'])\n",
        "# Creates mappings:\n",
        "# token_id_for_char = {'a': 0, 'b': 1, 'c': 2, ' ': 3, '!': 4}\n",
        "# char_for_token_id = {0: 'a', 1: 'b', 2: 'c', 3: ' ', 4: '!'}\n",
        "```\n",
        "\n",
        "**Training from Text:**\n",
        "```python\n",
        "text = \"hello world\"\n",
        "tokenizer = CharTokenizer.train_from_text(text)\n",
        "# Vocabulary: [' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']  # Sorted unique characters\n",
        "```\n",
        "\n",
        "**Encoding Example:**\n",
        "```python\n",
        "text = \"hello\"\n",
        "encoded = tokenizer.encode(text)\n",
        "# Result: tensor([3, 4, 5, 5, 6])  # Each character mapped to its ID\n",
        "```\n",
        "\n",
        "**Decoding Example:**\n",
        "```python\n",
        "token_ids = torch.tensor([3, 4, 5, 5, 6])\n",
        "decoded = tokenizer.decode(token_ids)\n",
        "# Result: \"hello\"  # IDs mapped back to characters\n",
        "```\n",
        "\n",
        "The IDs are generated automatically by the `enumerate()` function in the `__init__` method. Here's exactly how it works:\n",
        "\n",
        "###### ID Assignment Process\n",
        "\n",
        "```python\n",
        "def __init__(self, vocabulary):\n",
        "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "```\n",
        "\n",
        "###### Step-by-Step Breakdown\n",
        "\n",
        "**Input vocabulary:** `['a', 'b', 'c', ' ', '!']`\n",
        "\n",
        "**enumerate() function creates pairs:**\n",
        "```python\n",
        "list(enumerate(['a', 'b', 'c', ' ', '!']))\n",
        "# Result: [(0, 'a'), (1, 'b'), (2, 'c'), (3, ' '), (4, '!')]\n",
        "```\n",
        "\n",
        "**Dictionary comprehension assigns IDs:**\n",
        "```python\n",
        "# For token_id_for_char:\n",
        "{char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "# Becomes: {'a': 0, 'b': 1, 'c': 2, ' ': 3, '!': 4}\n",
        "\n",
        "# For char_for_token_id:\n",
        "{token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "# Becomes: {0: 'a', 1: 'b', 2: 'c', 3: ' ', 4: '!'}\n",
        "```\n",
        "\n",
        "###### The enumerate() Function\n",
        "\n",
        "`enumerate(sequence)` returns pairs of `(index, item)` where the index starts at 0:\n",
        "- Position 0: 'a' gets ID 0\n",
        "- Position 1: 'b' gets ID 1\n",
        "- Position 2: 'c' gets ID 2\n",
        "- Position 3: ' ' gets ID 3\n",
        "- Position 4: '!' gets ID 4\n",
        "\n",
        "The IDs are simply the sequential positions of characters in the vocabulary list, starting from 0. This ensures each character has a unique integer identifier that can be used as an index in neural network embedding layers.\n",
        "\n",
        "\n",
        "\n",
        "###### Practical Application Workflow\n",
        "\n",
        "**Step 1: Vocabulary Creation**\n",
        "The tokenizer scans input text to identify all unique characters and creates a sorted vocabulary ensuring consistent ordering across different runs.\n",
        "\n",
        "**Step 2: Text Processing**\n",
        "During encoding, each character in the input text is looked up in the vocabulary and replaced with its corresponding integer ID, creating a sequence of numbers suitable for neural network processing.\n",
        "\n",
        "**Step 3: Neural Network Integration**\n",
        "The encoded tensors can be fed directly into embedding layers of neural networks, where each character ID is mapped to a learned vector representation.\n",
        "\n",
        "**Step 4: Output Decoding**\n",
        "Model predictions (sequences of token IDs) are converted back to readable text using the reverse mapping.\n",
        "\n",
        "###### Use Cases and Applications\n",
        "\n",
        "**Character-Level Language Models:**\n",
        "- Text generation at character granularity\n",
        "- Handling out-of-vocabulary words naturally\n",
        "- Working with any language or script without preprocessing\n",
        "\n",
        "**Advantages:**\n",
        "- No unknown token issues (every character is in vocabulary)\n",
        "- Handles misspellings and novel words\n",
        "- Language-agnostic approach\n",
        "- Simple implementation and debugging\n",
        "\n",
        "**Limitations:**\n",
        "- Longer sequences than word-based tokenization\n",
        "- May struggle with long-range dependencies\n",
        "- Computationally more intensive for long texts\n",
        "\n",
        "###### Technical Implementation Details\n",
        "\n",
        "**Data Types:**\n",
        "The tokenizer uses `torch.long` dtype for token IDs, which is standard for indexing operations in PyTorch embeddings and ensures compatibility with neural network layers.\n",
        "\n",
        "**Memory Efficiency:**\n",
        "The bidirectional mapping approach provides O(1) lookup time for both encoding and decoding operations, making it efficient for processing large texts.\n",
        "\n",
        "**Vocabulary Consistency:**\n",
        "The sorting of unique characters ensures deterministic vocabulary creation, crucial for model reproducibility across different training runs.\n",
        "\n",
        "This tokenizer serves as a foundational component for character-level natural language processing tasks, providing the essential interface between raw text and numerical representations required by neural networks.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You're absolutely right to question this! The current `__init__` method has a **serious flaw** - it doesn't handle duplicates properly.\n",
        "\n",
        "###### The Problem with Duplicates\n",
        "\n",
        "If the vocabulary contains duplicates, the dictionary comprehension will overwrite previous mappings:\n",
        "\n",
        "```python\n",
        "# Problematic input with duplicates:\n",
        "vocabulary = ['a', 'b', 'a', 'c', 'a']\n",
        "\n",
        "# enumerate() produces:\n",
        "[(0, 'a'), (1, 'b'), (2, 'a'), (3, 'c'), (4, 'a')]\n",
        "\n",
        "# Dictionary comprehension overwrites:\n",
        "token_id_for_char = {'a': 0, 'b': 1, 'a': 2, 'c': 3, 'a': 4}\n",
        "# Final result: {'a': 4, 'b': 1, 'c': 3}  # 'a' only maps to ID 4!\n",
        "\n",
        "char_for_token_id = {0: 'a', 1: 'b', 2: 'a', 3: 'c', 4: 'a'}\n",
        "# Final result: {0: 'a', 1: 'b', 2: 'a', 3: 'c', 4: 'a'}  # Multiple IDs for 'a'\n",
        "```\n",
        "\n",
        "###### The Broken Behavior\n",
        "\n",
        "This creates inconsistent mappings:\n",
        "- `encode('a')` would return ID 4 (the last occurrence)\n",
        "- `decode([0])` would return 'a' \n",
        "- But `encode(decode([0]))` wouldn't equal `[0]` - it would be `[4]`!\n",
        "\n",
        "###### How the Code Should Handle This\n",
        "\n",
        "The code currently **doesn't** handle duplicates properly. A robust implementation should either:\n",
        "\n",
        "**Option 1: Remove duplicates before processing:**\n",
        "```python\n",
        "def __init__(self, vocabulary):\n",
        "    unique_vocab = list(dict.fromkeys(vocabulary))  # Preserves order, removes duplicates\n",
        "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(unique_vocab)}\n",
        "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(unique_vocab)}\n",
        "```\n",
        "\n",
        "**Option 2: Raise an error for duplicates:**\n",
        "```python\n",
        "def __init__(self, vocabulary):\n",
        "    if len(set(vocabulary)) != len(vocabulary):\n",
        "        raise ValueError(\"Vocabulary contains duplicate characters\")\n",
        "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "```\n",
        "\n",
        "###### Why `train_from_text()` Works\n",
        "\n",
        "The `train_from_text()` method avoids this issue by explicitly removing duplicates:\n",
        "```python\n",
        "vocabulary = sorted(list(set(text)))  # set() removes duplicates\n",
        "```\n",
        "\n",
        "So the class works correctly when using `train_from_text()`, but the `__init__` method itself is vulnerable to duplicate inputs. This is a design flaw that should be addressed for robustness.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qg-yHMXPGHYq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class CharTokenizer:\n",
        "    \"\"\"\n",
        "    A simple character-level tokenizer for converting text to and from numerical IDs.\n",
        "\n",
        "    This tokenizer builds a vocabulary from a given text and provides methods\n",
        "    to encode strings into integer tensors and decode them back into strings.\n",
        "    It is a basic but essential component for character-level language models.\n",
        "\n",
        "    Attributes:\n",
        "        token_id_for_char (dict): A mapping from each character in the vocabulary\n",
        "            to its unique integer ID.\n",
        "        char_for_token_id (dict): A reverse mapping from each integer ID back\n",
        "            to its corresponding character.\n",
        "    \"\"\"\n",
        "  \n",
        "    # def __init__(self, vocabulary):\n",
        "    #     \"\"\"\n",
        "    #     Initializes the CharTokenizer with a predefined vocabulary.\n",
        "\n",
        "    #     Args:\n",
        "    #         vocabulary (list or str): An ordered list or string of unique\n",
        "    #             characters that will form the tokenizer's vocabulary.\n",
        "    #     \"\"\"\n",
        "    #     self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "    #     self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "\n",
        "    def __init__(self, vocabulary):\n",
        "        \"\"\"\n",
        "        Initializes the CharTokenizer with a predefined vocabulary.\n",
        "\n",
        "        Args:\n",
        "            vocabulary (list or str): An ordered list or string of unique\n",
        "                characters that will form the tokenizer's vocabulary.\n",
        "        \"\"\"        \n",
        "        \n",
        "        unique_vocab = list(dict.fromkeys(vocabulary))  # Preserves order, removes duplicates\n",
        "        self.token_id_for_char = {char: token_id for token_id, char in enumerate(unique_vocab)}\n",
        "        self.char_for_token_id = {token_id: char for token_id, char in enumerate(unique_vocab)}\n",
        "\n",
        "    @staticmethod\n",
        "    def train_from_text(text):\n",
        "        \"\"\"\n",
        "        Creates a new CharTokenizer instance by building a vocabulary from text.\n",
        "\n",
        "        This static method scans the input text, finds all unique characters,\n",
        "        sorts them to ensure a consistent vocabulary order, and then creates\n",
        "        a new tokenizer instance based on this vocabulary.\n",
        "\n",
        "        Args:\n",
        "            text (str): The corpus of text from which to build the vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            CharTokenizer: A new instance of the tokenizer trained on the text.\n",
        "        \"\"\"\n",
        "        vocabulary = sorted(list(set(text)))\n",
        "        return CharTokenizer(vocabulary)\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a string of text into a tensor of token IDs.\n",
        "\n",
        "        Each character in the input string is mapped to its corresponding integer\n",
        "        ID from the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            text (str): The string to encode.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A 1D tensor of dtype torch.long containing the sequence\n",
        "                of token IDs.\n",
        "        \"\"\"\n",
        "        token_ids = []\n",
        "        for char in text:\n",
        "            token_ids.append(self.token_id_for_char[char])\n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"\n",
        "        Decodes a tensor of token IDs back into a string of text.\n",
        "\n",
        "        Each integer ID in the input tensor is mapped back to its corresponding\n",
        "        character from the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            token_ids (torch.Tensor): A 1D tensor of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded string.\n",
        "        \"\"\"\n",
        "        chars = []\n",
        "        # .tolist() converts the tensor to a standard Python list for iteration.\n",
        "        for token_id in token_ids.tolist():\n",
        "            chars.append(self.char_for_token_id[token_id])\n",
        "        return ''.join(chars)\n",
        "\n",
        "    def vocabulary_size(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of unique characters in the vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            int: The size of the vocabulary.\n",
        "        \"\"\"\n",
        "        return len(self.token_id_for_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pAYBFds4GNAb"
      },
      "outputs": [],
      "source": [
        "tokenizer = CharTokenizer.train_from_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LKq-R9xvJ3RX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n",
            "\n",
            "\n",
            "tensor([20, 43, 50, 50, 43, 53, 53])\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.encode(\"Hello world\"))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(tokenizer.encode(\"Helleoo\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tWpR_hr9GOKs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello world\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3a7qPM-mGPur"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocabulary_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UbtX7_JtHFXL"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(depth=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2I3m9KI6HM-S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: '\\n',\n",
            " 1: ' ',\n",
            " 2: '!',\n",
            " 3: '$',\n",
            " 4: '&',\n",
            " 5: \"'\",\n",
            " 6: ',',\n",
            " 7: '-',\n",
            " 8: '.',\n",
            " 9: '3',\n",
            " 10: ':',\n",
            " 11: ';',\n",
            " 12: '?',\n",
            " 13: 'A',\n",
            " 14: 'B',\n",
            " 15: 'C',\n",
            " 16: 'D',\n",
            " 17: 'E',\n",
            " 18: 'F',\n",
            " 19: 'G',\n",
            " 20: 'H',\n",
            " 21: 'I',\n",
            " 22: 'J',\n",
            " 23: 'K',\n",
            " 24: 'L',\n",
            " 25: 'M',\n",
            " 26: 'N',\n",
            " 27: 'O',\n",
            " 28: 'P',\n",
            " 29: 'Q',\n",
            " 30: 'R',\n",
            " 31: 'S',\n",
            " 32: 'T',\n",
            " 33: 'U',\n",
            " 34: 'V',\n",
            " 35: 'W',\n",
            " 36: 'X',\n",
            " 37: 'Y',\n",
            " 38: 'Z',\n",
            " 39: 'a',\n",
            " 40: 'b',\n",
            " 41: 'c',\n",
            " 42: 'd',\n",
            " 43: 'e',\n",
            " 44: 'f',\n",
            " 45: 'g',\n",
            " 46: 'h',\n",
            " 47: 'i',\n",
            " 48: 'j',\n",
            " 49: 'k',\n",
            " 50: 'l',\n",
            " 51: 'm',\n",
            " 52: 'n',\n",
            " 53: 'o',\n",
            " 54: 'p',\n",
            " 55: 'q',\n",
            " 56: 'r',\n",
            " 57: 's',\n",
            " 58: 't',\n",
            " 59: 'u',\n",
            " 60: 'v',\n",
            " 61: 'w',\n",
            " 62: 'x',\n",
            " 63: 'y',\n",
            " 64: 'z'}\n"
          ]
        }
      ],
      "source": [
        "pp.pprint(tokenizer.char_for_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "olX9rHjxHQih"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'\\n': 0,\n",
            " ' ': 1,\n",
            " '!': 2,\n",
            " '$': 3,\n",
            " '&': 4,\n",
            " \"'\": 5,\n",
            " ',': 6,\n",
            " '-': 7,\n",
            " '.': 8,\n",
            " '3': 9,\n",
            " ':': 10,\n",
            " ';': 11,\n",
            " '?': 12,\n",
            " 'A': 13,\n",
            " 'B': 14,\n",
            " 'C': 15,\n",
            " 'D': 16,\n",
            " 'E': 17,\n",
            " 'F': 18,\n",
            " 'G': 19,\n",
            " 'H': 20,\n",
            " 'I': 21,\n",
            " 'J': 22,\n",
            " 'K': 23,\n",
            " 'L': 24,\n",
            " 'M': 25,\n",
            " 'N': 26,\n",
            " 'O': 27,\n",
            " 'P': 28,\n",
            " 'Q': 29,\n",
            " 'R': 30,\n",
            " 'S': 31,\n",
            " 'T': 32,\n",
            " 'U': 33,\n",
            " 'V': 34,\n",
            " 'W': 35,\n",
            " 'X': 36,\n",
            " 'Y': 37,\n",
            " 'Z': 38,\n",
            " 'a': 39,\n",
            " 'b': 40,\n",
            " 'c': 41,\n",
            " 'd': 42,\n",
            " 'e': 43,\n",
            " 'f': 44,\n",
            " 'g': 45,\n",
            " 'h': 46,\n",
            " 'i': 47,\n",
            " 'j': 48,\n",
            " 'k': 49,\n",
            " 'l': 50,\n",
            " 'm': 51,\n",
            " 'n': 52,\n",
            " 'o': 53,\n",
            " 'p': 54,\n",
            " 'q': 55,\n",
            " 'r': 56,\n",
            " 's': 57,\n",
            " 't': 58,\n",
            " 'u': 59,\n",
            " 'v': 60,\n",
            " 'w': 61,\n",
            " 'x': 62,\n",
            " 'y': 63,\n",
            " 'z': 64}\n"
          ]
        }
      ],
      "source": [
        "pp.pprint(tokenizer.token_id_for_char)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (ml)",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
