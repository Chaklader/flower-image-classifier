{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from get_device import get_device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('../../data/tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CharTokenizer:\n",
    "    \"\"\"\n",
    "    A simple character-level tokenizer for converting text to and from numerical IDs.\n",
    "\n",
    "    This tokenizer builds a vocabulary from a given text and provides methods\n",
    "    to encode strings into integer tensors and decode them back into strings.\n",
    "    It is a basic but essential component for character-level language models.\n",
    "\n",
    "    Attributes:\n",
    "        token_id_for_char (dict): A mapping from each character in the vocabulary\n",
    "            to its unique integer ID.\n",
    "        char_for_token_id (dict): A reverse mapping from each integer ID back\n",
    "            to its corresponding character.\n",
    "    \"\"\"\n",
    "  \n",
    "    # def __init__(self, vocabulary):\n",
    "    #     \"\"\"\n",
    "    #     Initializes the CharTokenizer with a predefined vocabulary.\n",
    "\n",
    "    #     Args:\n",
    "    #         vocabulary (list or str): An ordered list or string of unique\n",
    "    #             characters that will form the tokenizer's vocabulary.\n",
    "    #     \"\"\"\n",
    "    #     self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    #     self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "    def __init__(self, vocabulary):\n",
    "        \"\"\"\n",
    "        Initializes the CharTokenizer with a predefined vocabulary.\n",
    "\n",
    "        Args:\n",
    "            vocabulary (list or str): An ordered list or string of unique\n",
    "                characters that will form the tokenizer's vocabulary.\n",
    "        \"\"\"        \n",
    "        \n",
    "        unique_vocab = list(dict.fromkeys(vocabulary))  # Preserves order, removes duplicates\n",
    "        self.token_id_for_char = {char: token_id for token_id, char in enumerate(unique_vocab)}\n",
    "        self.char_for_token_id = {token_id: char for token_id, char in enumerate(unique_vocab)}\n",
    "\n",
    "    @staticmethod\n",
    "    def train_from_text(text):\n",
    "        \"\"\"\n",
    "        Creates a new CharTokenizer instance by building a vocabulary from text.\n",
    "\n",
    "        This static method scans the input text, finds all unique characters,\n",
    "        sorts them to ensure a consistent vocabulary order, and then creates\n",
    "        a new tokenizer instance based on this vocabulary.\n",
    "\n",
    "        Args:\n",
    "            text (str): The corpus of text from which to build the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            CharTokenizer: A new instance of the tokenizer trained on the text.\n",
    "        \"\"\"\n",
    "        vocabulary = sorted(list(set(text)))\n",
    "        return CharTokenizer(vocabulary)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encodes a string of text into a tensor of token IDs.\n",
    "\n",
    "        Each character in the input string is mapped to its corresponding integer\n",
    "        ID from the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            text (str): The string to encode.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A 1D tensor of dtype torch.long containing the sequence\n",
    "                of token IDs.\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "        for char in text:\n",
    "            token_ids.append(self.token_id_for_char[char])\n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decodes a tensor of token IDs back into a string of text.\n",
    "\n",
    "        Each integer ID in the input tensor is mapped back to its corresponding\n",
    "        character from the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            token_ids (torch.Tensor): A 1D tensor of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        chars = []\n",
    "        # .tolist() converts the tensor to a standard Python list for iteration.\n",
    "        for token_id in token_ids.tolist():\n",
    "            chars.append(self.char_for_token_id[token_id])\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of unique characters in the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Language Model Dataset: Token Sequence Generation\n",
    "\n",
    "###### Purpose and Functionality\n",
    "\n",
    "The `TokenIdsDataset` class creates training data for autoregressive language models by converting a sequence of tokens into input-target pairs. This dataset implements the standard \"next token prediction\" training paradigm where the model learns to predict the subsequent token given a context window.\n",
    "\n",
    "###### Core Concept: Shifted Sequences\n",
    "\n",
    "**Training Objective:** Given a sequence of tokens, predict the next token\n",
    "**Implementation:** For each position, create pairs where the target is the input shifted by one position\n",
    "\n",
    "###### Detailed Example Walkthrough\n",
    "\n",
    "**Setup:**\n",
    "```python\n",
    "data = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])  # Token sequence\n",
    "block_size = 4  # Context window size\n",
    "dataset = TokenIdsDataset(data, block_size)\n",
    "```\n",
    "\n",
    "**Dataset Length Calculation:**\n",
    "```python\n",
    "len(dataset) = len(data) - block_size = 9 - 4 = 5\n",
    "# 5 possible starting positions for complete sequences\n",
    "```\n",
    "\n",
    "**Sample Generation:**\n",
    "```python\n",
    "# Position 0: x = [1, 2, 3, 4], y = [2, 3, 4, 5]\n",
    "# Position 1: x = [2, 3, 4, 5], y = [3, 4, 5, 6]  \n",
    "# Position 2: x = [3, 4, 5, 6], y = [4, 5, 6, 7]\n",
    "# Position 3: x = [4, 5, 6, 7], y = [5, 6, 7, 8]\n",
    "# Position 4: x = [5, 6, 7, 8], y = [6, 7, 8, 9]\n",
    "```\n",
    "\n",
    "###### Token-by-Token Prediction Logic\n",
    "\n",
    "For each input-target pair, the model learns multiple next-token predictions simultaneously:\n",
    "\n",
    "**Example with Position 0:**\n",
    "```python\n",
    "Input:  [1, 2, 3, 4]\n",
    "Target: [2, 3, 4, 5]\n",
    "\n",
    "# Training signals:\n",
    "# Given context [1] → predict 2\n",
    "# Given context [1, 2] → predict 3  \n",
    "# Given context [1, 2, 3] → predict 4\n",
    "# Given context [1, 2, 3, 4] → predict 5\n",
    "```\n",
    "\n",
    "###### Implementation Analysis\n",
    "\n",
    "**Memory Efficiency:**\n",
    "The dataset doesn't store all possible sequences but generates them on-demand using tensor slicing, making it memory-efficient for large corpora.\n",
    "\n",
    "**Boundary Handling:**\n",
    "```python\n",
    "def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "```\n",
    "This ensures every generated sequence has exactly `block_size` input tokens and `block_size` target tokens, preventing index overflow.\n",
    "\n",
    "**Tensor Slicing:**\n",
    "```python\n",
    "x = self.data[pos:pos + block_size]        # Input: 4 tokens\n",
    "y = self.data[pos + 1:pos + 1 + block_size] # Target: 4 tokens (shifted)\n",
    "```\n",
    "\n",
    "###### Integration with Training Loop\n",
    "\n",
    "**DataLoader Usage:**\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch_x, batch_y in dataloader:\n",
    "    # batch_x shape: (32, block_size)  \n",
    "    # batch_y shape: (32, block_size)\n",
    "    predictions = model(batch_x)\n",
    "    loss = criterion(predictions.view(-1, vocab_size), batch_y.view(-1))\n",
    "```\n",
    "\n",
    "The `-1` in `.view(-1, vocab_size)` tells PyTorch to automatically calculate that dimension based on the tensor's total size. Here's why this is necessary:\n",
    "\n",
    "###### Tensor Shape Problem\n",
    "\n",
    "**Before reshaping:**\n",
    "```python\n",
    "# predictions shape: (batch_size, block_size, vocab_size)\n",
    "# batch_y shape: (batch_size, block_size)\n",
    "\n",
    "# Example with batch_size=32, block_size=4, vocab_size=1000:\n",
    "predictions.shape = torch.Size([32, 4, 1000])\n",
    "batch_y.shape = torch.Size([32, 4])\n",
    "```\n",
    "\n",
    "**Loss function requirement:**\n",
    "Most loss functions expect:\n",
    "- Predictions: 2D tensor `(num_samples, num_classes)`\n",
    "- Targets: 1D tensor `(num_samples,)`\n",
    "\n",
    "###### What `.view(-1, vocab_size)` Does\n",
    "\n",
    "**Automatic dimension calculation:**\n",
    "```python\n",
    "predictions.view(-1, vocab_size)\n",
    "# PyTorch calculates: total_elements / vocab_size = first_dimension\n",
    "# (32 * 4 * 1000) / 1000 = 128\n",
    "# Result shape: (128, 1000)\n",
    "\n",
    "batch_y.view(-1)  \n",
    "# Flattens to: (128,)\n",
    "```\n",
    "\n",
    "**Step-by-step breakdown:**\n",
    "```python\n",
    "# Original: (32, 4, 1000) - 32 batches, 4 tokens each, 1000 vocab probabilities\n",
    "# Reshaped: (128, 1000) - 128 individual token predictions, 1000 vocab probabilities\n",
    "\n",
    "# Original targets: (32, 4) - 32 batches, 4 target tokens each  \n",
    "# Reshaped: (128,) - 128 individual target tokens\n",
    "```\n",
    "\n",
    "###### Why Use -1 Instead of Hard-coding?\n",
    "\n",
    "**Flexibility:**\n",
    "```python\n",
    "# Hard-coded (brittle):\n",
    "predictions.view(32 * 4, vocab_size)  # Breaks if batch_size changes\n",
    "\n",
    "# Auto-calculated (robust):  \n",
    "predictions.view(-1, vocab_size)  # Works with any batch_size\n",
    "```\n",
    "\n",
    "**Real example:**\n",
    "```python\n",
    "batch_size = 32\n",
    "block_size = 4\n",
    "vocab_size = 1000\n",
    "\n",
    "# Before reshaping\n",
    "print(predictions.shape)     # torch.Size([32, 4, 1000])\n",
    "print(batch_y.shape)         # torch.Size([32, 4])\n",
    "\n",
    "# After reshaping  \n",
    "print(predictions.view(-1, vocab_size).shape)  # torch.Size([128, 1000])\n",
    "print(batch_y.view(-1).shape)                  # torch.Size([128])\n",
    "```\n",
    "\n",
    "###### What This Accomplishes\n",
    "\n",
    "The reshaping converts from \"batch-of-sequences\" format to \"individual-predictions\" format, where each token prediction is treated as a separate classification problem. This allows the loss function to compute the cross-entropy between each predicted token distribution and its corresponding target token.\n",
    "\n",
    "The `-1` makes the code robust to different batch sizes without requiring manual calculation of the flattened dimension.\n",
    "\n",
    "\n",
    "###### Practical Considerations\n",
    "\n",
    "**Context Length Trade-offs:**\n",
    "- Larger `block_size`: Better long-range dependencies, more memory usage\n",
    "- Smaller `block_size`: Less memory, limited context understanding\n",
    "\n",
    "**Data Utilization:**\n",
    "From a sequence of length N with block size B, the dataset generates N-B training examples, maximizing data utilization through overlapping windows.\n",
    "\n",
    "**Computational Efficiency:**\n",
    "The sliding window approach creates multiple training examples from a single sequence, effectively augmenting the training data without additional storage requirements.\n",
    "\n",
    "This dataset design is fundamental to training transformer-based language models, providing the structured input-target pairs necessary for learning autoregressive text generation through next-token prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  \n",
    "  \"\"\"\n",
    "  A PyTorch Dataset for creating input-target pairs for language model training.\n",
    "\n",
    "  This dataset takes a long sequence of token IDs and a specified block size\n",
    "  (context length) to generate pairs of (input, target) tensors. The input `x`\n",
    "  is a chunk of the data, and the target `y` is the same chunk shifted by one\n",
    "  position to the right. This setup is standard for training a model to predict\n",
    "  the next token in a sequence.\n",
    "\n",
    "  For example, if the data is [1, 2, 3, 4, 5] and block_size is 3:\n",
    "  - A possible `x` would be [1, 2, 3].\n",
    "  - The corresponding `y` would be [2, 3, 4].\n",
    "  \"\"\"\n",
    "  def __init__(self, data, block_size):\n",
    "    \"\"\"\n",
    "    Initializes the dataset.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): A 1D tensor containing the entire sequence of\n",
    "            token IDs for the text corpus.\n",
    "        block_size (int): The context length or the size of the input\n",
    "            sequences to be generated.\n",
    "    \"\"\"\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    Returns the total number of possible sequences that can be generated.\n",
    "\n",
    "    The length is the total number of tokens minus the block size, as this\n",
    "    represents the number of possible starting positions for a full sequence.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of samples in the dataset.\n",
    "    \"\"\"\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    \"\"\"\n",
    "    Retrieves a single input-target pair at a given position.\n",
    "\n",
    "    Args:\n",
    "        pos (int): The starting index in the data tensor from which to\n",
    "            create the sequence.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: A tuple containing the input\n",
    "            tensor `x` and the target tensor `y`.\n",
    "    \"\"\"\n",
    "    # Ensure the requested position is valid.\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    # The input sequence starts at `pos` and has length `block_size`.\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    # The target sequence is shifted by one token to the right.\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The `//` operator performs **floor division** (integer division), which is crucial for ensuring the head size is always an integer.\n",
    "\n",
    "###### Floor Division vs Regular Division\n",
    "\n",
    "**Regular division (`/`):**\n",
    "```python\n",
    "embedding_dim = 768\n",
    "heads_num = 12\n",
    "\n",
    "head_size = embedding_dim / heads_num\n",
    "# Result: 64.0 (float)\n",
    "```\n",
    "\n",
    "**Floor division (`//`):**\n",
    "```python\n",
    "head_size = embedding_dim // heads_num  \n",
    "# Result: 64 (integer)\n",
    "```\n",
    "\n",
    "###### Why This Matters for Neural Networks\n",
    "\n",
    "**Integer requirement:**\n",
    "Neural network dimensions must be integers. You cannot have 64.5 neurons or create a tensor with fractional dimensions.\n",
    "\n",
    "**Example where it makes a difference:**\n",
    "```python\n",
    "embedding_dim = 770  # Not perfectly divisible\n",
    "heads_num = 12\n",
    "\n",
    "regular_division = embedding_dim / heads_num  # 64.16666... (float)\n",
    "floor_division = embedding_dim // heads_num   # 64 (integer)\n",
    "```\n",
    "\n",
    "###### Multi-Head Attention Context\n",
    "\n",
    "In transformer architecture, the embedding dimension must be evenly divided among attention heads:\n",
    "```python\n",
    "# Each attention head gets head_size dimensions\n",
    "# Total: heads_num × head_size = embedding_dim\n",
    "\n",
    "12 heads × 64 dimensions = 768 total embedding dimensions\n",
    "```\n",
    "\n",
    "**Configuration validation:**\n",
    "```python\n",
    "assert embedding_dim % heads_num == 0, \"embedding_dim must be divisible by heads_num\"\n",
    "head_size = embedding_dim // heads_num\n",
    "```\n",
    "\n",
    "The `//` operator ensures you get a clean integer division result, which is essential for creating properly sized tensor operations in the attention mechanism. Using regular division would produce floats that would cause errors when used as tensor dimension specifications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 256,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Single-Head Attention Mechanism Implementation\n",
    "\n",
    "###### Core Purpose\n",
    "\n",
    "The `AttentionHead` class implements a single attention head from the multi-head attention mechanism used in transformer architectures. It performs the fundamental attention operation: allowing each token to attend to (focus on) relevant tokens in the sequence while respecting causal constraints for language modeling.\n",
    "\n",
    "###### Architecture Components\n",
    "\n",
    "**Linear Projection Layers:**\n",
    "```python\n",
    "self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])  \n",
    "self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "```\n",
    "\n",
    "These layers project the input embeddings into three different subspaces:\n",
    "- **Query (Q)**: What the current token is looking for\n",
    "- **Key (K)**: What each token represents/offers  \n",
    "- **Value (V)**: The actual content each token contributes\n",
    "\n",
    "###### Causal Attention Mask\n",
    "\n",
    "```python\n",
    "casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "```\n",
    "\n",
    "**Purpose:** Ensures tokens can only attend to previous tokens (including themselves), not future tokens. This is crucial for autoregressive language modeling.\n",
    "\n",
    "**Structure:** Lower triangular matrix where:\n",
    "- 1 = allowed attention (current and previous positions)\n",
    "- 0 = blocked attention (future positions)\n",
    "\n",
    "**Example for context_size=4:**\n",
    "```\n",
    "[[1, 0, 0, 0],\n",
    " [1, 1, 0, 0], \n",
    " [1, 1, 1, 0],\n",
    " [1, 1, 1, 1]]\n",
    "```\n",
    "\n",
    "###### Forward Pass Breakdown\n",
    "\n",
    "**Step 1: Linear Projections**\n",
    "```python\n",
    "Q = self.Q_weights(input) # (B, C, head_size)\n",
    "K = self.K_weights(input) # (B, C, head_size)  \n",
    "V = self.V_weights(input) # (B, C, head_size)\n",
    "```\n",
    "Input shape: `(batch_size, sequence_length, embedding_dim)`\n",
    "Output shapes: `(batch_size, sequence_length, head_size)`\n",
    "\n",
    "**Step 2: Attention Score Computation**\n",
    "```python\n",
    "attention_scores = Q @ K.transpose(1, 2)  # (B, C, C)\n",
    "```\n",
    "Computes similarity between queries and keys using dot product. Result is a `(batch_size, sequence_length, sequence_length)` matrix where `attention_scores[i,j]` represents how much token `i` should attend to token `j`.\n",
    "\n",
    "**Step 3: Causal Masking**\n",
    "```python\n",
    "attention_scores = attention_scores.masked_fill(\n",
    "    self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "    -torch.inf\n",
    ")\n",
    "```\n",
    "Sets attention scores for future positions to negative infinity, ensuring they become zero after softmax.\n",
    "\n",
    "**Step 4: Scaled Dot-Product Attention**\n",
    "```python\n",
    "attention_scores = attention_scores / (K.shape[-1] ** 0.5)\n",
    "```\n",
    "Scales by `√(head_size)` to prevent softmax saturation with large dot products.\n",
    "\n",
    "**Step 5: Attention Probabilities**\n",
    "```python\n",
    "attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "attention_scores = self.dropout(attention_scores)\n",
    "```\n",
    "Converts scores to probabilities that sum to 1 for each query position, then applies dropout for regularization.\n",
    "\n",
    "**Step 6: Weighted Value Aggregation**\n",
    "```python\n",
    "return attention_scores @ V # (B, C, head_size)\n",
    "```\n",
    "Multiplies attention probabilities with values to get the final attended representation.\n",
    "\n",
    "###### Mathematical Formulation\n",
    "\n",
    "The complete attention operation can be expressed as:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $M$ is the causal mask (0 for allowed, $-\\infty$ for blocked)\n",
    "- $d_k$ is the head size (key dimension)\n",
    "\n",
    "###### Key Design Decisions\n",
    "\n",
    "**Head Size Calculation:** `head_size = embedding_dim // heads_num` ensures the total dimension is preserved when multiple heads are concatenated.\n",
    "\n",
    "**Buffer Registration:** Using `register_buffer` for the mask ensures it moves with the model to GPU/CPU without being treated as a trainable parameter.\n",
    "\n",
    "**Dropout Placement:** Applied to attention weights rather than the final output, providing regularization on the attention patterns themselves.\n",
    "\n",
    "This implementation forms the building block for multi-head attention, where multiple such heads operate in parallel and their outputs are concatenated to capture different types of relationships in the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "attention_scores = Q @ K.transpose(1, 2)  # Shape: (B, T, T)\n",
    "```\n",
    "\n",
    "The `@` symbol is the **matrix multiplication** operator in Python.\n",
    "\n",
    "In the context of PyTorch and NumPy, it's used to perform matrix multiplication on tensors or arrays. It's a more readable, infix alternative to calling a function like `torch.matmul()`.\n",
    "\n",
    "##### In Your Code: `attention_scores = Q @ K.transpose(1, 2)`\n",
    "\n",
    "Let's break down this specific line:\n",
    "\n",
    "1.  **`Q`**: This is the \"Query\" tensor, with a shape of `(Batch, Tokens, Head_size)`.\n",
    "2.  **`K`**: This is the \"Key\" tensor, also with a shape of `(Batch, Tokens, Head_size)`.\n",
    "3.  **`K.transpose(1, 2)`**: This transposes the Key tensor, swapping its last two dimensions. Its new shape becomes `(Batch, Head_size, Tokens)`. This is done to make the dimensions compatible for matrix multiplication.\n",
    "4.  **`Q @ ...`**: This performs the matrix multiplication:\n",
    "    *   `Q` shape: `(B, T, H)`\n",
    "    *   `K.transpose` shape: `(B, H, T)`\n",
    "    *   Resulting `attention_scores` shape: `(B, T, T)`\n",
    "\n",
    "The resulting `(B, T, T)` tensor holds the attention scores, where each token in the sequence has a score indicating how much it should \"attend to\" every other token.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single head of self-attention for a transformer model.\n",
    "\n",
    "    This module implements the scaled dot-product attention mechanism. It takes\n",
    "    a sequence of token embeddings and computes a new representation for each\n",
    "    token by attending to all other tokens in the sequence. It learns three\n",
    "    linear projections (Query, Key, Value) to transform the input embeddings.\n",
    "\n",
    "    The key components are:\n",
    "    - Q, K, V linear layers to project the input.\n",
    "    - A causal mask to prevent tokens from attending to future tokens.\n",
    "    - Scaled dot-product attention calculation.\n",
    "    - Dropout for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the AttentionHead module.\n",
    "\n",
    "        Args:\n",
    "            config (dict): A configuration dictionary containing the following keys:\n",
    "                - \"embedding_dim\" (int): The dimensionality of the input token embeddings.\n",
    "                - \"head_size\" (int): The dimensionality of the Query, Key, and Value projections.\n",
    "                - \"use_bias\" (bool): Whether to use a bias term in the linear layers.\n",
    "                - \"dropout_rate\" (float): The dropout rate to apply to the attention scores.\n",
    "                - \"context_size\" (int): The maximum sequence length (block size).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layers to project input embeddings into Query, Key, and Value spaces.\n",
    "        self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "        self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "        self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "\n",
    "        # Dropout layer to regularize attention scores.\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "        # Create a lower triangular matrix for the causal attention mask.\n",
    "        # This prevents tokens from attending to future tokens in the sequence.\n",
    "        casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "        \n",
    "        # `register_buffer` makes the mask a part of the module's state, but not\n",
    "        # a parameter to be trained. This ensures it's moved to the correct\n",
    "        # device (e.g., GPU) along with the model.\n",
    "        self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            input_embeddings (torch.Tensor): A tensor of shape (B, T, E) where\n",
    "                B is the batch size, T is the sequence length (tokens_num), and\n",
    "                E is the embedding dimension.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (B, T, H) where H is the\n",
    "                head size. This is the weighted aggregation of the Value vectors.\n",
    "        \"\"\"\n",
    "        batch_size, tokens_num, embedding_dim = input_embeddings.shape\n",
    "        \n",
    "        # 1. Project input into Query, Key, and Value tensors.\n",
    "        Q = self.Q_weights(input_embeddings) # Shape: (B, T, H)\n",
    "        K = self.K_weights(input_embeddings) # Shape: (B, T, H)\n",
    "        V = self.V_weights(input_embeddings) # Shape: (B, T, H)\n",
    "\n",
    "        # 2. Calculate attention scores by taking the dot product of Q and K.\n",
    "        # K is transposed to align dimensions for matrix multiplication.\n",
    "        attention_scores = Q @ K.transpose(1, 2)  # Shape: (B, T, T)\n",
    "\n",
    "        # 3. Apply the causal mask to prevent future-peeking.\n",
    "        # We set the scores for future positions to negative infinity so that\n",
    "        # they become zero after the softmax operation.\n",
    "        attention_scores = attention_scores.masked_fill(\n",
    "            self.casual_attention_mask[:tokens_num, :tokens_num] == 0,\n",
    "            -torch.inf\n",
    "        )\n",
    "        \n",
    "        # 4. Scale the attention scores to stabilize gradients.\n",
    "        # This is divided by the square root of the Key dimension (head_size).\n",
    "        attention_scores = attention_scores / (K.shape[-1] ** 0.5)\n",
    "        \n",
    "        # 5. Apply softmax to convert scores into probability distributions (weights).\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 6. Apply dropout for regularization.\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "        # 7. Compute the final output by taking a weighted sum of the Value vectors.\n",
    "        return attention_scores @ V # Shape: (B, T, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = AttentionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ah(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Multi-Head Attention Implementation\n",
    "\n",
    "###### Core Purpose\n",
    "\n",
    "The `MultiHeadAttention` class implements the complete multi-head attention mechanism by combining multiple parallel attention heads and processing their combined output. This allows the model to attend to different types of relationships and patterns simultaneously across multiple representation subspaces.\n",
    "\n",
    "###### Architecture Overview\n",
    "\n",
    "**Parallel Head Structure:**\n",
    "```python\n",
    "heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "self.heads = nn.ModuleList(heads_list)\n",
    "```\n",
    "\n",
    "Creates multiple independent attention heads (typically 12) that operate in parallel, each focusing on different aspects of the input relationships.\n",
    "\n",
    "**Output Processing:**\n",
    "```python\n",
    "self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "```\n",
    "\n",
    "A linear projection layer that processes the concatenated head outputs, followed by dropout for regularization.\n",
    "\n",
    "###### Forward Pass Breakdown\n",
    "\n",
    "**Step 1: Parallel Head Computation**\n",
    "```python\n",
    "heads_outputs = [head(input) for head in self.heads]\n",
    "```\n",
    "\n",
    "Each attention head processes the input independently:\n",
    "- Input shape: `(batch_size, sequence_length, embedding_dim)`\n",
    "- Each head output shape: `(batch_size, sequence_length, head_size)`\n",
    "- Number of outputs: `heads_num` (e.g., 12)\n",
    "\n",
    "**Step 2: Concatenation**\n",
    "```python\n",
    "scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "```\n",
    "\n",
    "Concatenates all head outputs along the feature dimension:\n",
    "- Individual head: `(B, C, head_size)` where `head_size = embedding_dim // heads_num`\n",
    "- After concatenation: `(B, C, heads_num × head_size) = (B, C, embedding_dim)`\n",
    "\n",
    "**Numerical Example:**\n",
    "```python\n",
    "# Config: embedding_dim=768, heads_num=12, head_size=64\n",
    "# Input: (32, 256, 768)  # batch_size=32, sequence_length=256\n",
    "\n",
    "# Each head output: (32, 256, 64)\n",
    "# After concatenation: (32, 256, 768)  # 12 × 64 = 768\n",
    "```\n",
    "\n",
    "**Step 3: Linear Projection**\n",
    "```python\n",
    "scores_change = self.linear(scores_change)\n",
    "```\n",
    "\n",
    "Applies a learned linear transformation to the concatenated outputs:\n",
    "- Weight matrix: `(embedding_dim, embedding_dim)` = `(768, 768)`\n",
    "- Allows heads to interact and combine their representations\n",
    "- Maintains the original embedding dimension\n",
    "\n",
    "**Step 4: Regularization**\n",
    "```python\n",
    "return self.dropout(scores_change)\n",
    "```\n",
    "\n",
    "Applies dropout to prevent overfitting on the attention patterns.\n",
    "\n",
    "###### Why Multiple Heads?\n",
    "\n",
    "**Representation Diversity:**\n",
    "Each head can specialize in different types of relationships:\n",
    "- Head 1: Syntactic dependencies (subject-verb relationships)\n",
    "- Head 2: Semantic similarity (related concepts)\n",
    "- Head 3: Positional patterns (sequential ordering)\n",
    "- Head 4: Long-range dependencies (paragraph-level connections)\n",
    "\n",
    "**Parallel Processing:**\n",
    "All heads compute simultaneously, making the operation efficient while capturing multiple perspectives on the same input.\n",
    "\n",
    "###### Mathematical Formulation\n",
    "\n",
    "The complete multi-head attention can be expressed as:\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "Where:\n",
    "- $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "- $W^O$ is the output projection matrix (`self.linear`)\n",
    "- $h$ is the number of heads\n",
    "\n",
    "###### Dimension Preservation\n",
    "\n",
    "**Key insight:** The total computational cost remains similar to single-head attention:\n",
    "- Single head: `(B, C, embedding_dim)` → `(B, C, embedding_dim)`\n",
    "- Multi-head: `heads_num × (B, C, head_size)` → `(B, C, embedding_dim)`\n",
    "- Total parameters: Similar due to dimension splitting\n",
    "\n",
    "**Head Size Relationship:**\n",
    "```python\n",
    "head_size = embedding_dim // heads_num  # 768 // 12 = 64\n",
    "total_dim = heads_num × head_size       # 12 × 64 = 768\n",
    "```\n",
    "\n",
    "###### Integration Benefits\n",
    "\n",
    "**Enhanced Representation:** Captures multiple types of attention patterns simultaneously rather than learning a single averaged attention pattern.\n",
    "\n",
    "**Computational Efficiency:** Parallel computation across heads with dimension splitting maintains reasonable computational cost.\n",
    "\n",
    "**Learning Flexibility:** Different heads can specialize during training without interfering with each other's learning process.\n",
    "\n",
    "This multi-head structure is fundamental to transformer architectures, enabling the model to build rich, multi-faceted representations of sequence relationships that single-head attention cannot achieve.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Attention mechanism for a transformer model.\n",
    "\n",
    "    This module runs multiple self-attention \"heads\" in parallel and then\n",
    "    concatenates their outputs. This allows the model to jointly attend to\n",
    "    information from different representation subspaces at different positions.\n",
    "    A final linear layer is applied to the concatenated outputs to produce\n",
    "    the final result.\n",
    "\n",
    "    This architecture is a core component of the Transformer model, enabling it\n",
    "    to capture a richer variety of relationships within the input sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadAttention module.\n",
    "\n",
    "        Args:\n",
    "            config (dict): A configuration dictionary containing the following keys:\n",
    "                - \"heads_num\" (int): The number of parallel attention heads to use.\n",
    "                - \"embedding_dim\" (int): The dimensionality of the input and output.\n",
    "                - \"dropout_rate\" (float): The dropout rate for the final output.\n",
    "                - Other keys required by `AttentionHead` (head_size, use_bias, etc.).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a list of `AttentionHead` modules, one for each head.\n",
    "        # `nn.ModuleList` is used to properly register all the heads as sub-modules.\n",
    "        heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "        self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "        # A final linear layer to project the concatenated head outputs back\n",
    "        # to the original embedding dimension.\n",
    "        self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        \n",
    "        # A dropout layer for regularization on the final output.\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for Multi-Head Attention.\n",
    "\n",
    "        Args:\n",
    "            input_embeddings (torch.Tensor): A tensor of shape (B, T, E) where\n",
    "                B is the batch size, T is the sequence length, and E is the\n",
    "                embedding dimension.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final output tensor of shape (B, T, E).\n",
    "        \"\"\"\n",
    "        # 1. Run each attention head in parallel on the same input.\n",
    "        # This results in a list of output tensors, each of shape (B, T, H).\n",
    "        heads_outputs = [head(input_embeddings) for head in self.heads]\n",
    "\n",
    "        # 2. Concatenate the outputs of all heads along the last dimension.\n",
    "        # If we have N heads, the shape becomes (B, T, N * H).\n",
    "        # Note: For this to work, N * H must equal the embedding_dim.\n",
    "        concatenated_heads = torch.cat(heads_outputs, dim=-1)\n",
    "\n",
    "        # 3. Pass the concatenated output through a final linear layer.\n",
    "        # This projects the combined attention information back to the original\n",
    "        # embedding dimension, shape (B, T, E).\n",
    "        projected_output = self.linear(concatenated_heads)\n",
    "        \n",
    "        # 4. Apply dropout for regularization.\n",
    "        return self.dropout(projected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
