{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from get_device import get_device\n",
    "\n",
    "# Use CUDA if available\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('../../data/tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 256,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "    self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, tokens_num, embedding_dim = input.shape\n",
    "    Q = self.Q_weights(input)\n",
    "    K = self.K_weights(input)\n",
    "    V = self.V_weights(input)\n",
    "\n",
    "    attention_scores = Q @ K.transpose(1, 2)\n",
    "    attention_scores = attention_scores.masked_fill(\n",
    "        self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "        -torch.inf\n",
    "    )\n",
    "    attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = AttentionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ah(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "    self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "    self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    # print(f\"Input shape: {input.shape}\")\n",
    "    heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "    scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "    # print(f\"heads shape: {scores_change.shape}\")\n",
    "\n",
    "    scores_change = self.linear(scores_change)\n",
    "    return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
    "        nn.Dropout(config[\"dropout_rate\"])\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = ff(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multi_head = MultiHeadAttention(config)\n",
    "    self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    self.feed_forward = FeedForward(config)\n",
    "    self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    residual = input\n",
    "    x = self.multi_head(self.layer_norm_1(input))\n",
    "    x = x + residual\n",
    "\n",
    "    residual = x\n",
    "    x = self.feed_forward(self.layer_norm_2(x))\n",
    "    return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = b(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
    "    self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
    "\n",
    "    blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "    self.layers = nn.Sequential(*blocks)\n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "    self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
    "\n",
    "  def forward(self, token_ids):\n",
    "    # print(\"Forward\")\n",
    "    batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "    x = self.token_embedding_layer(token_ids)\n",
    "    sequence = torch.arange(tokens_num, device=device)\n",
    "    x = x + self.positional_embedding_layer(sequence)\n",
    "\n",
    "    x = self.layers(x)\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.unembedding(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 65])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate(model, prompt_ids, max_tokens):\n",
    "#     output_ids = prompt_ids\n",
    "#     for _ in range(max_tokens):\n",
    "#       if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "#         break\n",
    "#       with torch.no_grad():\n",
    "#         logits = model(output_ids)\n",
    "\n",
    "#       logits = logits[:, -1, :]\n",
    "#       probs = F.softmax(logits, dim=-1)\n",
    "#       # Sample a random token given the softmax distribution\n",
    "#       next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "#       # Add new token to the output, and repeat the process\n",
    "#       output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "#     return output_ids\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, prompt_ids, max_tokens_to_generate, config):\n",
    "    \"\"\"\n",
    "    Generates a sequence of tokens auto-regressively from a given prompt.\n",
    "\n",
    "    This function takes a trained model and a starting sequence of token IDs\n",
    "    (the prompt) and generates new tokens one by one. In each step, it uses\n",
    "    the model to predict the next token, samples from the resulting probability\n",
    "    distribution, and appends the new token to the sequence, which then becomes\n",
    "    the input for the next step.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained DemoGPT transformer model.\n",
    "        prompt_ids (torch.Tensor): A tensor of shape (B, T) containing the\n",
    "            initial token IDs to start generation from. B is the batch size\n",
    "            (usually 1 for generation) and T is the length of the prompt.\n",
    "        max_tokens_to_generate (int): The maximum number of new tokens to generate after the prompt.\n",
    "        config (dict): The model's configuration dictionary, used to access\n",
    "            the `context_size`.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (B, T + generated_tokens) containing\n",
    "            the original prompt plus the newly generated tokens.\n",
    "    \"\"\"\n",
    "    # Start with the initial prompt.\n",
    "    output_ids = prompt_ids\n",
    "    \n",
    "    # Loop to generate tokens one by one.\n",
    "    for _ in range(max_tokens_to_generate):\n",
    "      # Stop if the context window is full.\n",
    "      if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "        break\n",
    "        \n",
    "      # Use torch.no_grad() to disable gradient calculations, as we are only\n",
    "      # doing inference, which saves memory and computation.\n",
    "      with torch.no_grad():\n",
    "        # Get the model's predictions (logits) for the current sequence.\n",
    "        logits = model(output_ids)\n",
    "\n",
    "      # Focus only on the logits for the very last token in the sequence,\n",
    "      # as that's the prediction for the *next* token.\n",
    "      last_token_logits = logits[:, -1, :]\n",
    "      \n",
    "      # Apply softmax to convert the logits into a probability distribution.\n",
    "      probs = F.softmax(last_token_logits, dim=-1)\n",
    "      \n",
    "      # Sample one token from the probability distribution.\n",
    "      # `torch.multinomial` treats the input as a set of weights for sampling.\n",
    "      next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "      \n",
    "      # Append the newly sampled token ID to our sequence.\n",
    "      output_ids = torch.cat([output_ids, next_token_id], dim=1)\n",
    "      \n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "#   model.eval()\n",
    "\n",
    "#   prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "#   return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])\n",
    "\n",
    "# def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "#   model.eval()\n",
    "\n",
    "#   prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "#   return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])\n",
    "\n",
    "\n",
    "def generate_with_prompt(model, tokenizer, config, prompt, max_tokens_to_generate=100):\n",
    "  \"\"\"\n",
    "  Generates text from a prompt using the specified model and tokenizer.\n",
    "\n",
    "  This function sets the model to evaluation mode, encodes the prompt, calls\n",
    "  the `generate` function to produce token IDs, and decodes them back into\n",
    "  human-readable text.\n",
    "\n",
    "  Args:\n",
    "      model (nn.Module): The trained transformer model.\n",
    "      tokenizer (CharTokenizer): The tokenizer for encoding/decoding text.\n",
    "      config (dict): The model's configuration dictionary.\n",
    "      prompt (str): The initial text to start generation from.\n",
    "      max_tokens_to_generate (int): The maximum number of new tokens to create.\n",
    "\n",
    "  Returns:\n",
    "      str: The generated text, including the original prompt.\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "\n",
    "  prompt_ids = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "  # Call the generate function with the correct arguments\n",
    "  generated_ids = generate(\n",
    "      model,\n",
    "      prompt_ids,\n",
    "      max_tokens_to_generate=max_tokens_to_generate,\n",
    "      config=config\n",
    "  )\n",
    "\n",
    "  return tokenizer.decode(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nm'AG,LAYaG\\nFluD&xAgV:hh&fXlDnhIlDcm;$q$vM?wIYStxvQGetvzXaO?zufGTlU.S\\nqpXrFMoS3s3DccTdfcLY!kArHvbh?cJ\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")\n",
    "\n",
    "generate_with_prompt(model, tokenizer, config, \"First Citizen:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# train_iterations = 5000\n",
    "train_iterations = 20\n",
    "evaluation_interval = 100\n",
    "learning_rate=4e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenizer.encode(text).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TokenIdsDataset(train_data, config[\"context_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset, num_samples=batch_size * train_iterations, replacement=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0. Loss 1.632\n",
      "Demo GPT:\n",
      "\n",
      "IsIUSTERIV:\n",
      "AyUELIO:\n",
      "I VINDUS:\n",
      "SIO:\n",
      "PoaisO, I'lbmory, Not Morksigor: God't, KING EDWARD:\n",
      "Upothou IUQ\n",
      "Step 1. Loss 2.850\n",
      "Step 2. Loss 2.318\n",
      "Step 3. Loss 2.061\n",
      "Step 4. Loss 1.941\n"
     ]
    }
   ],
   "source": [
    "for step_num, sample in enumerate(train_dataloader):\n",
    "\n",
    "    model.train()\n",
    "    input, targets = sample\n",
    "    logits = model(input)\n",
    "\n",
    "    logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "    targets_view = targets.view(batch_size * config[\"context_size\"])\n",
    "    \n",
    "    loss = F.cross_entropy(logits_view, targets_view)\n",
    "    # Backward propagation\n",
    "    loss.backward()\n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "    # Set to None to reduce memory usage\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    print(f\"Step {step_num}. Loss {loss.item():.3f}\")\n",
    "\n",
    "    if step_num % evaluation_interval == 0:\n",
    "      print(\"Demo GPT:\\n\" + generate_with_prompt(model, tokenizer, config, \"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
