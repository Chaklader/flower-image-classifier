{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from get_device import get_device\n",
    "\n",
    "\n",
    "# Use CUDA if available\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('../../data/tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 256,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "    self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, tokens_num, embedding_dim = input.shape\n",
    "    Q = self.Q_weights(input)\n",
    "    K = self.K_weights(input)\n",
    "    V = self.V_weights(input)\n",
    "\n",
    "    attention_scores = Q @ K.transpose(1, 2)\n",
    "    attention_scores = attention_scores.masked_fill(\n",
    "        self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "        -torch.inf\n",
    "    )\n",
    "    attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
    "\n",
    "ah = AttentionHead(config)\n",
    "\n",
    "output = ah(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "    self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "    self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    # print(f\"Input shape: {input.shape}\")\n",
    "    heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "    scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "    # print(f\"heads shape: {scores_change.shape}\")\n",
    "\n",
    "    scores_change = self.linear(scores_change)\n",
    "    return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Feed-Forward Network in Transformer Architecture\n",
    "\n",
    "```python\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network used in transformer blocks.\n",
    "    \n",
    "    This module implements a two-layer fully connected network with GELU activation\n",
    "    that processes each position in the sequence independently. It expands the \n",
    "    embedding dimension by a factor of 4, applies non-linear activation, then\n",
    "    projects back to the original dimension.\n",
    "    \n",
    "    The architecture follows the standard transformer design:\n",
    "    embedding_dim → 4×embedding_dim → embedding_dim\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing:\n",
    "            - embedding_dim (int): The input/output dimension size\n",
    "            - dropout_rate (float): Dropout probability for regularization\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "##### Architecture Overview\n",
    "\n",
    "**Two-Layer Structure:**\n",
    "```python\n",
    "nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),  # Expansion\n",
    "nn.GELU(),                                                        # Activation\n",
    "nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]), # Contraction\n",
    "nn.Dropout(config[\"dropout_rate\"])                               # Regularization\n",
    "```\n",
    "\n",
    "**Dimension Transformation:**\n",
    "- Input: `(batch_size, sequence_length, embedding_dim)`\n",
    "- Hidden: `(batch_size, sequence_length, embedding_dim × 4)`\n",
    "- Output: `(batch_size, sequence_length, embedding_dim)`\n",
    "\n",
    "##### GELU Activation Function\n",
    "\n",
    "**GELU (Gaussian Error Linear Unit)** is defined mathematically as:\n",
    "$$\\text{GELU}(x) = x \\cdot \\Phi(x)$$\n",
    "where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution.\n",
    "\n",
    "**Approximation used in practice:**\n",
    "$$\\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\right)\\right)$$\n",
    "\n",
    "**Properties of GELU:**\n",
    "- **Smooth**: Unlike ReLU, GELU is differentiable everywhere\n",
    "- **Non-monotonic**: Has a slight negative region for small negative inputs\n",
    "- **Probabilistic**: Based on probability theory rather than hard thresholding\n",
    "- **Better gradients**: Smoother gradients compared to ReLU variants\n",
    "\n",
    "**Comparison with other activations:**\n",
    "```python\n",
    "# ReLU: max(0, x) - hard cutoff at zero\n",
    "# GELU: x * Φ(x) - smooth probabilistic gating\n",
    "# Swish: x * sigmoid(x) - similar smooth properties\n",
    "```\n",
    "\n",
    "##### Why 4x Expansion?\n",
    "\n",
    "**Computational Capacity:**\n",
    "The 4x expansion provides sufficient representational capacity for complex transformations while maintaining computational efficiency.\n",
    "\n",
    "**Parameter Distribution:**\n",
    "```python\n",
    "# Example with embedding_dim = 768\n",
    "# Layer 1: 768 → 3072 (768 × 4) = 2,359,296 parameters\n",
    "# Layer 2: 3072 → 768 = 2,359,296 parameters  \n",
    "# Total: ~4.7M parameters per feed-forward block\n",
    "```\n",
    "\n",
    "**Information Processing:**\n",
    "- **Expansion phase**: Projects to higher-dimensional space for complex pattern recognition\n",
    "- **Contraction phase**: Compresses back to original dimension while preserving learned features\n",
    "\n",
    "##### Position-wise Processing\n",
    "\n",
    "**Independent Processing:**\n",
    "Each position in the sequence is processed independently - the same transformation is applied to every token position without interaction between positions.\n",
    "\n",
    "**Mathematical representation:**\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "(using ReLU in original paper, but GELU in modern implementations)\n",
    "\n",
    "##### Role in Transformer Architecture\n",
    "\n",
    "**Complementary to Attention:**\n",
    "- **Attention**: Models relationships between positions\n",
    "- **Feed-forward**: Processes individual positions with non-linear transformations\n",
    "\n",
    "**Residual Connection Context:**\n",
    "The feed-forward output is typically added to its input via residual connections:\n",
    "```python\n",
    "# In transformer block:\n",
    "x = x + attention(x)\n",
    "x = x + feedforward(x)  # This module\n",
    "```\n",
    "\n",
    "**Learning Capacity:**\n",
    "The feed-forward network often contains the majority of parameters in a transformer model, providing substantial learning capacity for pattern recognition and feature transformation.\n",
    "\n",
    "This component is essential for transformers to learn complex non-linear transformations while maintaining the ability to process variable-length sequences efficiently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  \"\"\"\n",
    "  Position-wise feed-forward network used in transformer blocks.\n",
    "  \n",
    "  This module implements a two-layer fully connected network with GELU activation\n",
    "  that processes each position in the sequence independently. It expands the \n",
    "  embedding dimension by a factor of 4, applies non-linear activation, then\n",
    "  projects back to the original dimension.\n",
    "  \n",
    "  The architecture follows the standard transformer design:\n",
    "  embedding_dim → 4×embedding_dim → embedding_dim\n",
    "  \n",
    "  Args:\n",
    "      config (dict): Configuration dictionary containing:\n",
    "          - embedding_dim (int): The input/output dimension size\n",
    "          - dropout_rate (float): Dropout probability for regularization\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
    "        nn.Dropout(config[\"dropout_rate\"])\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(config)\n",
    "\n",
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
    "output = ff(input)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Transformer Block: Complete Processing Unit\n",
    "\n",
    "###### Core Architecture\n",
    "\n",
    "The `Block` class implements a complete transformer layer that combines self-attention and feed-forward processing with residual connections and layer normalization. This represents one layer of a multi-layer transformer architecture.\n",
    "\n",
    "###### Component Structure\n",
    "\n",
    "**Two Main Sub-modules:**\n",
    "```python\n",
    "self.multi_head = MultiHeadAttention(config)    # Attention mechanism\n",
    "self.feed_forward = FeedForward(config)         # Position-wise processing\n",
    "```\n",
    "\n",
    "**Normalization Layers:**\n",
    "```python\n",
    "self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])  # Pre-attention norm\n",
    "self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])  # Pre-feedforward norm\n",
    "```\n",
    "\n",
    "###### Pre-Norm Architecture\n",
    "\n",
    "The implementation uses **Pre-LayerNorm** architecture (also called Pre-LN), where normalization is applied before each sub-module rather than after:\n",
    "\n",
    "**Pre-Norm Flow:**\n",
    "```python\n",
    "# Attention sub-layer\n",
    "residual = input\n",
    "x = self.multi_head(self.layer_norm_1(input))  # Norm → Attention\n",
    "x = x + residual                               # Add residual\n",
    "\n",
    "# Feed-forward sub-layer  \n",
    "residual = x\n",
    "x = self.feed_forward(self.layer_norm_2(x))    # Norm → FFN\n",
    "return x + residual                            # Add residual\n",
    "```\n",
    "\n",
    "**vs Post-Norm (original transformer):**\n",
    "```python\n",
    "# Would be: x = self.layer_norm_1(x + self.multi_head(x))\n",
    "# Would be: x = self.layer_norm_2(x + self.feed_forward(x))\n",
    "```\n",
    "\n",
    "###### Forward Pass Breakdown\n",
    "\n",
    "**Step 1: Attention Processing**\n",
    "```python\n",
    "residual = input                                    # Store original input\n",
    "x = self.multi_head(self.layer_norm_1(input))     # Normalize → Attention\n",
    "x = x + residual                                   # Add residual connection\n",
    "```\n",
    "\n",
    "- Input shape: `(batch_size, sequence_length, embedding_dim)`\n",
    "- LayerNorm normalizes across embedding dimension for each token\n",
    "- Multi-head attention processes normalized input\n",
    "- Residual connection preserves original information\n",
    "\n",
    "**Step 2: Feed-Forward Processing**\n",
    "```python\n",
    "residual = x                                       # Store attention output\n",
    "x = self.feed_forward(self.layer_norm_2(x))      # Normalize → FFN\n",
    "return x + residual                               # Add residual connection\n",
    "```\n",
    "\n",
    "- Takes attention output as input\n",
    "- Applies second normalization layer\n",
    "- Feed-forward network processes each position independently\n",
    "- Final residual connection completes the block\n",
    "\n",
    "###### Layer Normalization Details\n",
    "\n",
    "**LayerNorm Operation:**\n",
    "For each token position, normalizes across the embedding dimension:\n",
    "$$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean across embedding dimension\n",
    "- $\\sigma$ = standard deviation across embedding dimension\n",
    "- $\\gamma, \\beta$ = learnable scale and shift parameters\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# For one token with embedding_dim=768\n",
    "token_embedding = [0.1, 0.5, -0.2, ..., 0.3]  # 768 values\n",
    "# LayerNorm computes mean and std of these 768 values\n",
    "# Then normalizes: (value - mean) / std\n",
    "```\n",
    "\n",
    "###### Residual Connections\n",
    "\n",
    "**Purpose:**\n",
    "- Enables gradient flow through deep networks\n",
    "- Allows model to learn identity mappings when needed\n",
    "- Provides multiple paths for information flow\n",
    "\n",
    "**Mathematical Effect:**\n",
    "Each sub-layer computes: $\\text{output} = x + \\text{SubLayer}(x)$\n",
    "\n",
    "This means the sub-layer only needs to learn the \"change\" or \"refinement\" to apply to the input, rather than reconstructing the entire representation.\n",
    "\n",
    "###### Why Pre-Norm Architecture?\n",
    "\n",
    "**Training Stability:**\n",
    "- Better gradient flow during training\n",
    "- Reduced gradient exploding/vanishing problems\n",
    "- More stable training in deeper models\n",
    "\n",
    "**Normalization Benefits:**\n",
    "- Normalizes inputs to sub-layers rather than outputs\n",
    "- Ensures sub-layers receive well-conditioned inputs\n",
    "- Often converges faster than Post-Norm\n",
    "\n",
    "###### Information Flow Through Block\n",
    "\n",
    "**Multi-Path Processing:**\n",
    "1. **Attention path**: Models relationships between tokens\n",
    "2. **Feed-forward path**: Processes individual token representations\n",
    "3. **Residual paths**: Preserve original information at each step\n",
    "\n",
    "**Cumulative Effect:**\n",
    "Each block refines the representation while preserving previous learning through residual connections, allowing the model to build increasingly sophisticated representations layer by layer.\n",
    "\n",
    "This block design is the fundamental building unit of transformer architectures, with models typically stacking 6-96 such blocks to create powerful language models.\n",
    "\n",
    "\n",
    "##### Data Flow Diagram\n",
    "\n",
    "The data flow for the Pre-LayerNorm architecture implemented in the code. Here's the diagram:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Input\"] --> B[\"LayerNorm 1\"]\n",
    "    B --> C[\"MultiHeadAttention\"]\n",
    "    C --> D[\"Add (Residual 1)\"]\n",
    "    A --> D\n",
    "    D --> E[\"LayerNorm 2\"]\n",
    "    E --> F[\"FeedForward\"]\n",
    "    F --> G[\"Add (Residual 2)\"]\n",
    "    D --> G\n",
    "    G --> H[\"Output\"]\n",
    "    \n",
    "    style A fill:#F1F8E9\n",
    "    style B fill:#DCEDC8\n",
    "    style C fill:#C5E1A5\n",
    "    style D fill:#AED581\n",
    "    style E fill:#9CCC65\n",
    "    style F fill:#8BC34A\n",
    "    style G fill:#7CB342\n",
    "    style H fill:#CDDC39\n",
    "```\n",
    "\n",
    "**Data Flow Verification:**\n",
    "\n",
    "1. **Input** → **LayerNorm 1** → **MultiHeadAttention** → **Add (with input)** \n",
    "2. **Result** → **LayerNorm 2** → **FeedForward** → **Add (with previous result)** → **Output**\n",
    "\n",
    "This matches exactly with the code implementation:\n",
    "\n",
    "```python\n",
    "# First sub-layer\n",
    "residual = input\n",
    "x = self.multi_head(self.layer_norm_1(input))  # LayerNorm → Attention\n",
    "x = x + residual                               # Add residual\n",
    "\n",
    "# Second sub-layer  \n",
    "residual = x\n",
    "x = self.feed_forward(self.layer_norm_2(x))    # LayerNorm → FFN\n",
    "return x + residual                            # Add residual\n",
    "```\n",
    "\n",
    "The diagram shows the Pre-LayerNorm architecture where normalization occurs before each sub-module, with residual connections bypassing the normalized paths.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block, which is a fundamental building block of a Transformer model.\n",
    "\n",
    "    This module encapsulates two main sub-layers:\n",
    "    1. A Multi-Head Self-Attention mechanism.\n",
    "    2. A position-wise Feed-Forward Network.\n",
    "\n",
    "    Each sub-layer is followed by a residual connection and layer normalization,\n",
    "    a technique often referred to as \"Pre-LN\" (pre-layer normalization). This\n",
    "    structure helps stabilize training and allows for deeper models.\n",
    "\n",
    "    The data flow is as follows:\n",
    "    input -> LayerNorm -> MultiHeadAttention -> Add -> LayerNorm -> FeedForward -> Add -> output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer Block.\n",
    "\n",
    "        Args:\n",
    "            config (dict): A configuration dictionary containing parameters for\n",
    "                the sub-modules, such as \"embedding_dim\", \"heads_num\", etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The first sub-layer: Multi-Head Attention.\n",
    "        self.multi_head = MultiHeadAttention(config)\n",
    "        # Layer normalization applied *before* the attention mechanism.\n",
    "        self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "        # The second sub-layer: a simple Feed-Forward Network.\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        # Layer normalization applied *before* the feed-forward network.\n",
    "        self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): The input tensor of shape (B, T, E),\n",
    "                where B is batch size, T is sequence length, and E is embedding dim.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor with the same shape as the input.\n",
    "        \"\"\"\n",
    "        # --- First Sub-layer: Multi-Head Attention with Add & Norm ---\n",
    "        \n",
    "        # Store the original input for the first residual connection.\n",
    "        residual = input_tensor\n",
    "        \n",
    "        # Apply layer normalization, then the multi-head attention.\n",
    "        x = self.multi_head(self.layer_norm_1(input_tensor))\n",
    "        \n",
    "        # Add the residual connection. This allows the model to bypass the\n",
    "        # sub-layer if needed, aiding gradient flow.\n",
    "        x = x + residual\n",
    "\n",
    "        # --- Second Sub-layer: Feed-Forward Network with Add & Norm ---\n",
    "        \n",
    "        # Store the output of the first sub-layer for the second residual connection.\n",
    "        residual = x\n",
    "        \n",
    "        # Apply layer normalization, then the feed-forward network.\n",
    "        x = self.feed_forward(self.layer_norm_2(x))\n",
    "        \n",
    "        # Add the second residual connection.\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(config)\n",
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])\n",
    "ouptut = b(input)\n",
    "\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
