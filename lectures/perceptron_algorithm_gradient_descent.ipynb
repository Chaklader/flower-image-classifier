{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding the Perceptron Step Function\n",
    "\n",
    "This code is asking you to implement the `perceptron_step` function, which performs a single update step in the perceptron learning algorithm. Let me explain what's happening and how to correctly implement this function.\n",
    "\n",
    "##### The Perceptron Algorithm\n",
    "\n",
    "The perceptron is an algorithm for supervised learning of binary classifiers. It's a type of linear classifier that makes predictions based on a linear function of the inputs.\n",
    "\n",
    "Here's how the perceptron algorithm works:\n",
    "1. For each training example, calculate the predicted output using the current weights and bias\n",
    "2. If the prediction is correct, do nothing\n",
    "3. If the prediction is incorrect:\n",
    "   - If the actual output is 1 but predicted 0, increase the weights and bias\n",
    "   - If the actual output is 0 but predicted 1, decrease the weights and bias\n",
    "\n",
    "\n",
    "\n",
    "Here's a Mermaid diagram illustrating the perceptron algorithm workflow:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Start Training\"] --> B[\"Take Next Training Example (X, y)\"]\n",
    "    B --> C[\"Calculate Prediction: y_hat = step(W·X + b)\"]\n",
    "    C --> D{\"Is Prediction Correct? y_hat == y\"}\n",
    "    \n",
    "    D -->|\"Yes\"| E[\"No Weight Update Needed\"]\n",
    "    D -->|\"No\"| F{\"What Type of Error?\"}\n",
    "    \n",
    "    %% F -->|\"Actual: 1, Predicted: 0\"| G[\"Increase Weights and Bias: W = W + learn_rate * X b = b + learn_rate\"]\n",
    "    %% F -->|\"Actual: 0, Predicted: 1\"| H[\"Decrease Weights and Bias: W = W - learn_rate * X b = b - learn_rate\"]\n",
    "    F -->|\"Actual: 1, Predicted: 0\"| G[\"Increase Weights and Bias:<br>W = W + learn_rate * X<br>b = b + learn_rate\"]\n",
    "F -->|\"Actual: 0, Predicted: 1\"| H[\"Decrease Weights and Bias:<br>W = W - learn_rate * X<br>b = b - learn_rate\"]\n",
    "    \n",
    "    E --> I{\"More Training Examples?\"}\n",
    "    G --> I\n",
    "    H --> I\n",
    "    \n",
    "    I -->|\"Yes\"| B\n",
    "    I -->|\"No\"| J[\"End Training\"]\n",
    "    \n",
    "    style A fill:#9AE4F5\n",
    "    style B fill:#BCFB89\n",
    "    style C fill:#FBF266\n",
    "    style D fill:#FA756A\n",
    "    style E fill:#0096D9\n",
    "    style F fill:#FCEB14\n",
    "    style G fill:#FE9237\n",
    "    style H fill:#9AE4F5\n",
    "    style I fill:#FBF266\n",
    "    style J fill:#FA756A\n",
    "```\n",
    "\n",
    "This diagram shows:\n",
    "\n",
    "1. The training process starts by taking a training example\n",
    "2. The model calculates a prediction using current weights and bias\n",
    "3. The prediction is compared to the actual label\n",
    "4. If the prediction is correct, no update is needed\n",
    "5. If incorrect, the weights are updated based on the type of error:\n",
    "   - For false negatives (actual=1, predicted=0): increase weights and bias\n",
    "   - For false positives (actual=0, predicted=1): decrease weights and bias\n",
    "6. The process repeats for all training examples\n",
    "7. Multiple passes (epochs) may be needed for the model to converge\n",
    "\n",
    "The colored nodes represent different stages of the algorithm, helping to visualize the distinct steps in the perceptron learning process.\n",
    "\n",
    "##### Understanding the Code Components\n",
    "\n",
    "Let's analyze the provided code:\n",
    "\n",
    "1. `step_function(t)`: This is the activation function that outputs 1 if the input is non-negative, and 0 otherwise.\n",
    "\n",
    "2. `prediction(X, W, b)`: This function:\n",
    "   - Takes a data point X, weights W, and bias b\n",
    "   - Calculates the dot product X·W + b\n",
    "   - Passes the result through the step function\n",
    "   - Returns the predicted class (0 or 1)\n",
    "\n",
    "3. `perceptron_step(X, y, W, b, learn_rate = 0.01)`: This function should:\n",
    "   - Process the entire dataset X with labels y\n",
    "   - Update weights W and bias b according to the perceptron algorithm\n",
    "   - Return the updated weights and bias\n",
    "\n",
    "##### The Test Case\n",
    "\n",
    "Looking at the test case:\n",
    "- X_test contains 4 data points: [1,1], [1,-1], [-1,1], and [-1,-1]\n",
    "- y_test contains the corresponding labels: 1, 1, 0, 0\n",
    "- W_test is the initial weights: [0.5, 0.5]\n",
    "- b_test is the initial bias: 0.5\n",
    "- learn_rate is set to 0.01\n",
    "\n",
    "The expected outcome is:\n",
    "- W becomes [0.51, 0.49]\n",
    "- b becomes 0.49\n",
    "\n",
    "##### Implementing the Perceptron Step\n",
    "\n",
    "To correctly implement the `perceptron_step` function, we need to:\n",
    "\n",
    "1. For each data point, calculate the prediction\n",
    "2. Compare the prediction with the actual label\n",
    "3. Update the weights and bias accordingly\n",
    "\n",
    "Here's how we should implement it:\n",
    "\n",
    "```python\n",
    "def perceptron_step(X, y, W, b, learn_rate = 0.01):\n",
    "    \"\"\"\n",
    "    The function should receive as inputs the data X, the labels y, the\n",
    "    weights W (as an array), and the bias b, update the weights and bias\n",
    "    W, b, according to the perceptron algorithm, and return W and b.\n",
    "    \"\"\"\n",
    "    for i in range(len(X)):\n",
    "        # Calculate the prediction\n",
    "        y_hat = prediction(X[i:i+1], W, b)\n",
    "        \n",
    "        # Update weights and bias when prediction is wrong\n",
    "        if y[i] - y_hat == 1:  # Actual=1, Predicted=0\n",
    "            # Increase weights and bias\n",
    "            W[0][0] += X[i][0] * learn_rate\n",
    "            W[1][0] += X[i][1] * learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i] - y_hat == -1:  # Actual=0, Predicted=1\n",
    "            # Decrease weights and bias\n",
    "            W[0][0] -= X[i][0] * learn_rate\n",
    "            W[1][0] -= X[i][1] * learn_rate\n",
    "            b -= learn_rate\n",
    "            \n",
    "    return W, b\n",
    "```\n",
    "\n",
    "##### Tracing Through the Example\n",
    "\n",
    "Let's trace through the execution with the test data to verify our implementation:\n",
    "\n",
    "Starting with:\n",
    "- X = [[1,1], [1,-1], [-1,1], [-1,-1]]\n",
    "- y = [1, 1, 0, 0]\n",
    "- W = [[0.5], [0.5]]\n",
    "- b = 0.5\n",
    "- learn_rate = 0.01\n",
    "\n",
    "For the first data point [1,1]:\n",
    "- Prediction: step_function((1×0.5 + 1×0.5 + 0.5)) = step_function(1.5) = 1\n",
    "- Actual label: 1\n",
    "- Prediction is correct, so no update\n",
    "\n",
    "For the second data point [1,-1]:\n",
    "- Prediction: step_function((1×0.5 + (-1)×0.5 + 0.5)) = step_function(0.5) = 1\n",
    "- Actual label: 1\n",
    "- Prediction is correct, so no update\n",
    "\n",
    "For the third data point [-1,1]:\n",
    "- Prediction: step_function(((-1)×0.5 + 1×0.5 + 0.5)) = step_function(0.5) = 1\n",
    "- Actual label: 0\n",
    "- Prediction is incorrect (predicted 1 but actual is 0)\n",
    "- Update: \n",
    "  - W[0][0] = 0.5 - (-1)×0.01 = 0.5 + 0.01 = 0.51\n",
    "  - W[1][0] = 0.5 - 1×0.01 = 0.5 - 0.01 = 0.49\n",
    "  - b = 0.5 - 0.01 = 0.49\n",
    "\n",
    "For the fourth data point [-1,-1]:\n",
    "- Prediction: step_function(((-1)×0.51 + (-1)×0.49 + 0.49)) = step_function(-0.51) = 0\n",
    "- Actual label: 0\n",
    "- Prediction is correct, so no update\n",
    "\n",
    "Final values:\n",
    "- W = [[0.51], [0.49]]\n",
    "- b = 0.49\n",
    "\n",
    "This matches the expected output, confirming our implementation is correct.\n",
    "\n",
    "##### Key Insights\n",
    "\n",
    "1. The perceptron algorithm adjusts the weights and bias in the direction that reduces the error for each misclassified point.\n",
    "\n",
    "2. The learning rate controls how much the weights and bias change with each update. A smaller learning rate means smaller steps and potentially more stable convergence, but might require more iterations.\n",
    "\n",
    "3. In geometrical terms, the weights and bias define a decision boundary (a line in 2D). The perceptron algorithm moves this boundary until it correctly separates the two classes (if they are linearly separable).\n",
    "\n",
    "4. This implementation processes all examples and then returns the updated weights. In a full training loop, you would typically call this function repeatedly until convergence or for a fixed number of epochs.\n",
    "\n",
    "This perceptron implementation forms the foundation for more complex neural networks and deep learning models. While simple, it contains the essential elements of forward propagation (prediction) and weight updating based on error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice work coding the perceptron algorithm!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def step_function(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return step_function((np.matmul(X,W)+b)[0])\n",
    "\n",
    "def perceptron_step(X, y, W, b, learn_rate = 0.01):\n",
    "    \"\"\"\n",
    "    The function should receive as inputs the data X, the labels y, the\n",
    "    weights W (as an array), and the bias b, update the weights and bias\n",
    "    W, b, according to the perceptron algorithm, and return W and b.\n",
    "    \"\"\"\n",
    "    for i in range(len(X)):\n",
    "        # Calculate the prediction\n",
    "        y_hat = prediction(X[i:i+1], W, b)\n",
    "        \n",
    "        # Update weights and bias when prediction is wrong\n",
    "        if y[i] - y_hat == 1:  # Actual=1, Predicted=0\n",
    "            # Increase weights and bias\n",
    "            W[0][0] += X[i][0] * learn_rate\n",
    "            W[1][0] += X[i][1] * learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i] - y_hat == -1:  # Actual=0, Predicted=1\n",
    "            # Decrease weights and bias\n",
    "            W[0][0] -= X[i][0] * learn_rate\n",
    "            W[1][0] -= X[i][1] * learn_rate\n",
    "            b -= learn_rate\n",
    "            \n",
    "    return W, b\n",
    "    \n",
    "X_test = np.array([[1,1],[1,-1],[-1,1],[-1,-1]])\n",
    "y_test = np.array([1,1,0,0])\n",
    "W_test = np.array([[0.5], [0.5]])\n",
    "b_test = 0.5\n",
    "\n",
    "output_W, output_b = perceptron_step(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    W_test,\n",
    "    b_test,\n",
    "    0.01\n",
    ")\n",
    "solution_W = np.array([[ 0.51],[ 0.49]])\n",
    "solution_b = 0.49\n",
    "if np.array_equal(output_W, solution_W) and output_b == solution_b:\n",
    "    print(\"Nice work coding the perceptron algorithm!\")\n",
    "else:\n",
    "    print(\"Try again. For perceptron_step(\\n{}, \\n{}, \\n{}, \\n{}, \\\n",
    "\\n{}\\n)\\nthe expected result was \\nW=\\n{}\\nand b={}, but your output \\\n",
    "was \\nW=\\n{}\\nand b={}\".format(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        W_test,\n",
    "        b_test,\n",
    "        0.01,\n",
    "        solution_W,\n",
    "        solution_b,\n",
    "        output_W,\n",
    "        output_b\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done implementing the algorithm and the cell above prints \"Nice work coding the perceptron algorithm!\" go ahead and run the cell below, which will repeatedly run your `perceptron_step` function and plot the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron_algorithm(X, y, learn_rate=0.01, num_epochs=25):\n",
    "    \"\"\"\n",
    "    This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "    and returns a few of the boundary lines obtained in the iterations,\n",
    "    for plotting purposes.\n",
    "    \"\"\"\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptron_step(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n",
    "\n",
    "def plot_line(m, b, linestyle='dashed', color='gray', fill=False):\n",
    "    \"\"\"\n",
    "    Helper function to avoid repetitive code when plotting boundary lines\n",
    "    \"\"\"\n",
    "    x = np.arange(-10.0, 10.0, 0.1)\n",
    "    plt.plot(x, m*x+b, linestyle=linestyle, color=color)\n",
    "    if fill:\n",
    "        plt.fill_between(x, m*x+b, -0.05, color=blue, alpha=0.3)\n",
    "        plt.fill_between(x, m*x+b, 1.05, color=red, alpha=0.3)\n",
    "        \n",
    "# Load data\n",
    "data = np.asarray(pd.read_csv(\"data.csv\", header=None))\n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# Get list of boundary lines\n",
    "# Feel free to play with the learning rate and the num_epochs, and see\n",
    "# your results plotted below\n",
    "boundary_lines = train_perceptron_algorithm(X, y)\n",
    "\n",
    "# Set up plot styling\n",
    "plt.xlim(-0.05,1.05)\n",
    "plt.ylim(-0.05,1.05)\n",
    "plt.grid(False)\n",
    "plt.tick_params(axis='x', which='both', bottom='off', top='off')\n",
    "\n",
    "# Plot data points\n",
    "red = [1,0.3,0.3]\n",
    "blue = [0.25,0.5,1]\n",
    "red_points = X[np.argwhere(y==0).flatten()]\n",
    "blue_points = X[np.argwhere(y==1).flatten()]\n",
    "plt.scatter(red_points[:,0], red_points[:,1], s=50, color=red, edgecolor='k')\n",
    "plt.scatter(blue_points[:,0], blue_points[:,1], s=50, color=blue, edgecolor='k')\n",
    "\n",
    "# Plot boundary lines and solution regions\n",
    "for line in boundary_lines:\n",
    "    slope = line[0]\n",
    "    b = line[1]\n",
    "    plot_line(slope, b)\n",
    "solution_slope = boundary_lines[-1][0]\n",
    "solution_intercept = boundary_lines[-1][1]\n",
    "plot_line(solution_slope, solution_intercept, 'solid', 'k', True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aipnd)",
   "language": "python",
   "name": "aipnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
