{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this code, I need to implement the forward pass through the neural network, specifically calculating the input and output of the hidden layer. Let me fill in the missing code.\n",
    "\n",
    "The forward pass consists of two main steps:\n",
    "1. Calculate the input to the hidden layer by taking the dot product of the input X with the weights connecting the input to the hidden layer\n",
    "2. Apply the sigmoid activation function to get the output of the hidden layer\n",
    "\n",
    "Let me explain what's happening in the forward pass:\n",
    "\n",
    "1. `hidden_layer_in = np.dot(X, weights_input_to_hidden)` \n",
    "   - This calculates the weighted sum of inputs for each hidden neuron\n",
    "   - X has shape (4,) and weights_input_to_hidden has shape (4,3)\n",
    "   - The result has shape (3,), representing the input value for each of the 3 hidden neurons\n",
    "\n",
    "2. `hidden_layer_out = sigmoid(hidden_layer_in)`\n",
    "   - This applies the sigmoid activation function to each value in hidden_layer_in\n",
    "   - The sigmoid function transforms values to be between 0 and 1\n",
    "   - This creates the output of the hidden layer, which would normally be passed to the output layer in a complete forward pass\n",
    "\n",
    "This forward pass is the fundamental building block of neural networks, where information flows from the input layer through the hidden layer(s) to the output layer, with weights controlling the strength of connections between neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "grader_id": "lz4ffelzky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[0.41492192 0.42604313 0.5002434 ]\n",
      "Good job! You got the correct calculation on the hidden layer.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "# TODO: Make a forward pass through the network (hidden layer)\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)  # Calculate input to hidden layer\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)  # Apply activation function to get hidden layer output\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "### Notebook grading\n",
    "hidden_layer_in_ans = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out_ans = sigmoid(hidden_layer_in_ans)\n",
    "\n",
    "test_hidden_layer_in = hidden_layer_in_ans == hidden_layer_in\n",
    "test_hidden_layer_out = hidden_layer_out_ans == hidden_layer_out\n",
    "\n",
    "if test_hidden_layer_out.all():\n",
    "    print(\"Good job! You got the correct calculation on the hidden layer.\")\n",
    "else:\n",
    "    print('Try again. hidden_layer_out should be {}'.format(hidden_layer_out_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the forward pass through the neural network's output layer, I need to implement the same pattern we used for the hidden layer. Let me correct the code:\n",
    "\n",
    "\n",
    "Here's what's happening in this part of the forward pass:\n",
    "\n",
    "1. `output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)`\n",
    "   - This calculates the weighted sum of inputs from the hidden layer to each output neuron\n",
    "   - We take the dot product of the hidden layer outputs (which have shape (3,)) and the weights connecting the hidden layer to the output layer (which have shape (3,2))\n",
    "   - The result has shape (2,), representing the input value for each of the 2 output neurons\n",
    "\n",
    "2. `output_layer_out = sigmoid(output_layer_in)`\n",
    "   - This applies the sigmoid activation function to each value in output_layer_in\n",
    "   - The sigmoid function transforms the values to be between 0 and 1, which can be interpreted as probabilities for binary classification problems\n",
    "   - These final values are the network's predictions\n",
    "\n",
    "This completes the full forward pass through the neural network, where we've propagated our input data X through the hidden layer and then through the output layer to get the final output. In a training scenario, we would compare these outputs with the true targets to calculate the error, then use backpropagation to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "grader_id": "3a1z1kz74eh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output-layer Output:\n",
      "[0.49815196 0.48539772]\n",
      "Good job! You got the correct calculation on the output layer.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Make a forward pass through the network (output layer)\n",
    "\n",
    "# Calculate input to output layer (dot product of hidden layer output with weights)\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "\n",
    "# Apply sigmoid activation to get output layer output\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)\n",
    "\n",
    "### Notebook grading\n",
    "output_layer_in_ans = np.dot(hidden_layer_out_ans, weights_hidden_to_output)\n",
    "output_layer_out_ans = sigmoid(output_layer_in_ans)\n",
    "\n",
    "test_output_layer_out = output_layer_out_ans == output_layer_out\n",
    "\n",
    "if test_output_layer_out.all():\n",
    "    print(\"Good job! You got the correct calculation on the output layer.\")\n",
    "else:\n",
    "    print('Try again. output_layer_out should be {}'.format(output_layer_out_ans))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aipnd)",
   "language": "python",
   "name": "aipnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
