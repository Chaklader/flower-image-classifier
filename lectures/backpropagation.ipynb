{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy is a loss (cost) function that measures how well a set of predicted probabilities P matches the true labels Y.\n",
    "\n",
    "For binary classification (labels 0 or 1) the formula is\n",
    "\n",
    "L(Y,P) = - Σ [ Y · log P + (1-Y) · log (1-P) ]\n",
    "\n",
    "where\n",
    "· Y is the true label (1 = positive, 0 = negative)\n",
    "· P is the model's predicted probability that the label is 1\n",
    "· The sum Σ runs over all samples.\n",
    "\n",
    "Key points\n",
    "· If the prediction is perfect (P ≈ 1 when Y = 1, or P ≈ 0 when Y = 0) the loss is near 0.\n",
    "· Confident wrong predictions (e.g. P ≈ 1 when Y = 0) incur a very large loss because log(0) → -∞.\n",
    "· Cross-entropy is convex for logistic regression, giving smooth gradients for optimisation.\n",
    "\n",
    "The code you posted implements this exactly:\n",
    "\n",
    "```python\n",
    "return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n",
    "```\n",
    "\n",
    "It converts Y and P to floats, takes element-wise logs, multiplies by the appropriate label term, sums over all samples, and returns the negative of that sum (so lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y  *np.log(P) + (1 - Y)*  np.log(1 - P))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph \"Input Layer\"\n",
    "        I0[\"Input 0 | (0.5)\"]\n",
    "        I1[\"Input 1 | (0.1)\"]\n",
    "        I2[\"Input 2 | (-0.2)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Hidden Layer\"\n",
    "        H0[\"Hidden 0 | (σ = 0.632)\"]\n",
    "        H1[\"Hidden 1 | (σ = 0.456)\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Output Layer\"\n",
    "        O0[\"Output | (σ = 0.540) | Target = 0.6\"]\n",
    "    end\n",
    "    \n",
    "    I0 -->|0.5| H0\n",
    "    I0 -->|-0.6| H1\n",
    "    I1 -->|0.1| H0\n",
    "    I1 -->|-0.2| H1\n",
    "    I2 -->|0.1| H0\n",
    "    I2 -->|0.7| H1\n",
    "    \n",
    "    H0 -->|0.1| O0\n",
    "    H1 -->|-0.3| O0\n",
    "    \n",
    "    \n",
    "    style I0 fill:#9AE4F5\n",
    "    style I1 fill:#9AE4F5\n",
    "    style I2 fill:#9AE4F5\n",
    "    style H0 fill:#BCFB89\n",
    "    style H1 fill:#BCFB89\n",
    "    style O0 fill:#FA756A\n",
    "```\n",
    "\n",
    "This diagram shows:\n",
    "\n",
    "1. The input layer with 3 nodes (blue)\n",
    "2. The hidden layer with 2 nodes (green)\n",
    "3. The output layer with 1 node (coral)\n",
    "\n",
    "The connections between the input and hidden layers show the weights from your weight matrix:\n",
    "- Input 0 connects to Hidden 0 with weight 0.5\n",
    "- Input 0 connects to Hidden 1 with weight -0.6\n",
    "- Input 1 connects to Hidden 0 with weight 0.1\n",
    "- Input 1 connects to Hidden 1 with weight -0.2\n",
    "- Input 2 connects to Hidden 0 with weight 0.1\n",
    "- Input 2 connects to Hidden 1 with weight 0.7\n",
    "\n",
    "The connections between the hidden layer and output layer are shown as w1 and w2, as these weren't specified in your example.\n",
    "\n",
    "This network structure performs the matrix multiplication you described: when a 3-element input vector is multiplied by the 3×2 weight matrix, it produces a 2-element vector that serves as input to the hidden layer neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "grader_id": "tpan1eblwvo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n",
      "Well done!\n",
      "Well done!\n",
      "Well done!\n",
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n",
      "Well done!\n",
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# x is a single feature vector for the training that feeds into the network’s input layer. \n",
    "# It has three features—[0.5, 0.1, -0.2]—matching the three input nodes used in the notebook’s \n",
    "# weight matrices.\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "\n",
    "# target is the desired (ground-truth) value for that training example.\n",
    "target = 0.6\n",
    "\n",
    "# learnrate is the learning rate, a hyperparameter that controls how much we adjust the weights in each iteration.\n",
    "learnrate = 0.5\n",
    "\n",
    "# weights_input_hidden is the weight matrix connecting the input layer to the hidden layer.\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "\n",
    "# Calculate output error\n",
    "error = target - output      \n",
    "print(\"Output Error:\", error)\n",
    "\n",
    "# Calculate error term for output layer\n",
    "\n",
    "# output * (1 - output) is the derivative of the sigmoid activation\n",
    "# σ'(z) = σ(z)(1 - σ(z)).\n",
    "# In back-propagation you multiply that derivative by the error coming from\n",
    "# the loss:\n",
    "\n",
    "# error = (y_true - output) # ∂L/∂output\n",
    "# output_error_term = error * output * (1 - output) # ∂L/∂z\n",
    "\n",
    "# So output_error_term is not the derivative of the sigmoid by itself; it is the\n",
    "# gradient of the loss with respect to the neuron's pre-activation input z.\n",
    "# In other words, it's the product of:\n",
    "\n",
    "# 1. the derivative of the loss w.r.t. the neuron's output (error), and\n",
    "# 2. the derivative of the sigmoid w.r.t. its input (output * (1-output)).\n",
    "# Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output) # ∂L/∂z\n",
    "print(\"Output Error Term:\", output_error_term)\n",
    "\n",
    "# Calculate error term for hidden layer\n",
    "# hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
    "#                     hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "\n",
    "# Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output) # ∂L/∂z\n",
    "print(\"Hidden Error Term:\", hidden_error_term)\n",
    "\n",
    "\n",
    "# Calculate change in weights for hidden layer to output layer\n",
    "# delta_w_h_o = learnrate  *output_error_term*  hidden_layer_output\n",
    "# Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output # <-- LINE 4\n",
    "print(\"Delta W H O:\", delta_w_h_o)\n",
    "\n",
    "\n",
    "# Calculate change in weights for input layer to hidden layer\n",
    "# delta_w_i_h = learnrate  *hidden_error_term*  x[:, None]\n",
    "\n",
    "# Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None] # <-- LINE 5\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
