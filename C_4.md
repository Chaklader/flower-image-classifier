# C-4: Transformer Architecture

1. Transformers and Attention Mechanisms
    - Attention Mechanism Fundamentals
    - Self-Attention Implementation
    - Multi-Head Attention
    - Scaled Dot-Product Attention
    - Positional Encoding
    - Transformer Block Architecture
2. Tokenization and Embeddings in NLP
    - Tokenization Methods
    - Word Embeddings Techniques
    - Contextual vs. Static Embeddings
    - Embedding Vector Properties
    - Subword Tokenization Approaches

#### Transformers and Attention Mechanisms

The transformer architecture represents one of the most significant breakthroughs in artificial intelligence in recent
years. To truly appreciate its elegance and power, we need to understand how it revolutionized the way neural networks
process sequential data like text, audio, or time series. Let's explore the core components that make transformers work,
starting with the fundamental concept of attention.

<div align="center">
<img src="images/transformer.png" width="600" height="auto">
<p style="color: #555;">Figure: Transformer Architecture</p>
</div>

##### Attention Mechanism Fundamentals

Imagine you're reading a complex sentence: "After reviewing all the documents, the lawyer who specialized in corporate
law determined that the contract needed significant revisions." As you process this sentence, your mind doesn't give
equal importance to every word when understanding each part. When you focus on what the lawyer determined, you pay
special attention to "lawyer" and "determined," creating a connection between these distant but related words.

This natural cognitive process—selectively focusing on relevant information—is precisely what attention mechanisms
simulate in neural networks.

<div align="center">
<img src="images/b.png" width="600" height="auto">
<p style="color: #555;">Figure: Transformer Architecture</p>
</div>

At its core, an attention mechanism allows a model to focus on specific parts of the input when producing each element
of the output. Let's break down this process step by step:

1. **Query, Key, and Value Concept**

    Think of attention as a sophisticated information retrieval system:

    - **Query (Q)**: Represents what we're looking for or asking about
    - **Key (K)**: Represents the available information and how it relates to our query
    - **Value (V)**: Contains the actual content we want to retrieve

    <div align="center">
    <img src="images/queries_3.png" width="600" height="auto">
    <p style="color: #555;">Figure: Query, Key, and Value Matrices in Attention Mechanism</p>
    </div>

    An intuitive analogy is searching in a library. Your query is what you're interested in finding, each book title
    serves as a key that might match your query, and the content inside each book is the value you ultimately want to
    access.

2. **Attention Scoring**

    For each query, we calculate its compatibility or relevance to every key:

    $\text{score}(q_i, k_j) = \text{similarity}(q_i, k_j)$

    <div align="center">
    <img src="images/queries_4.png" width="600" height="auto">
    <p style="color: #555;">Figure: Query, Key, and Value Matrices in Attention Mechanism</p>
    </div>

    This score measures how much attention should be paid to the jth value when creating the output for position i.
    Various similarity functions can be used, including dot products, scaled dot products, or additive attention.

3. **Weight Normalization**

    The raw scores are converted into a probability distribution using the softmax function:

    $\alpha_{ij} = \frac{\exp(\text{score}(q_i, k_j))}{\sum_k \exp(\text{score}(q_i, k_k))}$

    This ensures all weights sum to 1, creating a "soft selection" mechanism. Rather than making a hard choice to select
    specific values, attention takes a weighted average, allowing the model to blend information from multiple sources.

4. **Value Aggregation**

    Finally, we compute a weighted sum of the values:

    $\text{output}*i = \sum_j \alpha*{ij} v_j$

    The output for position i is a weighted combination of all values, with weights determined by the attention scores.

<div align="center">
<img src="images/queries.png" width="600" height="auto">
<p style="color: #555;">Figure: Query, Key, and Value Matrices in Attention Mechanism</p>
</div>

Let's work through a simplified example. Imagine processing the sentence "The animal didn't cross the street because it
was too tired." What does "it" refer to—the animal or the street? An attention mechanism would calculate high scores
between "it" and "animal" based on their semantic relationship, helping resolve this ambiguity.

The revolutionary aspect of attention mechanisms is their ability to create direct connections between any positions in
a sequence, regardless of their distance. This solved a fundamental limitation of previous sequence models like RNNs,
which struggled with long-range dependencies due to their sequential nature.

<div align="center">
<img src="images/queries_2.png" width="600" height="auto">
<p style="color: #555;">Figure: Query, Key, and Value Matrices in Attention Mechanism</p>
</div>

##### Self-Attention Implementation

Self-attention is a specific application of the attention mechanism where the queries, keys, and values all come from
the same source. This allows each element in a sequence to attend to all other elements, essentially enabling the
sequence to look at itself (hence "self"-attention) to compute a better representation.

<div align="center">
<img src="images/queries_5.png" width="600" height="auto">
<p style="color: #555;">Figure: Query, Key, and Value Matrices in Attention Mechanism</p>
</div>

Let's walk through the concrete implementation steps for self-attention:

1. **Projection Matrices**

    First, we need to transform the input embeddings into the query, key, and value spaces. If our input is a sequence
    of vectors with dimension $d_{model}$, we create three matrices:

    - $W^Q$ of shape $d_{model} \times d_k$ for queries
    - $W^K$ of shape $d_{model} \times d_k$ for keys
    - $W^V$ of shape $d_{model} \times d_v$ for values

    For each position $i$ in our sequence with input embedding $x_i$, we compute:

    $q_i = x_i W^Q$ $k_i = x_i W^K$ $v_i = x_i W^V$

    These linear transformations allow the model to project the same input into different representation spaces
    optimized for their specific roles in the attention mechanism.

2. **Computing Attention Scores**

    Once we have queries and keys, we compute the attention scores by taking the dot product of each query with all
    keys:

    $s_{ij} = q_i \cdot k_j^T$

    This results in a matrix where entry $(i,j)$ represents how much position $i$ should attend to position $j$.

    In practice, this is calculated efficiently as a matrix multiplication:

    $S = QK^T$

    Where $Q$ and $K$ are matrices containing all queries and keys for the sequence.

3. **Scaling the Scores**

    The dot product can grow large for long vectors, pushing the softmax function into regions with very small
    gradients. To counteract this, we scale the scores by the square root of the dimension of the key vectors:

    $S = \frac{QK^T}{\sqrt{d_k}}$

    This scaling factor keeps the values in a range where the softmax function remains sensitive to input differences.

4. **Applying Softmax**

    Next, we apply the softmax function row-wise to obtain the attention weights:

    $A = \text{softmax}(S)$

    Each row of $A$ becomes a probability distribution over all positions in the sequence, representing how much each
    position contributes to the new representation of position $i$.

5. **Computing Weighted Values**

    Finally, we compute the output as a weighted sum of the values:

    $O = AV$

    Each row in $O$ represents the new, context-aware representation for the corresponding position in the input
    sequence.

To visualize this process, consider a simple sentence: "The dog chased the cat." When processing the word "chased,"
self-attention might assign high weights to "dog" (the subject doing the chasing) and "cat" (the object being chased),
capturing the semantic relationships between these words regardless of their positions in the sentence.

The power of self-attention becomes even more apparent with ambiguous references. In the sentence "The trophy wouldn't
fit in the suitcase because it was too big," self-attention helps determine that "it" likely refers to "trophy" through
the attention weights, solving a complex coreference resolution problem.

Self-attention can be visualized as a directed graph where each node (word) is connected to every other node, with edge
weights representing attention scores. This creates a fully connected network of relationships, allowing information to
flow directly between any positions without passing through intermediate steps.

<div align="center">
<img src="images/mask.png" width="600" height="auto">
<p style="color: #555;">Figure: Masked Multi-Head Attention</p>
</div>

##### Multi-Head Attention

While self-attention is powerful, it has a limitation: a single attention mechanism can only capture one type of
relationship at a time. Consider the sentence "She met the old man at the bank." The word "bank" could have different
types of relationships with other words—a semantic relationship with "she" and "met" indicating a financial institution,
or a semantic relationship with "old man" possibly suggesting a riverbank.

Multi-head attention addresses this limitation by running multiple self-attention mechanisms in parallel, allowing the
model to jointly attend to information from different representation subspaces.

<div align="center">
<img src="images/mask_1.png" width="600" height="auto">
<p style="color: #555;">Figure: Masked Multi-Head Attention</p>
</div>

Let's understand how multi-head attention works:

1. **Creating Multiple Projection Sets**

    Instead of having just one set of projection matrices ($W^Q$, $W^K$, $W^V$), we create $h$ different sets, where $h$
    is the number of heads:

    $W_i^Q, W_i^K, W_i^V$ for $i = 1, 2, ..., h$

    Each head has its own projection matrices, usually with reduced dimensions. If the model dimension is $d_{model}$
    and we have $h$ heads, each head typically projects to dimension $d_k = d_v = d_{model}/h$.

2. **Parallel Attention Computation**

    Each head computes attention independently:

    $\text{head}_i = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)$

    Where $X$ is the input sequence, and $\text{Attention}$ refers to the scaled dot-product attention described
    earlier.

    Each head can learn to focus on different aspects of the data:

    - One head might focus on syntactic relationships
    - Another might capture subject-verb relationships
    - A third might identify entity relationships
    - Others might learn positional or local phrase patterns

3. **Concatenation and Final Projection**

    After computing the output from each attention head, we concatenate them:

    $\text{MultiHead} = \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_h)$

    This concatenation results in a matrix of shape $(seq_length, d_{model})$, maintaining the original dimension size.

    Finally, we apply one more linear transformation:

    $\text{MultiHead} = \text{MultiHead} \cdot W^O$

    Where $W^O$ is a parameter matrix of shape $(d_{model}, d_{model})$ that mixes information from different heads.

<div align="center">
<img src="images/a.png" width="600" height="auto">
<p style="color: #555;">Figure: Query, Key, and Value Matrices in Attention Mechanism</p>
</div>

To visualize how multiple heads work together, let's use our previous example: "She met the old man at the bank."

- **Head 1** might focus on syntactic structure, connecting "she" to the verb "met"
- **Head 2** might capture entity relationships, linking "old man" with its definite article "the"
- **Head 3** might attend to financial context words, connecting "bank" with "met" (suggesting a financial institution)
- **Head 4** might focus on spatial relationships, connecting "at" with "bank"

The concatenated output from these heads provides a rich, multi-faceted representation that captures different aspects
of meaning simultaneously.

A typical transformer implementation uses 8-16 attention heads. Empirical studies have shown that different heads indeed
learn to specialize in different types of relationships, although there is usually some redundancy between heads. Some
heads focus on local relationships, others on long-distance dependencies, and some on specific syntactic or semantic
patterns.

##### Scaled Dot-Product Attention

Now that we understand self-attention and multi-head attention at a high level, let's examine the specific attention
mechanism used in transformer models: scaled dot-product attention.

The formula for scaled dot-product attention is:

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

This seemingly simple equation packs several important design choices that make it particularly effective for
transformers. Let's break it down step by step:

1. **Dot Product for Similarity**

    The dot product between query and key vectors measures their similarity:

    $QK^T$ results in a matrix where entry $(i,j)$ is the dot product $q_i \cdot k_j$.

    The dot product has an intuitive interpretation: vectors pointing in similar directions have high dot products,
    while orthogonal vectors have zero dot products. This naturally captures the notion of relevance—how much one
    position should attend to another.

    Compared to alternatives like additive attention (using a small neural network to compute compatibility), the dot
    product is:

    - More computationally efficient
    - More space-efficient (requires fewer parameters)
    - Well-optimized on modern hardware through matrix multiplication

2. **Scaling Factor**

    The scaling factor $\frac{1}{\sqrt{d_k}}$ is a critical component often overlooked in simplified explanations.

    Why is scaling necessary? As the dimension $d_k$ grows, the variance of the dot product grows as well. For random
    vectors with components of variance 1, their dot product would have variance $d_k$. These large values push the
    softmax function into regions with extremely small gradients, causing the gradient to vanish during backpropagation.

    By dividing by $\sqrt{d_k}$, we keep the variance of the dot product at 1 regardless of the dimension, ensuring
    stable gradients during training.

    To illustrate, imagine two vectors of dimension 64 with typical values around 1. Their dot product could easily
    reach magnitude 8 (square root of 64), which when passed through softmax would give nearly all weight to the highest
    value, effectively making attention almost a hard selection rather than a soft weighting.

3. **Softmax Normalization**

    The softmax function converts the scaled dot products into a probability distribution:

    $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$

    This serves two important purposes:

    - Ensures all attention weights are positive and sum to 1
    - Creates a differentiable "soft" selection mechanism

    The softmax operation is applied row-wise, meaning each position in the sequence gets its own probability
    distribution over all positions it can attend to.

4. **Value Weighting**

    The final step multiplies the attention weight matrix by the value matrix:

    $\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

    This creates a weighted combination of value vectors for each position.

    If position $i$ attends strongly to positions $j$ and $k$, then the output for position $i$ will be dominated by the
    value vectors $v_j$ and $v_k$.

In practice, scaled dot-product attention is implemented using highly optimized matrix operations:

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    # Calculate attention scores
    d_k = K.shape[-1]
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

    # Apply mask (if provided)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # Apply softmax
    attention_weights = torch.softmax(scores, dim=-1)

    # Multiply by values
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
```

The mask parameter is particularly important for transformer models. In decoder self-attention, we need to ensure that
predictions at position $i$ can only depend on outputs at positions less than $i$. This causal masking is implemented by
setting the upper triangular part of the attention matrix to negative infinity before softmax, effectively forcing those
attention weights to zero.

Scaled dot-product attention provides an elegant balance of computational efficiency and expressive power, making it the
attention mechanism of choice for transformer architectures.

##### Positional Encoding

A key limitation of the attention mechanism is that it's permutation invariant—it doesn't inherently consider the order
of elements in the sequence. If we shuffled the words in a sentence, the self-attention operation would produce the same
results, which is problematic since word order is crucial for understanding language.

Transformers solve this problem through positional encoding—adding position-specific information to the input embeddings
before they're processed by the attention layers.

Let's explore how positional encoding works:

1. **Fixed Sinusoidal Encodings**

    The original transformer paper introduced a clever approach using sine and cosine functions of different
    frequencies:

    $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$

    $PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$

    Where:

    - $pos$ is the position in the sequence (0, 1, 2, ...)
    - $i$ is the dimension index (0, 1, 2, ..., $d_{model}/2 - 1$)
    - $d_{model}$ is the embedding dimension

    This creates a unique encoding vector for each position, with specific patterns that help the model learn about
    relative positions.

    Why use sinusoidal functions? They have several desirable properties:

    - Each position gets a unique encoding
    - The encodings for nearby positions are similar, while distant positions are more different
    - The pattern extends to positions beyond those seen during training
    - It allows the model to easily learn relative position relationships

2. **Adding Positional Information**

    The positional encodings are simply added to the input embeddings:

    $\text{input} = \text{embedding} + \text{positional\_encoding}$

    This maintains the original semantic information while enriching it with positional context.

    The dimensions of the positional encoding vector match the embedding dimensions, ensuring they can be added
    together.

To understand how sinusoidal encodings capture position information, let's examine their patterns:

- For small values of $i$ (early dimensions), the sinusoids have a high frequency, changing rapidly with position
- For large values of $i$ (later dimensions), the sinusoids have a low frequency, changing slowly with position

This creates a multi-scale representation of position, where different dimensions capture position information at
different resolutions.

Remarkably, this encoding scheme allows the model to generalize to sequence lengths longer than those seen during
training. The sinusoidal pattern continues predictably for any position value, enabling the model to handle longer texts
at inference time.

Here's a simplified visualization of positional encodings for the first few positions and dimensions:

For position 0: $[sin(0), cos(0), sin(0), cos(0), ...] = [0, 1, 0, 1, ...]\\$

For position 1: $[sin(1/10000^0), cos(1/10000^0), sin(1/10000^0.02), ...] ≈ [0.84, 0.54, 0.84, ...]\\$

For position 2: $[sin(2/10000^0), cos(2/10000^0),sin(2/10000^0.02), ...] ≈ [0.91, -0.42, 0.91, ...]\\$

Each position gets a unique fingerprint, with patterns that reflect the relative distances between positions.

While fixed sinusoidal encodings are elegant, many modern transformer implementations use learned positional embeddings
instead, which are simple position-specific vectors that the model learns during training. These learned embeddings
often perform slightly better but don't generalize to unseen sequence lengths as well as the sinusoidal encodings.

More recent transformer models have explored alternatives like rotary position embeddings (RoPE), relative positional
encoding, and position-aware attention mechanisms, each offering different trade-offs in terms of performance,
efficiency, and maximum sequence length.

##### Transformer Block Architecture

With attention mechanisms and positional encoding in place, we can now explore the complete architecture of a
transformer block—the fundamental building unit of transformer models.

<div align="center">
<img src="images/b.png" width="600" height="auto">
<p style="color: #555;">Figure: Masked Multi-Head Attention</p>
</div>

A transformer block consists of several components arranged in a specific structure with residual connections and
normalization layers. Let's examine each component and how they work together:

1. **Multi-Head Self-Attention Layer**

    As we've discussed, this layer allows each position to attend to all positions, capturing different types of
    relationships through multiple attention heads.

    The key innovation of transformers is replacing recurrent or convolutional operations with self-attention, enabling:

    - Direct modeling of long-range dependencies
    - Parallel computation across the sequence
    - More interpretable attention patterns

2. **Residual Connection and Layer Normalization**

    After the self-attention layer, transformers employ two techniques to improve training stability:

    - **Residual Connection**: The input to the self-attention layer is added to its output, creating a direct path for
      gradients to flow during backpropagation. This helps combat the vanishing gradient problem in deep networks:

        $\text{output} = \text{LayerNorm}(\text{input} + \text{SelfAttention}(\text{input}))$

    - **Layer Normalization**: Normalizes the output across the feature dimension, standardizing the mean and variance
      for each position in the sequence. This stabilizes the activations and speeds up training:

        $\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$

        Where $\mu$ and $\sigma$ are the mean and standard deviation of the features, and $\gamma$ and $\beta$ are
        learnable parameters.

    The specific arrangement of normalization layers has evolved across transformer implementations. The original
    architecture used "post-norm" (normalization after addition), while many modern implementations use "pre-norm"
    (normalization before attention and FFN), which provides more training stability.

3. **Position-wise Feed-Forward Network**

    After the attention mechanism, each position is processed independently through a small two-layer neural network:

    $\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$

    This network consists of:

    - A linear transformation expanding the dimension (typically by a factor of 4)
    - A ReLU activation function
    - A second linear transformation projecting back to the original dimension

    The feed-forward network serves several important purposes:

    - Adds non-linearity to the model
    - Processes the attention outputs further
    - Provides additional parameter capacity

    Since the same FFN is applied to each position independently, it's called "position-wise." It's essentially a 1x1
    convolution or a token-by-token transformation.

4. **Second Residual Connection and Layer Normalization**

    Similar to the first residual connection, the output from the multi-head attention is combined with the output from
    the feed-forward network:

    $\text{output} = \text{LayerNorm}(\text{attention\_output} + \text{FFN}(\text{attention\_output}))$

    This creates a second gradient pathway, further improving training dynamics.

The complete transformer block can be summarized as:

```shell
x → [Multi-Head Attention → Add & Norm → Feed-Forward → Add & Norm] → output
     ↑                                   ↑
     └───────────────x────────────┘     └───────────output of attention───┘
```

<div align="center">
<img src="images/transformer_block.png" width="600" height="auto">
<p style="color: #555;">Figure: Transformer Block</p>
</div>

This block structure is then stacked multiple times to create the full transformer model. The original transformer paper
used 6 identical layers for both the encoder and decoder, but modern implementations often use more layers (BERT has
12-24, GPT-3 has 96).

<div align="center">
<img src="images/m1.png" width="600" height="auto">
<p style="color: #555;">Figure: Encoder-Decoder Architecture</p>
</div>

In the encoder-decoder architecture, the transformer block in the decoder has an additional component:

- An encoder-decoder attention layer that allows the decoder to attend to all positions in the encoder output
- This layer sits between the masked self-attention and the feed-forward network

For autoregressive models like GPT, the self-attention mechanism in the decoder is masked to prevent attending to future
positions, ensuring the model only conditions on previous tokens when generating text.

<div align="center">
<img src="images/m2.png" width="600" height="auto">
<p style="color: #555;">Figure: Translating using Encoder-Decoder Architecture</p>
</div>

The transformer block architecture combines several key innovations:

1. Self-attention for capturing long-range dependencies
2. Multi-head attention for representing different types of relationships
3. Residual connections for improving gradient flow
4. Layer normalization for stabilizing training
5. Position-wise FFN for adding non-linearity and capacity

This elegant design has proven remarkably effective across a wide range of sequence processing tasks, from machine
translation to language modeling, establishing transformers as the backbone of modern NLP systems.

Understanding the transformer architecture at this level provides the foundation for working with, adapting, and
innovating on modern language models, which have revolutionized natural language processing and continue to drive
breakthroughs in artificial intelligence.

#### Tokenization and Embeddings in NLP

Natural Language Processing (NLP) systems must first transform raw text into a numerical representation before they can
process it effectively. This transformation involves two critical steps: tokenization, which segments text into discrete
units, and embedding, which maps these tokens to numerical vectors. These processes fundamentally shape how machines
understand and generate human language.

##### Tokenization Methods

Tokenization is the foundation of text processing, breaking down a sequence of characters into meaningful units called
tokens. This seemingly straightforward process involves numerous design decisions that significantly impact model
performance.

At its most basic level, tokenization divides text along white spaces and punctuation to produce word-level tokens.
However, this simple approach fails to address the complexity and diversity of natural language. Modern tokenization
approaches include:

**Word-level Tokenization**

Word tokenization splits text at word boundaries, typically using spaces and punctuation as delimiters. For example, the
sentence "The cat sat on the mat." would be tokenized as ["The", "cat", "sat", "on", "the", "mat", "."]. While
intuitive, word tokenization faces several challenges:

1. **Vocabulary explosion**: Languages contain millions of unique words, including inflections, derivations, compounds,
   and neologisms. Creating embeddings for each possible word requires enormous vocabulary sizes.
2. **Out-of-vocabulary (OOV) words**: Words not encountered during training cannot be directly represented, requiring
   fallback strategies like using a special [UNK] token or handling at the character level.
3. **Morphological variation**: Word tokenization treats morphologically related words (e.g., "run," "running," "runs")
   as entirely distinct entities, failing to capture their semantic relationships.
4. **Language dependence**: Word boundaries are not clearly marked in many languages. For instance, Chinese and Japanese
   texts do not use spaces between words, requiring specialized segmentation algorithms.

Various libraries implement word tokenization, with subtle differences in how they handle punctuation, contractions, and
special characters. For example, NLTK and spaCy offer language-aware tokenizers that handle language-specific nuances
like contractions in English (e.g., "don't" → "do", "n't") or compound words in German.

**Character-level Tokenization**

Character tokenization treats individual characters as tokens, addressing many limitations of word tokenization. The
sentence "The cat" would be tokenized as ["T", "h", "e", " ", "c", "a", "t"]. This approach offers several advantages:

1. **Closed vocabulary**: The character set is finite and known in advance, eliminating OOV issues.
2. **Language-agnostic operation**: Works without modification across many languages, including those without clear word
   boundaries.
3. **Morphological awareness**: Captures subword patterns implicitly through character sequences.

However, character tokenization also has drawbacks:

1. **Sequence length**: Character sequences are much longer than word sequences, increasing computational demands and
   making it harder to capture long-range dependencies.
2. **Semantic dilution**: The semantic information present in a word becomes diffused across many tokens, potentially
   making it harder for models to capture meaning.
3. **Inefficiency for common patterns**: Common morphological patterns must be relearned from scratch instead of being
   represented as single units.

Character tokenization has proven particularly effective for specialized tasks like spelling correction, informal text
processing (with many spelling variations), and processing languages with complex morphology or no clear word
boundaries.

**Hybrid Approaches**

Hybrid methods combine word-level and character-level tokenization to balance their strengths and weaknesses. These
include:

1. **Word+Character models**: Use word tokenization when possible, but fall back to character-level representation for
   OOV words.
2. **Mixed-granularity models**: Process text at multiple granularity levels simultaneously (words, characters, and
   potentially other units).

Hybrid approaches can be particularly effective for handling languages with complex morphology or mixed scripts, but
they add complexity to model architecture and training procedures.

Modern NLP systems have increasingly moved toward subword tokenization methods, which we'll cover in detail in a later
section. These methods provide an elegant solution to many of the challenges faced by both word-level and
character-level approaches.

##### Word Embeddings Techniques

Word embeddings are dense vector representations of words that capture semantic relationships in a continuous vector
space. Unlike sparse one-hot encodings, where each word is represented by a vector with a single 1 and all other entries
0, embeddings map words to dense vectors typically ranging from 50 to 300 dimensions.

Several landmark techniques have shaped the development of word embeddings:

**Word2Vec**

Developed by Mikolay et al. at Google in 2013, Word2Vec revolutionized word representation by learning embeddings
through context prediction tasks. Word2Vec comes in two flavors:

1. **Continuous Bag of Words (CBOW)**: Predicts a target word given its surrounding context words. For example, given
   "The \_\_\_\_ sat on the mat," the model predicts "cat."
2. **Skip-gram**: The inverse of CBOW, predicting context words given a target word. Given "cat," the model predicts
   surrounding words like "the," "sat," "on."

Mathematically, Skip-gram maximizes the average log probability:

$\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} | w_t)$

Where $c$ is the context window size, $T$ is the text length, and $w_t$ is the target word.

Word2Vec produces embeddings with remarkable properties:

- Words with similar meanings cluster together in the vector space.
- Vector arithmetic captures semantic relationships: vec("king") - vec("man") + vec("woman") ≈ vec("queen").
- The distance between word vectors correlates with semantic similarity.
- The embeddings encode various linguistic regularities and patterns.

**GloVe (Global Vectors)**

Developed by Pennington et al. at Stanford in 2014, GloVe combines the benefits of count-based methods (like Latent
Semantic Analysis) with prediction-based methods (like Word2Vec). GloVe works by factorizing the logarithm of the word
co-occurrence matrix.

The core insight of GloVe is that the ratio of co-occurrence probabilities can encode meaning. For instance, if we have
words $i$ and $j$ and two context words $k$ and $l$, the ratio $P_{ik}/P_{jk}$ will be large if word $i$ is related to
context $k$ but word $j$ is not, close to 1 if both or neither are related, and small if word $j$ is related but word
$i$ is not.

The GloVe objective function is:

$J = \sum_{i,j=1}^{V} f(X_{ij})(\mathbf{w}_i^T \tilde{\mathbf{w}}_j + b_i + \tilde{b}*j - \log X*{ij})^2$

Where:

- $X_{ij}$ is the co-occurrence count between words $i$ and $j$
- $\mathbf{w}_i$ and $\tilde{\mathbf{w}}_j$ are word vectors
- $b_i$ and $\tilde{b}_j$ are bias terms
- $f(X_{ij})$ is a weighting function that prevents rare co-occurrences from being overweighted

GloVe embeddings generally perform similarly to Word2Vec but can be more efficient to train on large corpora since they
directly utilize global co-occurrence statistics.

**FastText**

Developed by Facebook AI Research in 2016, FastText extends Word2Vec by representing each word as a bag of character
n-grams. For example, the word "apple" with n=3 would be represented as {"<ap", "app", "ppl", "ple", "le>"} plus the
whole word "apple".

The key advantage of FastText is handling OOV words and morphologically rich languages better than Word2Vec and GloVe.
By learning representations for character n-grams rather than whole words, FastText can generate embeddings for
previously unseen words based on their subword components.

FastText uses the same Skip-gram or CBOW objectives as Word2Vec but with the modification that each word is represented
as the sum of its constituent n-gram vectors.

The subword approach is particularly beneficial for:

- Languages with rich morphology like Finnish, Turkish, or German
- Technical texts with many compound words
- Social media content with misspellings and neologisms

**Implementation Considerations**

When implementing or selecting word embeddings, several practical considerations arise:

1. **Dimensionality**: Higher dimensions can capture more nuanced relationships but require more data to train
   effectively and consume more memory. Common dimensions range from 50 (for simple tasks) to 300 (for complex tasks).
2. **Context window size**: Larger windows capture topical similarity, while smaller windows capture functional or
   syntactic similarity.
3. **Minimum frequency thresholds**: Words appearing fewer than a threshold number of times are often excluded or mapped
   to a special token like [UNK].
4. **Preprocessing**: Choices around lowercasing, stemming, lemmatization, and handling punctuation can significantly
   impact embedding quality.
5. **Training corpus**: Domain-specific corpora produce embeddings that better capture the nuances relevant to specific
   applications.

Pre-trained word embeddings like Google's Word2Vec trained on Google News, Stanford's GloVe trained on Common Crawl, or
Facebook's FastText trained on Wikipedia provide a strong starting point for many applications and can be fine-tuned on
domain-specific data if needed.

##### Contextual vs. Static Embeddings

Traditional word embeddings like Word2Vec, GloVe, and FastText assign the same vector to a word regardless of its
context. This creates an inherent limitation: words with multiple meanings (polysemy) receive a single representation
that averages their various senses. For example, "bank" could refer to a financial institution or a riverside, but
static embeddings cannot distinguish between these meanings.

**Static Embeddings**

Static embeddings maintain several advantages:

- Computational efficiency both during training and inference
- Straightforward interpretation and visualization
- Easy integration with various downstream models
- Effective for tasks where context is less critical

However, they face significant limitations:

- Inability to distinguish word senses based on context
- Poor handling of homonyms, polysemous words, and contextual variations
- Insensitivity to syntactic roles that vary by context

**Contextual Embeddings**

Contextual embeddings, introduced by models like ELMo, BERT, and GPT, revolutionized NLP by generating dynamic
representations for words based on their surrounding context. In these models, the same word receives different
embeddings in different contexts, capturing its specific meaning in each instance.

The key innovations that enabled contextual embeddings include:

1. **Bidirectional context modeling**: Models like BERT consider both left and right context when generating embeddings,
   unlike traditional language models that only looked at preceding words.
2. **Transformer architecture**: The self-attention mechanism in transformers allows each word to directly attend to all
   other words in the sequence, enabling more effective context modeling.
3. **Pre-training and fine-tuning paradigm**: Models are pre-trained on large corpora with self-supervised objectives
   like masked language modeling, then fine-tuned for specific tasks.

Contextual embeddings have several important characteristics:

1. **Layer-specific representations**: Different layers of deep contextual models capture different aspects of meaning:
    - Lower layers tend to capture syntactic and local features
    - Middle layers capture a mix of syntactic and semantic information
    - Higher layers capture semantic and task-specific information
2. **Context window limitations**: Most contextual models have a maximum context length (e.g., 512 tokens for BERT,
   gradually increasing in newer models), limiting their ability to capture very long-range dependencies.
3. **Computational demands**: Generating contextual embeddings typically requires running the entire model forward pass,
   which is more computationally intensive than simple lookup operations for static embeddings.

**Comparing Performance**

Contextual embeddings have consistently outperformed static embeddings across virtually all NLP tasks, with particularly
dramatic improvements in tasks requiring nuanced understanding of word meaning in context:

1. **Word sense disambiguation**: Contextual models can distinguish between "The bank approved the loan" and "The river
   bank was muddy."
2. **Coreference resolution**: Better handling of pronouns and referring expressions that depend on context.
3. **Syntactic parsing**: Improved understanding of words' grammatical roles in different contexts.
4. **Named entity recognition**: More accurate identification of entities and their types based on contextual cues.

The performance gap is typically larger for:

- Complex semantic tasks requiring deep understanding
- Tasks with ambiguous language and context-dependent meanings
- Scenarios with limited training data where pre-training knowledge helps

**Hybrid Approaches**

In practice, hybrid approaches that leverage both static and contextual embeddings can offer advantages:

1. **Static embeddings for efficiency**: Using static embeddings for computationally constrained environments or initial
   filtering stages.
2. **Contextual embeddings for precision**: Applying more expensive contextual embeddings when nuanced understanding is
   required.
3. **Distillation**: Distilling knowledge from contextual models into more efficient static embeddings.

The evolution from static to contextual embeddings represents one of the most significant paradigm shifts in NLP,
contributing substantially to the remarkable progress in language understanding capabilities in recent years.

##### Embedding Vector Properties

Word embeddings derive their utility from specific mathematical and structural properties that enable them to capture
semantic and syntactic relationships between words. Understanding these properties helps explain why embeddings work so
well for NLP tasks and guides their effective application.

**Geometric Properties**

Embedding spaces exhibit several important geometric properties:

1. **Cosine similarity**: The cosine of the angle between two word vectors often correlates strongly with semantic
   similarity:

    $\text{similarity}(A, B) = \frac{\vec{A} \cdot \vec{B}}{|\vec{A}||\vec{B}|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}$

    For normalized vectors, this simplifies to the dot product. Words with similar meanings have vectors pointing in
    similar directions, resulting in high cosine similarity.

2. **Euclidean distance**: The straight-line distance between vectors can also indicate semantic distance, though it's
   more sensitive to vector magnitudes:

    $\text{distance}(A, B) = |\vec{A} - \vec{B}| = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}$

3. **Linear substructures**: Relationships between words often form linear structures. For example, gender relationships
   might align along a consistent direction in the embedding space, allowing for vector arithmetic like:

    $\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}$

4. **Clustering**: Words with similar meanings naturally cluster together in the vector space, allowing for semantic
   grouping.

**Distributional Properties**

Word embeddings are founded on the distributional hypothesis: words that occur in similar contexts tend to have similar
meanings. This manifests in several ways:

1. **Context prediction**: Word2Vec and similar algorithms learn embeddings that are good at predicting context words
   (Skip-gram) or center words from context (CBOW).
2. **Co-occurrence patterns**: GloVe directly leverages global word co-occurrence statistics, encoding these patterns in
   the resulting vectors.
3. **Frequency effects**: Word frequency can influence embedding quality, with rare words typically having less reliable
   embeddings due to limited training examples.

**Mathematical Properties**

Several mathematical properties make embeddings effective for computation:

1. **Dense representation**: Unlike sparse one-hot encodings, embeddings compactly represent semantic information in a
   few hundred dimensions, making them computationally efficient.
2. **Continuous space**: The continuous nature of the vector space allows for smooth interpolation between concepts and
   measuring degrees of similarity.
3. **Compositionality**: Word vectors can be combined (through averaging, concatenation, or more sophisticated
   operations) to represent phrases and sentences.
4. **Dimensionality properties**: The choice of embedding dimension involves a trade-off:
    - Too few dimensions: Insufficient capacity to capture semantic nuances
    - Too many dimensions: Risk of overfitting and computational inefficiency
    - Optimal dimensions: Typically between 100-300 for most applications

**Linguistic Properties**

Embeddings capture various linguistic relationships:

1. **Semantic relationships**:
    - Synonymy: Similar words like "happy" and "glad" have similar vectors
    - Antonymy: Opposites like "hot" and "cold" are often similar in most dimensions but differ strongly in specific
      dimensions
    - Hypernymy/hyponymy: Category relationships like "animal" → "dog" are partially captured
2. **Syntactic relationships**:
    - Part of speech patterns
    - Grammatical number (singular/plural)
    - Tense relationships for verbs
3. **Analogical relationships**: The famous example "man is to woman as king is to queen" demonstrates how embeddings
   capture proportional analogies.

**Limitations and Biases**

Embedding vectors also exhibit important limitations:

1. **Social biases**: Embeddings trained on large corpora absorb and potentially amplify societal biases present in the
   training data. For example, profession words may show gender biases.
2. **Temporal limitations**: Static embeddings don't account for language evolution and changing word meanings over
   time.
3. **Domain specificity**: Embeddings trained on general corpora may perform poorly on specialized domains with unique
   vocabulary and usage patterns.
4. **Sense conflation**: Static embeddings merge different senses of polysemous words into a single representation.

Understanding these properties enables more effective application of embeddings across NLP tasks and informs approaches
to addressing their limitations through techniques like debiasing algorithms, domain-specific training, and contextual
representations.

##### Subword Tokenization Approaches

Subword tokenization bridges the gap between word-level and character-level tokenization by breaking words into
meaningful subunits. This approach addresses vocabulary explosion, handles OOV words, and captures morphological
structure, making it the preferred tokenization method for modern NLP systems.

**Byte Pair Encoding (BPE)**

Originally developed as a data compression algorithm, BPE was adapted for NLP by Sennrich et al. in 2016. The algorithm
starts with a character-level vocabulary and iteratively merges the most frequent adjacent pairs of tokens to form new
subword units.

The BPE training process follows these steps:

1. Initialize the vocabulary with individual characters from the training corpus.
2. Count frequencies of adjacent character pairs in the corpus.
3. Merge the most frequent pair and add the resulting subword to the vocabulary.
4. Update the corpus by replacing all occurrences of the pair with the new subword.
5. Repeat steps 2-4 for a predetermined number of merge operations or until the vocabulary reaches a target size.

During tokenization, words are split using the learned merge operations applied in the same order as during training.
The algorithm greedily applies the longest possible subword substitutions.

BPE offers several advantages:

- Effectively handles rare words and morphologically rich languages
- Produces a fixed-size vocabulary that can be controlled by the number of merge operations
- Segments common words into single tokens while decomposing rare words into meaningful subwords

For example, with appropriate merge operations learned, "unhappiness" might be tokenized as ["un", "happiness"] rather
than character by character or as an OOV token.

BPE is used in models like GPT-2, RoBERTa, and XLM, with variations in preprocessing (e.g., GPT-2 uses byte-level BPE
rather than unicode character-level BPE).

**WordPiece**

Developed by Google, WordPiece is similar to BPE but uses a slightly different merging criterion. Instead of merging the
most frequent pair, WordPiece selects the pair that maximizes the likelihood of the training data when added to the
vocabulary.

The algorithm follows these steps:

1. Start with a character-level vocabulary.
2. Train a language model on the current segmentation of the training corpus.
3. Consider all possible pairs of tokens in the current vocabulary.
4. For each pair, compute how much adding it to the vocabulary would increase the likelihood of the training data
   according to the language model.
5. Add the pair that gives the largest increase in likelihood.
6. Repeat steps 2-5 until reaching the desired vocabulary size.

During tokenization, WordPiece typically uses a greedy longest-match-first approach, and commonly adds special prefix
markers (like "##" in BERT) to subwords that don't start a word.

For example, "embeddings" might be tokenized as ["em", "##bed", "##ding", "##s"] in BERT's WordPiece implementation.

WordPiece is used in models like BERT, DistilBERT, and ALBERT, contributing to their effectiveness across various
languages and domains.

**Unigram Language Model**

The Unigram method, implemented in SentencePiece by Google, approaches subword segmentation as a probabilistic problem.
It starts with a large vocabulary and iteratively prunes it to optimize a language model likelihood.

The algorithm works as follows:

1. Initialize with a large vocabulary of potential subword units (e.g., all character n-grams up to a certain length).
2. Train a unigram language model assuming that tokens are produced independently.
3. Compute the loss in likelihood that would result from removing each token from the vocabulary.
4. Remove the tokens with the smallest loss (typically a fixed percentage in each iteration).
5. Retrain the language model with the reduced vocabulary.
6. Repeat steps 3-5 until reaching the target vocabulary size.

During tokenization, the algorithm finds the most likely segmentation of a word using the trained unigram language
model, typically using the Viterbi algorithm to find the optimal segmentation efficiently.

Unigram model tokenization offers several benefits:

- Produces probabilistic segmentations rather than deterministic ones
- Can consider multiple possible segmentations during training and inference
- Directly optimizes a language modeling objective

The Unigram method is used in SentencePiece and is the tokenizer of choice for models like XLNet and many multilingual
models.

**SentencePiece**

SentencePiece is not a specific algorithm but a tokenization framework that implements both BPE and Unigram methods with
an important addition: it treats the input text as a raw Unicode sequence without any pre-tokenization or
language-specific processing.

This language-agnostic approach offers several advantages:

- Works for any language, including those without explicit word boundaries
- Ensures consistent tokenization across languages
- Handles whitespace as just another character, allowing the model to learn meaningful whitespace patterns

SentencePiece also directly models the reversibility of tokenization by preserving the original text and providing
detokenization functionality.

**Practical Considerations**

When implementing subword tokenization, several practical considerations affect performance:

1. **Vocabulary size**: Larger vocabularies reduce the average number of tokens per sentence, speeding up processing,
   but may not generalize as well to rare words. Common sizes range from 8,000 to 50,000 subwords.
2. **Special tokens**: Most implementations add special tokens like [UNK] (unknown), [CLS] (classification), [SEP]
   (separator), [MASK] (masked token), and [PAD] (padding).
3. **Pre-normalization**: Text normalization choices like lowercase, accent removal, or Unicode normalization impact the
   resulting tokenization.
4. **Handling whitespace**: Different approaches include treating spaces as separate tokens, preserving them as part of
   tokens, or using special markers.
5. **Subword markers**: Visual indicators like "##" in BERT or "▁" (underscore) in SentencePiece help track token
   boundaries.

Subword tokenization has become the standard approach in modern NLP systems due to its balanced handling of vocabulary
size, OOV words, and morphological structure. Most state-of-the-art models like BERT, GPT, RoBERTa, and T5 use some form
of subword tokenization, though they differ in specific implementation details.
