# C-4: Transformer Architecture

1. Transformers and Attention Mechanisms
    - Attention Mechanism Fundamentals
    - Self-Attention Implementation
    - Multi-Head Attention
    - Scaled Dot-Product Attention
    - Positional Encoding
    - Transformer Block Architecture
2. Tokenization and Embeddings in NLP
    - Tokenization Methods
    - Word Embeddings Techniques
    - Contextual vs. Static Embeddings
    - Embedding Vector Properties
    - Subword Tokenization Approaches
