# C-5: Implementation and Applications

1. **Building Transformer Models in PyTorch**
   - Model Components Implementation
   - Attention Block Coding
   - Feedforward Networks
   - Layer Normalization
   - Training and Evaluation Process
2. **Decoder-only Architecture**
   - GPT-style Models
   - Causal Masking Implementation
   - Autoregressive Property
   - Generation Strategies
   - Applications and Limitations
3. **Using Pre-trained Models with Hugging Face**
   - Pipeline API Usage
   - Model Selection Guidelines
   - Text Generation Parameters
   - Fine-tuning Pre-trained Models
   - Multi-modal Applications

---

#### Building Transformer Models

##### Understanding the Transformer Revolution

Imagine you're trying to understand a conversation where multiple people are speaking simultaneously, and you need to
figure out not just what each person is saying, but how their words relate to everyone else's. This is essentially the
challenge that transformers solve in natural language processing. Unlike previous approaches that processed text
sequentially (like reading word by word), transformers can look at all words simultaneously and understand their
relationships, much like how you might grasp the overall meaning of a sentence at a glance rather than parsing it word
by word.

The transformer architecture represents one of the most significant breakthroughs in deep learning, fundamentally
changing how machines process and understand language. To truly appreciate this innovation, let's build our
understanding from the ground up, starting with the key insight: **attention is all you need**.

##### Model Components Implementation

###### The Architecture Blueprint

A transformer model is like a sophisticated assembly line for processing language. At its core, it consists of several
key components that work in harmony:

1. **Input Embeddings**: Convert discrete tokens (words or subwords) into continuous vector representations
2. **Positional Encodings**: Add information about word order (since transformers process all positions simultaneously)
3. **Attention Mechanisms**: Allow the model to focus on relevant parts of the input
4. **Feed-forward Networks**: Process information at each position independently
5. **Layer Normalization**: Stabilize the learning process
6. **Output Projections**: Convert internal representations back to predictions

Let's understand each component in detail:

###### Configuration Parameters

Before building a transformer, we need to define its architecture through configuration parameters. Think of these as
the blueprint specifications:

- **Vocabulary Size**: The total number of unique tokens the model can recognize (typically 30,000-50,000 for most
  languages)
- **Hidden Size (d_model)**: The dimensionality of the model's internal representations (commonly 768 for BERT-base,
  1024 for BERT-large)
- **Number of Attention Heads**: How many different "perspectives" the model uses to look at the input (typically 12
  or 16)
- **Number of Layers**: The depth of the network (12 for BERT-base, 24 for BERT-large, up to 96 for GPT-3)
- **Intermediate Size**: The dimensionality of the feed-forward network's hidden layer (typically 4× the hidden size)

These parameters create a delicate balance: larger values generally improve performance but increase computational
requirements exponentially.

###### The Embedding Layer: From Words to Vectors

The journey of understanding begins with embeddings. Consider the word "cat" – to a computer, this is just a sequence of
characters. The embedding layer transforms it into a dense vector of numbers that captures its meaning in a
high-dimensional space.

**Word Embeddings**: Each token in our vocabulary is assigned a unique vector of size `hidden_size`. Initially random,
these vectors are learned during training to capture semantic relationships. For example:

- Similar words (cat, kitten) will have similar vectors
- The vector arithmetic can capture analogies: king - man + woman ≈ queen

**Positional Embeddings**: Since transformers process all positions simultaneously, they lose the natural ordering of
words. Positional embeddings solve this by adding position-specific patterns to each word embedding. For a sequence of
length $n$, we create position embeddings for positions $0, 1, 2, ..., n-1$.

The final embedding for a token at position $i$ is:

$$
\text{embedding}_i = \text{word embedding}_i + \text{position embedding}_i
$$

**Numerical Example**: Let's say we have a simple 3-word sentence "The cat sleeps" with a hidden size of 4:

- Word embedding for "cat": $[0.2, -0.5, 0.8, 0.1]$
- Position embedding for position 1: $[0.1, 0.0, -0.1, 0.2]$
- Combined embedding: $[0.3, -0.5, 0.7, 0.3]$

This combined representation now contains both semantic information (what the word means) and positional information
(where it appears in the sentence).

##### Attention Block Coding

###### The Heart of the Transformer: Multi-Head Attention

Attention is the transformer's superpower – the ability to dynamically focus on relevant parts of the input when
processing each element. To understand this, imagine reading a complex sentence: "The scientist who discovered
penicillin in 1928 revolutionized medicine." When processing "revolutionized," your brain naturally connects it to
"scientist" despite the intervening words. This is what attention mechanisms achieve mathematically.

###### Scaled Dot-Product Attention

The fundamental operation in attention is computing how much each word should "attend to" every other word. This is done
through three transformations:

1. **Query (Q)**: "What am I looking for?"
2. **Key (K)**: "What do I have to offer?"
3. **Value (V)**: "What information do I contain?"

The attention formula is:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Let's break this down step by step:

**Step 1: Compute Attention Scores**

$$
\text{scores} = QK^T
$$

This matrix multiplication computes how well each query matches each key. Higher scores indicate stronger relationships.

**Step 2: Scale the Scores**

$$
\text{scaled scores} = \frac{\text{scores}}{\sqrt{d_k}}
$$

We divide by $\sqrt{d_k}$ (where $d_k$ is the dimension of the keys) to prevent the scores from becoming too large,
which would cause the softmax to saturate.

**Step 3: Apply Softmax**

$$
\text{attention weights} = \text{softmax}(\text{scaled scores})
$$

This converts scores to probabilities that sum to 1 for each query.

**Step 4: Weighted Sum of Values**

$$
\text{output} = \text{attention weights} \cdot V
$$

The final output is a weighted combination of all values, where weights represent the attention each position pays to
others.

**Numerical Example**: Consider a simplified 3-token sequence with dimension 2:

Queries $Q$:

$$
Q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix}
$$

Keys $K$:

$$
K = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix}
$$

Values $V$:

$$
V = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix}
$$

Step 1:

$$
QK^T = \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 1 & 2 \end{bmatrix}
$$

Step 2: Scale by $\sqrt{2}$:

$$
\begin{bmatrix} 0.71 & 0 & 0.71 \\ 0 & 0.71 & 0.71 \\ 0.71 & 0.71 & 1.41 \end{bmatrix}
$$

Step 3: Apply softmax (row-wise):

$$
\begin{bmatrix} 0.38 & 0.24 & 0.38 \\ 0.24 & 0.38 & 0.38 \\ 0.24 & 0.24 & 0.52 \end{bmatrix}
$$

Step 4: Multiply by values to get final attention output.

###### Multi-Head Attention: Multiple Perspectives

Instead of using a single attention function, transformers use multiple "attention heads" that can focus on different
types of relationships. Think of it like having multiple experts, each looking for different patterns:

- One head might focus on syntactic relationships (subject-verb agreement)
- Another might capture semantic dependencies (word meanings)
- Yet another might track coreference (pronouns and their antecedents)

For $h$ heads with model dimension $d_{model}$:

- Each head has dimension $d_k = d_{model} / h$
- Separate $Q$, $K$, $V$ projections for each head
- Outputs are concatenated and projected back to $d_{model}$

The multi-head attention formula:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
$$

Where each head is:

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

###### Residual Connections and Layer Normalization

After the attention computation, two crucial operations ensure stable training:

1. **Residual Connection**: Add the input back to the output $$\text{output} = \text{input} + \text{AttentionOutput}$$

   This helps with gradient flow and allows the network to learn incremental changes.

2. **Layer Normalization**: Normalize across the feature dimension
   $$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

   Where $\mu$ and $\sigma$ are the mean and standard deviation computed across the hidden dimension for each position.

##### Feedforward Networks

###### The Processing Power

After attention determines what information to focus on, the feed-forward network (FFN) processes this information at
each position independently. Think of it as the "thinking" step after the "looking" step of attention.

The FFN consists of two linear transformations with a non-linear activation in between:

$$
\text{FFN}(x) = W_2 \cdot \text{activation}(W_1 \cdot x + b_1) + b_2
$$

Where:

- $W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}$ expands the dimension (typically $d_{ff} = 4 \cdot d_{model}$)
- $W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$ projects back to model dimension
- The activation function is typically GELU or ReLU

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_01.png" width="400" height="auto"> <p style="color: #555;">Figure: Feedforward Block</p> </div>

###### Why the Expansion?

The expansion to a larger intermediate dimension ($d_{ff} = 4 \cdot d_{model}$) gives the network more capacity to learn
complex patterns. It's like temporarily giving the model more "workspace" to process information before compressing it
back to the original size.

**Numerical Example**: With $d_{model} = 768$ and $d_{ff} = 3072$:

- Input: vector of size 768
- After $W_1$: expanded to size 3072
- After activation: non-linear transformation applied
- After $W_2$: compressed back to size 768

This expansion-compression pattern allows the model to learn more complex transformations than a single linear layer
would permit.

##### Layer Normalization

###### Stabilizing the Learning Process

Layer normalization is crucial for training deep networks effectively. Unlike batch normalization (which normalizes
across the batch dimension), layer normalization operates on each example independently, normalizing across the feature
dimension.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_02.png" width="400" height="auto"> <p style="color: #555;">Figure: Layer Normalization</p> </div>

The formula:

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

Where for each position in the sequence:

- $\mu = \frac{1}{d} \sum_{i=1}^{d} x_i$ (mean across hidden dimension)
- $\sigma^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2$ (variance across hidden dimension)
- $\gamma$ and $\beta$ are learned parameters for scaling and shifting
- $\epsilon$ is a small constant (typically $10^{-12}$) for numerical stability

**Numerical Example**: Consider a hidden state vector: $x = [2, 4, 6, 8]$

- Mean: $\mu = (2+4+6+8)/4 = 5$
- Variance: $\sigma^2 = ((2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2)/4 = 5$
- Standard deviation: $\sigma = \sqrt{5} \approx 2.24$
- Normalized: $x_{norm} = [(2-5)/2.24, (4-5)/2.24, (6-5)/2.24, (8-5)/2.24]$
- Result: $x_{norm} \approx [-1.34, -0.45, 0.45, 1.34]$

With learned $\gamma = 1$ and $\beta = 0$, the output equals the normalized values.

###### Pre-Norm vs Post-Norm

There are two common patterns for applying layer normalization:

**Post-Norm** (Original Transformer): $$\text{output} = \text{LayerNorm}(\text{input} + \text{Sublayer}(\text{input}))$$

**Pre-Norm** (Modern Transformers): $$\text{output} = \text{input} + \text{Sublayer}(\text{LayerNorm}(\text{input}))$$

Pre-norm has become more popular because:

- It provides better gradient flow in very deep networks
- It stabilizes training, especially for large models
- It often converges faster

##### Training and Evaluation Process

###### The Learning Journey

Training a transformer involves several key considerations that go beyond standard neural network training:

**1. Learning Rate Scheduling**

Transformers benefit from a specific learning rate schedule with warmup:

- Start with a very low learning rate
- Linearly increase for the first few thousand steps (warmup)
- Then decrease, often following an inverse square root schedule

The formula for the original transformer schedule:

$$
lr = d_{model}^{-0.5} \cdot \min(\text{step}^{-0.5}, \text{step} \cdot \text{warmup steps}^{-1.5})
$$

This schedule helps because:

- Initial warmup prevents large gradient updates when parameters are random
- Gradual decay helps fine-tune the model as training progresses

**2. Gradient Clipping**

Due to the complex interactions in transformers, gradients can occasionally explode. Gradient clipping prevents this by
scaling down gradients that exceed a threshold:

$$
g' = \begin{cases} g & \text{if } ||g|| \leq \text{threshold} \\ \frac{\text{threshold}}{||g||} \cdot g & \text{otherwise} \end{cases}
$$

Typical threshold values range from 1.0 to 5.0.

**3. Loss Calculation for Language Modeling**

For autoregressive language modeling, we use cross-entropy loss:

$$
\mathcal{L} = -\sum_{t=1}^{T} \log P(x_t | x_{<t})
$$

Where $P(x_t | x_{<t})$ is the model's predicted probability for the correct token at position $t$.

**4. Perplexity as an Evaluation Metric**

Perplexity measures how well a model predicts text:

$$
\text{Perplexity} = \exp\left(\frac{1}{N} \sum_{i=1}^{N} -\log P(x_i | x_{<i})\right)
$$

Lower perplexity indicates better performance. A perplexity of 20 means the model is as confused as if it were choosing
uniformly among 20 alternatives at each step.

**5. Computational Considerations**

Training transformers requires careful resource management:

- **Memory**: Attention has $O(n^2)$ memory complexity for sequence length $n$
- **Computation**: Each layer requires $O(n^2 \cdot d + n \cdot d^2)$ operations
- **Gradient Accumulation**: For large models that don't fit in memory, accumulate gradients over multiple smaller
  batches

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_04.png" width="400" height="auto"> <p style="color: #555;">Figure: Encoder-Decoder Architecture</p> </div>

#### Decoder-only Architecture

##### The Evolution to Simplicity

While the original transformer used both encoder and decoder components, researchers discovered that using only the
decoder portion could achieve remarkable results for many language tasks. This insight led to the development of GPT
(Generative Pre-trained Transformer) and similar models that have revolutionized natural language processing.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_05.png" width="400" height="auto"> <p style="color: #555;">Figure: Encoder-Decoder Architecture</p> </div>

The key insight is elegant: by training a model to predict the next word given all previous words, we can create a
system that naturally learns to understand and generate language. This autoregressive approach mirrors how humans
produce text – one word at a time, with each choice influenced by what came before.

##### GPT-style Models

###### The Architectural Philosophy

GPT-style models embrace radical simplification while maintaining the power of the transformer architecture. Instead of
separate encoding and decoding stages, these models use a unified stack of transformer decoder blocks, modified to work
without encoder input.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_06.png" width="400" height="auto"> <p style="color: #555;">Figure: Other Model Architectures</p> </div>

The core design principles that make GPT models powerful:

**1. Unidirectional (Causal) Attention**

Unlike bidirectional models (like BERT) that can look at the entire context, GPT models use causal attention where each
position can only attend to previous positions. This constraint is essential for generation – you can't look at words
that haven't been generated yet!

The attention mask ensures position $i$ can only attend to positions $j \leq i$:
$$\text{Mask}_{ij} = \begin{cases} 0 & \text{if } j \leq i \ -\infty & \text{if } j > i \end{cases}$$

**2. Deep Architecture**

GPT models stack many transformer layers to build increasingly abstract representations:

- GPT-1: 12 layers
- GPT-2: Up to 48 layers
- GPT-3: Up to 96 layers
- GPT-4: Estimated 100+ layers

Each layer refines the representation, with lower layers capturing syntax and grammar, middle layers understanding
semantics, and upper layers handling complex reasoning.

**3. The Language Modeling Objective**

The training objective is deceptively simple – predict the next token:

$$
\mathcal{L}(\theta) = -\sum_{i=1}^{n} \log P_\theta(x_i | x_1, x_2, ..., x_{i-1})
$$

This objective forces the model to learn:

- Syntax (grammatical structure)
- Semantics (word meanings and relationships)
- World knowledge (facts and common sense)
- Reasoning patterns (logical connections)

**4. Scale as a Key Ingredient**

The GPT family demonstrates that scale dramatically improves capabilities:

| Model | Parameters | Training Tokens | Emergent Capabilities                        |
| ----- | ---------- | --------------- | -------------------------------------------- |
| GPT-1 | 117M       | ~5B             | Basic text completion                        |
| GPT-2 | 1.5B       | ~10B            | Coherent paragraph generation                |
| GPT-3 | 175B       | ~300B           | Few-shot learning, complex reasoning         |
| GPT-4 | ~1T+       | ~10T+           | Multimodal understanding, advanced reasoning |

The relationship between scale and capability appears to follow power laws, with new abilities emerging at certain scale
thresholds.

##### Causal Masking Implementation

###### The Mathematics of Looking Backward

Causal masking is the mechanism that enforces the autoregressive property in decoder-only transformers. It prevents each
position from "seeing into the future" – a critical constraint for valid text generation.

Consider a sequence of 5 tokens. The causal mask looks like:

$$
\text{Mask} = \begin{bmatrix} 0 & -\infty & -\infty & -\infty & -\infty \\ 0 & 0 & -\infty & -\infty & -\infty \\ 0 & 0 & 0 & -\infty & -\infty \\ 0 & 0 & 0 & 0 & -\infty \\ 0 & 0 & 0 & 0 & 0 \end{bmatrix}
$$

When added to attention scores before softmax, the $-\infty$ values become 0 after softmax, effectively blocking
attention to future positions.

**How It Works in Practice**:

1. **During Training**: All positions are processed in parallel, but each position only attends to previous positions
2. **During Generation**: Tokens are generated one at a time, naturally respecting causality

**Numerical Example**: For the sentence "The cat sits", when processing "sits":

- Can attend to: "The" (position 0), "cat" (position 1), "sits" (position 2)
- Cannot attend to: Any future positions (which don't exist yet during generation)

The attention scores before masking might be:

$$
\text{Scores} = \begin{bmatrix} 2.1 & 1.5 & 3.2 & 0.8 \end{bmatrix}
$$

After applying causal mask for position 2:

$$
\text{Masked Scores} = \begin{bmatrix} 2.1 & 1.5 & 3.2 & -\infty \end{bmatrix}
$$

After softmax:

$$
\text{Attention Weights} = \begin{bmatrix} 0.25 & 0.15 & 0.60 & 0.00 \end{bmatrix}
$$

##### Autoregressive Property

###### The Chain Rule of Language

The autoregressive property is the mathematical foundation that allows decoder-only models to generate coherent text.
It's based on the chain rule of probability, decomposing the joint probability of a sequence into a product of
conditional probabilities.

For a sequence $x = (x_1, x_2, ..., x_n)$:

$$
P(x) = P(x_1) \cdot P(x_2|x_1) \cdot P(x_3|x_1, x_2) \cdot ... \cdot P(x_n|x_1, ..., x_{n-1})
$$

This can be written more compactly as:

$$
P(x) = \prod_{i=1}^{n} P(x_i | x_{<i})
$$

**Why This Matters**:

1. **Natural Language Structure**: Language is inherently sequential – each word depends on previous context
2. **Generation Capability**: By modeling conditional probabilities, we can generate text by sampling from these
   distributions
3. **Flexibility**: The same model can be used for various tasks by framing them as conditional generation

**The Generation Process**:

Starting with a prompt $x_{1:k}$, generate the next token by:

1. Compute $P(x_{k+1} | x_{1:k})$ for all possible tokens
2. Sample from this distribution (or select the maximum)
3. Append the selected token to the sequence
4. Repeat until reaching a stop condition

**Mathematical Properties**:

The log-likelihood of a sequence under the model:

$$
\log P(x) = \sum_{i=1}^{n} \log P(x_i | x_{<i})
$$

This additive form is convenient for:

- Gradient computation during training
- Comparing different sequences (higher log-likelihood = more probable)
- Computing perplexity: $\text{PPL} = \exp(-\frac{1}{n} \log P(x))$

**Challenges of Autoregression**:

1. **Error Accumulation**: Mistakes compound as generation proceeds
2. **Exposure Bias**: Training sees perfect context, but generation uses model predictions
3. **Length Bias**: Tendency toward shorter or longer sequences depending on training

##### Generation Strategies

###### The Art of Sampling from Language Models

Once trained, a decoder-only model provides probability distributions over the vocabulary at each step. How we sample
from these distributions dramatically affects the generated text's quality, creativity, and coherence.

**1. Greedy Decoding**

The simplest approach: always select the highest probability token.

$$
x_{t+1} = \arg\max_{x} P(x | x_{1:t})
$$

Advantages:

- Deterministic and reproducible
- Fast computation
- Good for tasks requiring accuracy

Disadvantages:

- Repetitive and predictable output
- Can get stuck in loops
- Misses potentially better sequences

**2. Beam Search**

Maintain $k$ most probable sequences (beams) at each step:

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_07.png" width="400" height="auto"> <p style="color: #555;">Figure: Beam Search</p> </div>

At each step:

1. Expand each beam with top-$k$ next tokens
2. Keep only the $k$ highest-scoring sequences overall
3. Continue until all beams reach end tokens

The score of a sequence:

$$
\text{Score}(x_{1:n}) = \sum_{i=1}^{n} \log P(x_i | x_{<i})
$$

Often normalized by length to avoid bias toward shorter sequences:

$$
\text{Normalized Score} = \frac{1}{n^\alpha} \sum_{i=1}^{n} \log P(x_i | x_{<i})
$$

Where $\alpha \in [0.6, 0.7]$ is the length penalty factor.

**3. Temperature Sampling**

Modify the probability distribution's sharpness before sampling:

$$
P'(x_i) = \frac{\exp(\log P(x_i) / T)}{\sum_j \exp(\log P(x_j) / T)}
$$

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_03.png" width="400" height="auto"> <p style="color: #555;">Figure: Temperature Parameter</p> </div>

Where $T$ is the temperature:

- $T < 1$: Sharper distribution (more focused)
- $T = 1$: Original distribution
- $T > 1$: Flatter distribution (more random)

**Numerical Example**: Original probabilities: [0.7, 0.2, 0.1]

With $T = 0.5$ (focused):

- Logits: [-0.357, -1.609, -2.303]
- Scaled: [-0.714, -3.218, -4.606]
- New probabilities: [0.89, 0.08, 0.02]

With $T = 2.0$ (creative):

- Scaled: [-0.179, -0.805, -1.152]
- New probabilities: [0.48, 0.31, 0.21]

**4. Top-k Sampling**

Restrict sampling to the $k$ most likely tokens:

1. Sort tokens by probability
2. Keep only top $k$
3. Renormalize probabilities
4. Sample from this reduced distribution

This prevents sampling very unlikely tokens while maintaining diversity.

**5. Nucleus (Top-p) Sampling**

Dynamically select the smallest set of tokens whose cumulative probability exceeds threshold $p$:

1. Sort tokens by probability (descending)
2. Calculate cumulative sum
3. Keep tokens until cumulative probability > $p$
4. Renormalize and sample

**Example with $p = 0.9$**:

- Token probabilities: $[0.4, 0.3, 0.15, 0.08, 0.04, 0.03]$
- Cumulative: $[0.4, 0.7, 0.85, 0.93, 0.97, 1.0]$
- Keep first 4 tokens (cumulative = 0.93 > 0.9)
- Renormalize: $[0.43, 0.32, 0.16, 0.09]$

**6. Repetition Control**

Several techniques prevent repetitive text:

**Repetition Penalty**: Reduce probability of previously used tokens by factor $\rho$:

$$
P'(x_i) = \begin{cases} P(x_i) / \rho & \text{if } x_i \in \text{previous tokens} \\ P(x_i) & \text{otherwise} \end{cases}
$$

**N-gram Blocking**: Forbid exact repetition of n-grams:

- Track all n-grams in generated text
- Set probability to 0 for tokens that would create repeated n-grams

**Comparison of Strategies**:

| Strategy        | Coherence | Diversity | Use Case          |
| --------------- | --------- | --------- | ----------------- |
| Greedy          | High      | Low       | Factual Q&A       |
| Beam Search     | High      | Low       | Translation       |
| Temperature=0.7 | Good      | Moderate  | General text      |
| Top-k=40        | Good      | Good      | Creative writing  |
| Top-p=0.9       | Good      | Adaptive  | Most applications |

##### Applications and Limitations

###### Where Decoder-Only Models Excel

**Applications**:

1. **Text Generation**: From creative writing to technical documentation
2. **Code Generation**: Understanding programming patterns and generating functional code
3. **Conversational AI**: Maintaining context over long dialogues
4. **Few-Shot Learning**: Adapting to new tasks with just a few examples
5. **Reasoning Tasks**: Chain-of-thought prompting for complex problem-solving

The versatility comes from the models' ability to:

- Capture long-range dependencies
- Generate coherent text at multiple scales
- Adapt to various domains through prompting

**Fundamental Limitations**:

**1. Hallucination Problem**

Models can generate plausible-sounding but false information. This occurs because:

- Training objective rewards likely text, not truthful text
- Models interpolate between training examples
- No explicit fact-checking mechanism

Mitigation strategies:

- Retrieval-augmented generation (RAG)
- Fine-tuning on curated factual data
- Post-generation fact-checking

**2. Context Length Constraints**

All transformers have finite context windows:

- Attention complexity: $O(n^2)$ for sequence length $n$
- Memory requirements grow quadratically
- Information at the beginning of long contexts may be "forgotten"

Recent innovations (sparse attention, linear attention) partially address this.

**3. Reasoning Limitations**

While capable of impressive reasoning, models struggle with:

- Multi-step mathematical proofs
- Consistent logical deduction
- Causal reasoning
- Temporal reasoning

Chain-of-thought prompting helps but doesn't fully solve these issues.

**4. Computational Requirements**

Large models require substantial resources:

- GPT-3 (175B parameters): ~350GB of memory for inference
- Training costs: Millions of dollars for large models
- Inference latency: Proportional to model size and sequence length

This limits accessibility and raises environmental concerns.

**5. Bias and Safety Concerns**

Models inherit biases from training data:

- Social biases (gender, race, culture)
- Geographical biases (overrepresentation of certain regions)
- Linguistic biases (better performance on
- certain languages)

Addressing these requires:

- Diverse training data
- Bias detection and mitigation techniques
- Alignment training (RLHF - Reinforcement Learning from Human Feedback)
- Continuous monitoring and evaluation

##### Future Directions

The field is rapidly evolving to address these limitations:

- **Efficient architectures**: Reducing computational requirements while maintaining performance
- **Multimodal integration**: Combining text with vision, audio, and other modalities
- **Improved factuality**: Better grounding in verified information sources
- **Enhanced reasoning**: Architectural innovations for better logical reasoning
- **Continuous learning**: Updating knowledge without full retraining

##### Using Pre-trained Models with Hugging Face

###### The Democratization of AI

The Hugging Face ecosystem has transformed how practitioners work with transformer models, making state-of-the-art
NLP accessible to everyone. Instead of requiring massive computational resources to train models from scratch,
developers can leverage pre-trained models and adapt them to specific needs.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_08.png" width="400" height="auto"> <p style="color: #555;">Figure: Using Pre-trained Transformers</p> </div>

##### Pipeline API Usage

###### The Power of Abstraction

The Pipeline API represents the highest level of abstraction in the Hugging Face ecosystem. It encapsulates the entire
workflow – from raw text to processed results – in a simple interface. Think of it as a black box that handles all the
complex preprocessing, model inference, and postprocessing steps automatically.

**Core Components of a Pipeline**:

- **Tokenizer**: Converts raw text into tokens the model understands
- **Model**: The pre-trained transformer that processes the tokens
- **Post-processor**: Converts model outputs into human-readable results

**How Pipelines Work Internally**:

When you use a pipeline for sentiment analysis, here's what happens behind the scenes:

- **Text Input**: "I love this movie!"
- **Tokenization**: ["I", "love", "this", "movie", "!"] → [101, 1045, 2293, 2023, 3185, 999, 102]
- **Model Processing**: Tokens → Hidden states → Logits
- **Post-processing**: Logits → Probabilities → {"label": "POSITIVE", "score": 0.999}

**Common Pipeline Types and Their Applications**:

| Pipeline Type            | Input              | Output               | Use Case                   |
| ------------------------ | ------------------ | -------------------- | -------------------------- |
| sentiment-analysis       | Text               | Label + Score        | Customer feedback analysis |
| text-generation          | Prompt             | Generated text       | Content creation           |
| question-answering       | Question + Context | Answer               | Information extraction     |
| ner                      | Text               | Entities + Positions | Information extraction     |
| summarization            | Long text          | Summary              | Document processing        |
| translation              | Text               | Translated text      | Multilingual applications  |
| zero-shot-classification | Text + Labels      | Classifications      | Flexible categorization    |

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_09.png" width="400" height="auto"> <p style="color: #555;">Figure: Hugging Face Library</p> </div>

**Advanced Pipeline Features**:

- **Batch Processing**: Process multiple texts simultaneously for efficiency
- **Device Management**: Automatically handle GPU/CPU allocation
- **Model Caching**: Download models once, use them repeatedly
- **Automatic Model Selection**: Choose appropriate default models for each task

The Pipeline API strikes a perfect balance between simplicity and power, making it ideal for:

- Rapid prototyping
- Production applications with standard requirements
- Educational purposes
- Benchmarking and evaluation

##### Model Selection Guidelines

###### Choosing the Right Tool for the Job

With thousands of models available on the Hugging Face Hub, selecting the optimal model requires understanding the
tradeoffs between different architectures, sizes, and specializations.

**Key Dimensions for Model Selection**:

**1. Architecture Type**

Different architectures excel at different tasks:

| Architecture | Type            | Strengths             | Best For                       |
| ------------ | --------------- | --------------------- | ------------------------------ |
| BERT         | Encoder-only    | Bidirectional context | Classification, NER, QA        |
| GPT          | Decoder-only    | Generation, few-shot  | Text generation, completion    |
| T5           | Encoder-decoder | Versatility           | Translation, summarization     |
| RoBERTa      | Encoder-only    | Robust training       | General understanding tasks    |
| BART         | Encoder-decoder | Denoising             | Text generation, summarization |

**2. Model Size Considerations**

Model size directly impacts performance and resource requirements:

| Size Category | Parameters | Memory   | Inference Speed | Accuracy |
| ------------- | ---------- | -------- | --------------- | -------- |
| Tiny          | <10M       | <50MB    | Very Fast       | Lower    |
| Small         | 10-100M    | 50-500MB | Fast            | Good     |
| Base          | 100-500M   | 0.5-2GB  | Moderate        | Better   |
| Large         | 500M-1B    | 2-4GB    | Slower          | High     |
| XL            | >1B        | >4GB     | Slow            | Highest  |

**The Performance-Efficiency Tradeoff**:

$$
\text{Utility} = \alpha \cdot \text{Performance} - \beta \cdot \text{Computational Cost}
$$

Where $\alpha$ and $\beta$ weight the importance of performance vs. efficiency for your use case.

**3. Domain Specialization**

Models trained on domain-specific data typically outperform general models:

| Domain     | Example Models      | Advantages                             |
| ---------- | ------------------- | -------------------------------------- |
| Biomedical | BioBERT, PubMedBERT | Medical terminology, clinical concepts |
| Legal      | Legal-BERT          | Legal language, case law understanding |
| Financial  | FinBERT             | Financial terminology, sentiment       |
| Scientific | SciBERT             | Scientific literature, citations       |
| Code       | CodeBERT, CodeT5    | Programming languages, documentation   |

**4. Language Support**

Consider your language requirements:

- **Monolingual models**: Better performance for specific language
- **Multilingual models**: Support multiple languages but with some performance tradeoff
- **Cross-lingual models**: Can transfer knowledge between languages

**Decision Framework for Model Selection**:

```mermaid
flowchart TD
    A["Define Requirements"] --> B{"Task Type?"}
    B -->|"Generation"| C["Decoder-only<br>(GPT, OPT)"]
    B -->|"Understanding"| D["Encoder-only<br>(BERT, RoBERTa)"]
    B -->|"Translation/Summary"| E["Encoder-Decoder<br>(T5, BART)"]

    C --> F{"Resource Constraints?"}
    D --> F
    E --> F

    F -->|"Tight"| G["Small/Distilled Models"]
    F -->|"Moderate"| H["Base Models"]
    F -->|"Abundant"| I["Large Models"]

    G --> J{"Domain Specific?"}
    H --> J
    I --> J

    J -->|<span style='background-color:lime; color:black; padding:2px 6px; border-radius:3px'>Yes</span>| K["Domain-Specific Model"]
    J -->|<span style='background-color:orangered; color:white; padding:2px 6px; border-radius:3px'>No</span>| L["General Model"]

    K --> M["Evaluate Performance"]
    L --> M

style A fill:#FCE4EC
style B fill:#F8BBD9
style C fill:#F48FB1
style D fill:#F06292
style E fill:#EC407A
style F fill:#E91E63
style G fill:#D81B60
style H fill:#E1BEE7
style I fill:#CE93D8
style J fill:#BA68C8
style K fill:#AB47BC
style L fill:#B39DDB
style M fill:#9575CD
```

**Evaluation Metrics for Selection**:

- **Task-Specific Metrics**:
  - Classification: Accuracy, F1, Precision, Recall
  - Generation: Perplexity, BLEU, ROUGE
  - QA: Exact Match, F1
- **Efficiency Metrics**:
  - Inference time per sample
  - Memory usage
  - Model size on disk
  - Energy consumption
- **Robustness Metrics**:
  - Performance on out-of-distribution data
  - Adversarial robustness
  - Fairness across demographic groups

##### Text Generation Parameters

###### Fine-Tuning the Creative Process

Text generation involves numerous parameters that control the characteristics of generated text. Understanding these
parameters is essential for achieving the desired balance between creativity, coherence, and quality.

**Core Generation Parameters**:

**1. Temperature ($T$)**

Controls randomness by scaling the logits before softmax:

$$
P'(x_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$$

Where $z_i$ are the raw logits from the model.

Effect on distribution entropy: $$H(P) = -\sum_i P(x_i) \log P(x_i)$$

- Low $T$ → Lower entropy → More focused
- High $T$ → Higher entropy → More diverse

**Numerical Example**: Original logits: $[2.0, 1.0, 0.5, -1.0]$

With $T = 0.5$:

- Scaled: $[4.0, 2.0, 1.0, -2.0]$
- Probabilities: $[0.84, 0.11, 0.04, 0.01]$
- Entropy: $0.68$

With $T = 2.0$:

- Scaled: $[1.0, 0.5, 0.25, -0.5]$
- Probabilities: $[0.42, 0.26, 0.20, 0.12]$
- Entropy: $1.75$

**2. Top-k Sampling**

Limits vocabulary to $k$ most likely tokens:

Algorithm:

- Sort probabilities:
  $$
  P(x_1) \geq P(x_2) \geq ... \geq P(x_V)
  $$
- Keep top $k$:
  $$
  {x_1, x_2, ..., x_k}
  $$
- Renormalize (for $i \leq k$):
  $$
  P'(x_i) = P(x_i) / \sum_{j=1}^k P(x_j)
  $$

Trade-off:

- Small $k$ (5-20): Safe but potentially boring
- Medium $k$ (40-50): Good balance
- Large $k$ (100+): More creative but risk of incoherence

**3. Top-p (Nucleus) Sampling**

Dynamically selects vocabulary size based on cumulative probability:

Algorithm:

- Sort probabilities descending
- Find smallest set $S$ where $\sum_{x \in S} P(x) \geq p$
- Sample from renormalized distribution over $S$

Advantage over top-k: Adapts to the probability distribution's shape

- Peaked distribution → Smaller nucleus
- Flat distribution → Larger nucleus

**4. Repetition Penalty ($\rho$)**

Discourages repetition by modifying probabilities:

$$
P'(x_i) = \begin{cases} P(x_i) / \rho & \text{if } x_i \in \text{generated tokens} \\ P(x_i) \cdot C & \text{otherwise} \end{cases}
$$

Where $C$ is a normalization constant.

Effect scales with repetition count:

- First occurrence: Penalty = $\rho$
- Second occurrence: Penalty = $\rho^2$
- $n$-th occurrence: Penalty = $\rho^n$

**5. Length Control**

**Length Penalty** in beam search:

$$
\text{Score} = \frac{1}{(5 + |Y|)^\alpha} \sum_{i=1}^{|Y|} \log P(y_i | y_{<i})
$$

Where:

- $|Y|$ is sequence length
- $\alpha$ controls preference (typically 0.6-0.7)
- The constant 5 prevents too much penalty for very short sequences

**Optimal Parameter Combinations**:

| Task              | Temperature | Top-p | Top-k | Rep. Penalty |
| ----------------- | ----------- | ----- | ----- | ------------ |
| Factual Q&A       | 0.1-0.3     | 0.7   | 10    | 1.0          |
| Technical Writing | 0.3-0.5     | 0.8   | 40    | 1.1          |
| Creative Writing  | 0.7-0.9     | 0.9   | 50    | 1.2          |
| Poetry            | 0.9-1.2     | 0.95  | 0     | 1.0          |
| Dialogue          | 0.6-0.8     | 0.9   | 40    | 1.3          |

**Advanced Techniques**:

**1. Contrastive Decoding**

Use two models to improve generation:

$$
P_{\text{contrastive}}(x) = P_{\text{large}}(x) - \alpha \cdot P_{\text{small}}(x)
$$

This emphasizes tokens where the large model is more confident than the small model.

**2. Typical Sampling**

Sample from tokens with "typical" information content:

$$
\text{Typical}(x) = \exp(-|H(P) + \log P(x)|)
$$

Where $H(P)$ is the entropy of the distribution.

**3. Mirostat**

Dynamically adjusts top-k to maintain constant perplexity during generation, providing more consistent output quality.

##### Fine-tuning Pre-trained Models

###### Adapting General Intelligence to Specific Tasks

Fine-tuning allows us to specialize pre-trained models for specific domains or tasks, leveraging transfer learning to
achieve high performance with limited data.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_10.png" width="400" height="auto"> <p style="color: #555;">Figure: Fine-tuning Pre-trained Models</p> </div>

**The Transfer Learning Paradigm**:

Pre-training captures general language understanding:

$$
\theta^* = \arg\min_\theta \mathcal{L}_{\text{pretrain}}(\theta; \mathcal{D}_{\text{large}})
$$

Fine-tuning adapts to specific task:

$$
\theta_{\text{fine}} = \arg\min_\theta \mathcal{L}_{\text{task}}(\theta; \mathcal{D}_{\text{task}}) \text{ initialized from } \theta^*
$$

**Why Fine-tuning Works**:

1. **Feature Reuse**: Lower layers learn general features (syntax, basic semantics)
2. **Hierarchical Learning**: Higher layers can be quickly adapted to new tasks
3. **Data Efficiency**: Requires orders of magnitude less data than training from scratch

**Fine-tuning Strategies**:

**1. Full Fine-tuning**

Update all parameters:

- Pros: Maximum flexibility and performance
- Cons: Risk of catastrophic forgetting, requires more data

**2. Layer-wise Fine-tuning**

Different learning rates for different layers:

$$
\eta_l = \eta_{\text{base}} \cdot \lambda^{L-l}
$$

Where $l$ is the layer index and $\lambda < 1$ (typically 0.9).

**3. Parameter-Efficient Fine-tuning (PEFT)**

**LoRA (Low-Rank Adaptation)**: Instead of updating weight matrix $W \in \mathbb{R}^{d \times k}$, learn low-rank
decomposition:

$$
W' = W + BA
$$

Where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with $r \ll \min(d, k)$.

Parameters reduced from $d \times k$ to $r \times (d + k)$.

**Example**: For $d = k = 768$ and $r = 16$:

- Original parameters: 589,824
- LoRA parameters: 24,576 (4% of original)

**Adapter Layers**: Insert small bottleneck layers:

$$
h' = h + f(hW_{\text{down}})W_{\text{up}}
$$

Where $W_{\text{down}} \in \mathbb{R}^{d \times r}$ and $W_{\text{up}} \in \mathbb{R}^{r \times d}$.

**Data Requirements for Fine-tuning**:

| Task Complexity        | Data Required    | Examples                   |
| ---------------------- | ---------------- | -------------------------- |
| Simple Classification  | 100-1K samples   | Sentiment analysis         |
| Complex Classification | 1K-10K samples   | Multi-label classification |
| Generation Tasks       | 10K-100K samples | Summarization              |
| Domain Adaptation      | 100K+ samples    | Medical/Legal domains      |

**Learning Rate Scheduling for Fine-tuning**:

Linear warmup with decay:

$$
\eta(t) = \large \begin{cases} \frac{t}{T_{\text{warm}}} \cdot \eta_{\text{max}} & t < T_{\text{warm}} \\ \eta_{\text{max}} \cdot \frac{T_{\text{total}} - t}{T_{\text{total}} - T_{\text{warm}}} & t \geq T_{\text{warm}} \end{cases}
$$

Typical values:

- $\eta_{\text{max}}$: 2e-5 to 5e-5
- $T_{\text{warm}}$: 6% of total steps
- Weight decay: 0.01

**Preventing Catastrophic Forgetting**:

**1. Elastic Weight Consolidation (EWC)**:

$$
\mathcal{L}_{\text{EWC}} = \mathcal{L}_{\text{task}} + \lambda \sum_i F_i(\theta_i - \theta_i^*)^2
$$

Where $F_i$ is the Fisher information matrix diagonal.

**2. Gradual Unfreezing**:

- Start by training only the top layer
- Progressively unfreeze lower layers
- Prevents disruption of learned features

**3. Mixed Training**: Include some original pre-training data to maintain general capabilities.

##### Multi-modal Applications

###### Beyond Text: Integrating Multiple Modalities

The transformer architecture's flexibility has enabled powerful multi-modal models that process and relate different
types of data – text, images, audio, and more.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_11.png" width="400" height="auto"> <p style="color: #555;">Figure: Multimodal Distribution</p> </div>

**Vision-Language Models**:

**CLIP (Contrastive Language-Image Pre-training)**:

CLIP learns joint embeddings for images and text using contrastive learning:

$$
\mathcal{L}_{\text{CLIP}} = -\frac{1}{N} \sum_{i=1}^{N} \left[ \log \frac{\exp(s_{ii}/\tau)}{\sum_{j=1}^{N} \exp(s_{ij}/\tau)} + \log \frac{\exp(s_{ii}/\tau)}{\sum_{j=1}^{N} \exp(s_{ji}/\tau)} \right]
$$

Where:

- $s_{ij} = \text{similarity}(\text{image}_i, \text{text}_j)$
- $\tau$ is temperature parameter
- Diagonal elements $s_{ii}$ are positive pairs

The model learns to:

- Maximize similarity between matched image-text pairs
- Minimize similarity between unmatched pairs

**Applications of Vision-Language Models**:

- **Zero-shot Image Classification**: Given image $I$ and text labels ${t_1, ..., t_n}$:
  $$
  P(t_i | I) = \frac{\exp(\text{sim}(I, t_i)/\tau)}{\sum_j \exp(\text{sim}(I, t_j)/\tau)}
  $$
- **Image-Text Retrieval**: Find images matching text query or vice versa using embedding similarity.
- **Visual Question Answering**: Combine image and question embeddings to generate answers.

**Audio-Language Models**:

**Wav2Vec2 for Speech Recognition**:

Processes raw audio waveforms:

- **Feature extraction**: CNN layers extract features from raw audio
- **Contextualization**: Transformer layers process features
- **CTC decoding**: Connectionist Temporal Classification for text output

The CTC loss handles alignment between audio and text:

$$
\mathcal{L}_{\text{CTC}} = -\log P(Y | X) = -\log \sum_{\pi \in \mathcal{B}^{-1}(Y)} P(\pi | X)
$$

Where $\mathcal{B}^{-1}(Y)$ maps text $Y$ to all possible alignments.

**Multimodal Fusion Strategies**:

- **Early Fusion**: Combine modalities at input level
- **Late Fusion**: Process separately, combine at decision level
- **Cross-Attention Fusion**: Allow modalities to attend to each other

**Cross-Modal Attention**:

$$
\text{Attention}_{\text{cross}}(Q_{\text{text}}, K_{\text{image}}, V_{\text{image}})
$$

This allows text queries to attend to image features and vice versa.

**Challenges in Multimodal Learning**:

- **Modality Gap**: Different modalities have different statistical properties
- **Alignment**: Temporal or spatial alignment between modalities
- **Missing Modalities**: Handling cases where some modalities are unavailable
- **Computational Cost**: Processing multiple modalities increases requirements

**Future Directions**:

The field is moving toward:

- **Unified architectures**: Single model handling all modalities
- **Emergent cross-modal understanding**: Learning relationships not explicitly trained
- **Efficient multimodal models**: Reducing computational overhead
- **New modalities**: Incorporating touch, smell, proprioception
