# C-5: Implementation and Applications

1. Building Transformer Models in PyTorch
    - Model Components Implementation
    - Attention Block Coding
    - Feedforward Networks
    - Layer Normalization
    - Training and Evaluation Process
2. Decoder-only Architecture
    - GPT-style Models
    - Causal Masking Implementation
    - Autoregressive Property
    - Generation Strategies
    - Applications and Limitations
3. Using Pre-trained Models with Hugging Face
    - Pipeline API Usage
    - Model Selection Guidelines
    - Text Generation Parameters
    - Fine-tuning Pre-trained Models
    - Multi-modal Applications

#### Building Transformer Models

Transformer models represent one of the most significant advances in deep learning for natural language processing and
beyond. In this section, we'll explore how to implement these powerful architectures from scratch using PyTorch. By
breaking down the implementation into manageable components, we'll gain a deeper understanding of how transformers work
and how each part contributes to their impressive capabilities.

##### Model Components Implementation

To build a transformer model, we need to implement several key components that work together. Let's start by
understanding the high-level structure and then dive into each component's implementation.

A transformer model consists of an encoder, a decoder, or both, depending on the specific application. Each of these
contains multiple identical layers built from the same set of components. Let's first create the basic building blocks
that we'll use to construct these layers.

First, we need to define our model's configuration parameters. These control the size and behavior of our transformer:

```python
class TransformerConfig:
    def __init__(self,
                 vocab_size=30000,
                 max_position_embeddings=512,
                 hidden_size=768,
                 num_hidden_layers=12,
                 num_attention_heads=12,
                 intermediate_size=3072,
                 hidden_dropout_prob=0.1,
                 attention_probs_dropout_prob=0.1,
                 layer_norm_eps=1e-12):
        """Initialize the transformer configuration.

        Args:
            vocab_size: Size of the vocabulary
            max_position_embeddings: Maximum sequence length
            hidden_size: Dimension of the hidden representations
            num_hidden_layers: Number of transformer layers
            num_attention_heads: Number of attention heads
            intermediate_size: Dimension of the feed-forward intermediate layer
            hidden_dropout_prob: Dropout probability for hidden layers
            attention_probs_dropout_prob: Dropout probability for attention probabilities
            layer_norm_eps: Small constant for layer normalization stability
        """
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.intermediate_size = intermediate_size
        self.hidden_dropout_prob = hidden_dropout_prob
        self.attention_probs_dropout_prob = attention_probs_dropout_prob
        self.layer_norm_eps = layer_norm_eps
```

Next, we need to implement the embedding layer that maps tokens to dense vectors and adds positional information:

```python
import torch
import torch.nn as nn
import math

class TransformerEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids):
        """Convert input token ids to embeddings with positional information.

        Args:
            input_ids: Tensor of shape [batch_size, seq_length] containing token IDs

        Returns:
            embeddings: Tensor of shape [batch_size, seq_length, hidden_size]
        """
        seq_length = input_ids.size(1)

        # Create word embeddings from token IDs
        word_embeddings = self.word_embeddings(input_ids)

        # Create position IDs (0, 1, 2, ..., seq_length-1)
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)

        # Create position embeddings
        position_embeddings = self.position_embeddings(position_ids)

        # Combine word and position embeddings
        embeddings = word_embeddings + position_embeddings

        # Apply layer normalization and dropout
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)

        return embeddings
```

The `TransformerEmbeddings` class performs several critical functions:

1. It converts token IDs to dense vector representations using a learned embedding matrix.
2. It adds positional information through learned position embeddings.
3. It normalizes the combined embeddings and applies dropout for regularization.

Rather than using fixed sinusoidal encodings as in the original paper, this implementation uses learned position
embeddings, which often work better in practice. The positional embeddings capture the sequence order information that
would otherwise be lost in the transformer's parallel processing approach.

With the embedding layer in place, we have the foundation for building our transformer. These embeddings will be fed
into the attention blocks and subsequent layers to process the input sequence.

##### Attention Block Coding

The attention mechanism is the heart of the transformer architecture. It allows the model to weigh the importance of
different parts of the input sequence when processing each element. Let's implement the multi-head attention block:

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()

        # Ensure hidden_size is divisible by num_attention_heads
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                f"The hidden size ({config.hidden_size}) is not a multiple of the number of attention "
                f"heads ({config.num_attention_heads})"
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = config.hidden_size // config.num_attention_heads
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        # Create query, key, value projections
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        # Output projection
        self.output = nn.Linear(config.hidden_size, config.hidden_size)

        # Regularization
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        """Reshape the tensor for multi-head attention calculation.

        Args:
            x: Tensor of shape [batch_size, seq_length, hidden_size]

        Returns:
            Tensor of shape [batch_size, num_heads, seq_length, head_size]
        """
        new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_shape)
        # Transpose to [batch_size, num_heads, seq_length, head_size]
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, attention_mask=None):
        """Compute multi-head attention on the input.

        Args:
            hidden_states: Tensor of shape [batch_size, seq_length, hidden_size]
            attention_mask: Optional mask of shape [batch_size, 1, 1, seq_length]
                            with ones for positions to attend to and zeros for masked positions

        Returns:
            attention_output: Tensor of shape [batch_size, seq_length, hidden_size]
        """
        # Project inputs to queries, keys, and values
        query_layer = self.query(hidden_states)
        key_layer = self.key(hidden_states)
        value_layer = self.value(hidden_states)

        # Reshape for multi-head attention
        query_layer = self.transpose_for_scores(query_layer)
        key_layer = self.transpose_for_scores(key_layer)
        value_layer = self.transpose_for_scores(value_layer)

        # Compute scaled dot-product attention
        # (batch_size, num_heads, seq_length, seq_length)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        # Apply attention mask if provided
        if attention_mask is not None:
            # Add large negative values to masked positions
            attention_scores = attention_scores + attention_mask

        # Apply softmax to get attention probabilities
        attention_probs = nn.functional.softmax(attention_scores, dim=-1)

        # Apply dropout
        attention_probs = self.dropout(attention_probs)

        # Apply attention to values
        context_layer = torch.matmul(attention_probs, value_layer)

        # Reshape back to [batch_size, seq_length, hidden_size]
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_shape)

        # Apply output projection
        attention_output = self.output(context_layer)

        return attention_output
```

This implementation of multi-head attention follows the scaled dot-product attention mechanism described in the original
transformer paper. Let's break down what's happening:

1. We first project the input hidden states into query, key, and value representations using separate linear
   transformations.
2. We reshape these projections to separate the attention heads, allowing each head to focus on different aspects of the
   input.
3. We compute attention scores by taking the dot product of queries and keys, scaling by the square root of the head
   dimension to stabilize gradients.
4. If an attention mask is provided (e.g., for causal attention in autoregressive models or to handle padding), we apply
   it to prevent attention to certain positions.
5. We apply the softmax function to get attention probabilities, followed by dropout for regularization.
6. We compute the weighted sum of values using the attention probabilities.
7. Finally, we reshape the result and project it back to the original hidden size.

The attention mask is particularly important in transformer models. For encoder-decoder attention, it masks padding
tokens. For causal self-attention in decoders, it ensures that each position can only attend to previous positions,
preventing information leakage from future tokens during training.

To make our implementation more modular, let's create a complete attention block that includes the multi-head attention
with a residual connection and layer normalization:

```python
class AttentionBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention = MultiHeadAttention(config)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, attention_mask=None):
        """Apply attention with residual connection and layer normalization.

        Args:
            hidden_states: Tensor of shape [batch_size, seq_length, hidden_size]
            attention_mask: Optional mask tensor

        Returns:
            output: Tensor of shape [batch_size, seq_length, hidden_size]
        """
        # Apply multi-head attention
        attention_output = self.attention(hidden_states, attention_mask)

        # Apply dropout, residual connection, and layer normalization
        output = self.layer_norm(hidden_states + self.dropout(attention_output))

        return output
```

This block implements the pre-norm (or pre-LN) variant of the transformer, where layer normalization is applied to the
input before the attention or feed-forward computations. This differs slightly from the original transformer paper,
which used post-normalization, but pre-normalization has been shown to improve training stability, especially for deep
transformers.

##### Feedforward Networks

The feed-forward network is the second main component of each transformer layer. It's a simple yet crucial part that
processes each position independently, applying the same transformation to each element in the sequence:

```python
class FeedForwardNetwork(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.activation = nn.GELU()  # Using GELU activation as in modern transformers
        self.dense2 = nn.Linear(config.intermediate_size, config.hidden_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states):
        """Apply feed-forward network to input.

        Args:
            hidden_states: Tensor of shape [batch_size, seq_length, hidden_size]

        Returns:
            output: Tensor of shape [batch_size, seq_length, hidden_size]
        """
        # Apply first linear transformation with activation
        intermediate_output = self.activation(self.dense1(hidden_states))

        # Apply second linear transformation
        output = self.dense2(intermediate_output)

        # Apply dropout
        output = self.dropout(output)

        return output
```

The feed-forward network consists of two linear transformations with a non-linear activation function in between. While
the original transformer paper used ReLU, modern implementations often use GELU (Gaussian Error Linear Unit) activation,
which has been found to work better in practice.

<div align="center">
<img src="images/c.png" width="600" height="auto">
<p style="color: #555;">Figure: Feedforward Block</p>
</div>

The first transformation expands the dimension (typically by a factor of 4), giving the network more capacity to process
the information. The second transformation projects back to the original hidden size, allowing the output to be combined
with the residual connection.

Now let's create a complete feed-forward block that includes the FFN with a residual connection and layer normalization:

```python
class FeedForwardBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ffn = FeedForwardNetwork(config)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states):
        """Apply feed-forward network with residual connection and layer normalization.

        Args:
            hidden_states: Tensor of shape [batch_size, seq_length, hidden_size]

        Returns:
            output: Tensor of shape [batch_size, seq_length, hidden_size]
        """
        # Apply layer normalization
        layernorm_output = self.layer_norm(hidden_states)

        # Apply feed-forward network
        ffn_output = self.ffn(layernorm_output)

        # Apply residual connection
        output = hidden_states + ffn_output

        return output
```

Similar to the attention block, we're using the pre-norm approach here, applying layer normalization before the
feed-forward computation.

With the attention and feed-forward blocks in place, we can now combine them to create a complete transformer layer:

```python
class TransformerLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention_block = AttentionBlock(config)
        self.feed_forward_block = FeedForwardBlock(config)

    def forward(self, hidden_states, attention_mask=None):
        """Apply a full transformer layer: attention followed by feed-forward.

        Args:
            hidden_states: Tensor of shape [batch_size, seq_length, hidden_size]
            attention_mask: Optional attention mask tensor

        Returns:
            output: Tensor of shape [batch_size, seq_length, hidden_size]
        """
        # Apply attention block
        attention_output = self.attention_block(hidden_states, attention_mask)

        # Apply feed-forward block
        layer_output = self.feed_forward_block(attention_output)

        return layer_output
```

This completes our implementation of a single transformer layer. To create the full transformer, we'll stack multiple
such layers together.

##### Layer Normalization

Layer normalization is a crucial component of transformer models, helping to stabilize training by normalizing the
activations within each layer. We've already included layer normalization in our attention and feed-forward blocks, but
let's understand how it works in more detail.

Layer normalization operates on each individual example independently, normalizing the activations across the feature
dimension. This is different from batch normalization, which normalizes across the batch dimension.

<div align="center">
<img src="images/layer_normalization.png" width="600" height="auto">
<p style="color: #555;">Figure: Layer Normalization</p>
</div>

The formula for layer normalization is:

$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$

Where:

- $\mu$ is the mean of the features for each example
- $\sigma$ is the standard deviation of the features for each example
- $\gamma$ and $\beta$ are learnable parameters for scaling and shifting
- $\epsilon$ is a small constant for numerical stability

In PyTorch, layer normalization is implemented in the `nn.LayerNorm` module, which we've been using in our code. It's
applied to the last dimension of the input by default, which in our case is the hidden size.

Layer normalization helps with several aspects of training deep networks:

1. It stabilizes the training process, allowing for higher learning rates and faster convergence.
2. It reduces the internal covariate shift (the change in the distribution of layer inputs due to parameter updates).
3. It helps with gradient flow through the network, mitigating vanishing or exploding gradients.

The position of layer normalization (pre-norm or post-norm) can significantly impact training dynamics. In our
implementation, we've chosen the pre-norm approach, which applies normalization before each sub-block, followed by a
residual connection that adds the sub-block output to the original input:

Pre-norm: $\text{output} = \text{input} + \text{Sublayer}(\text{LayerNorm}(\text{input}))$

This differs from the original post-norm approach:

Post-norm: $\text{output} = \text{LayerNorm}(\text{input} + \text{Sublayer}(\text{input}))$

Research has shown that pre-norm often provides better training stability, especially for deeper networks, as it keeps
the residual connection "pure" without normalization affecting it directly.

Now, let's complete our transformer model by stacking multiple layers and adding the necessary input and output
components:

```python
class TransformerEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embeddings = TransformerEmbeddings(config)
        self.layers = nn.ModuleList([TransformerLayer(config) for _ in range(config.num_hidden_layers)])
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, input_ids, attention_mask=None):
        """Apply the transformer encoder to the input.

        Args:
            input_ids: Tensor of shape [batch_size, seq_length] containing token IDs
            attention_mask: Optional mask of shape [batch_size, seq_length]
                            with 1 for tokens to attend to and 0 for tokens to ignore

        Returns:
            sequence_output: Tensor of shape [batch_size, seq_length, hidden_size]
        """
        # Convert attention mask to the format expected by the attention layers
        if attention_mask is not None:
            # Create a 4D attention mask [batch_size, 1, 1, seq_length]
            # with zeros for positions to mask out
            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            # Convert zeros to large negative values for softmax
            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        else:
            extended_attention_mask = None

        # Apply embeddings
        hidden_states = self.embeddings(input_ids)

        # Apply transformer layers
        for layer in self.layers:
            hidden_states = layer(hidden_states, extended_attention_mask)

        # Apply final layer normalization
        sequence_output = self.layer_norm(hidden_states)

        return sequence_output
```

For a language model, we would add a prediction head on top of the encoder:

```python
class TransformerForLanguageModeling(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.transformer = TransformerEncoder(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Share input and output embeddings (weight tying)
        self.lm_head.weight = self.transformer.embeddings.word_embeddings.weight

    def forward(self, input_ids, attention_mask=None, labels=None):
        """Apply transformer for language modeling.

        Args:
            input_ids: Tensor of shape [batch_size, seq_length] containing token IDs
            attention_mask: Optional mask tensor
            labels: Optional tensor of shape [batch_size, seq_length] containing target token IDs

        Returns:
            loss: Language modeling loss if labels are provided, otherwise None
            logits: Tensor of shape [batch_size, seq_length, vocab_size] containing token predictions
        """
        # Get transformer outputs
        hidden_states = self.transformer(input_ids, attention_mask)

        # Apply language modeling head
        logits = self.lm_head(hidden_states)

        # Calculate loss if labels are provided
        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            # Calculate cross-entropy loss
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        return loss, logits
```

This completes the core components of our transformer model. Note that we're using weight tying (sharing weights between
the input embeddings and the output projection), which is a common technique in language models that reduces the number
of parameters and often improves performance.

For an autoregressive (causal) language model like GPT, we would need to modify the attention masking to ensure that
tokens can only attend to previous positions:

```python
def create_causal_mask(seq_length, device):
    """Create a causal mask for autoregressive decoding.

    Args:
        seq_length: Length of the sequence
        device: Device to put the mask on

    Returns:
        mask: Tensor of shape [1, 1, seq_length, seq_length]
              with 1s in the lower triangular part and 0s elsewhere
    """
    # Create a matrix where the upper triangle (including diagonal) is 1
    mask = torch.triu(torch.ones((seq_length, seq_length), device=device), diagonal=1)

    # Convert 1s to large negative values and 0s to 0s
    mask = (mask * -10000.0).unsqueeze(0).unsqueeze(1)

    return mask
```

This causal mask would be combined with the padding mask to ensure that the model only attends to previous tokens and
ignores padding.

##### Training and Evaluation Process

Now that we have our transformer model implementation, let's focus on how to train and evaluate it effectively. Training
transformers requires careful consideration of various factors due to their complexity and memory requirements.

First, let's define a training function:

```python
def train_transformer(model, train_dataloader, optimizer, scheduler, device, num_epochs, eval_dataloader=None):
    """Train a transformer model.

    Args:
        model: The transformer model
        train_dataloader: DataLoader for training data
        optimizer: Optimizer for parameter updates
        scheduler: Learning rate scheduler
        device: Device to train on
        num_epochs: Number of training epochs
        eval_dataloader: Optional DataLoader for evaluation

    Returns:
        model: The trained model
    """
    model.train()

    for epoch in range(num_epochs):
        total_loss = 0

        for batch in train_dataloader:
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Zero gradients
            optimizer.zero_grad()

            # Forward pass
            loss, _ = model(input_ids, attention_mask=attention_mask, labels=labels)

            # Backward pass
            loss.backward()

            # Clip gradients to prevent exploding gradients
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters
            optimizer.step()

            # Update learning rate
            scheduler.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_dataloader)
        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}")

        # Evaluate if eval_dataloader is provided
        if eval_dataloader is not None:
            eval_loss = evaluate_transformer(model, eval_dataloader, device)
            print(f"Epoch {epoch+1}/{num_epochs}, Eval Loss: {eval_loss:.4f}")
            # Switch back to training mode
            model.train()

    return model

def evaluate_transformer(model, eval_dataloader, device):
    """Evaluate a transformer model.

    Args:
        model: The transformer model
        eval_dataloader: DataLoader for evaluation data
        device: Device to evaluate on

    Returns:
        avg_loss: Average loss on the evaluation data
    """
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in eval_dataloader:
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            loss, _ = model(input_ids, attention_mask=attention_mask, labels=labels)

            total_loss += loss.item()

    avg_loss = total_loss / len(eval_dataloader)
    return avg_loss
```

These functions handle the core training and evaluation loops. There are several important considerations for training
transformers effectively:

1. **Gradient accumulation**: For very large models that don't fit in memory with reasonable batch sizes, we can
   accumulate gradients over multiple forward and backward passes before updating the parameters:

```python
# Inside the training loop
accumulation_steps = 4  # Update every 4 batches
loss = loss / accumulation_steps  # Scale the loss
loss.backward()

if (batch_idx + 1) % accumulation_steps == 0:
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
```

1. **Learning rate scheduling**: Transformers benefit from careful learning rate scheduling. The original paper used a
   learning rate with a linear warmup followed by a decay proportional to the inverse square root of the step number.
   Modern implementations often use linear warmup followed by linear or cosine decay:

```python
from transformers import get_linear_schedule_with_warmup

num_warmup_steps = 1000
num_training_steps = len(train_dataloader) * num_epochs

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)
```

1. **Mixed precision training**: For further memory efficiency and speed, especially on modern GPUs, we can use mixed
   precision training with PyTorch's automatic mixed precision:

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# Inside the training loop
with autocast():
    loss, _ = model(input_ids, attention_mask=attention_mask, labels=labels)

scaler.scale(loss).backward()
scaler.unscale_(optimizer)
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
scaler.step(optimizer)
scaler.update()
scheduler.step()
```

1. **Evaluation metrics**: For language modeling, perplexity is a common evaluation metric, which is the exponentiated
   average negative log-likelihood per token:

```python
def compute_perplexity(model, dataloader, device):
    model.eval()
    total_loss = 0
    total_tokens = 0

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Get loss
            loss, _ = model(input_ids, attention_mask=attention_mask, labels=labels)

            # Count non-padding tokens
            non_padding = labels != -100  # Assuming -100 is the padding label
            num_tokens = non_padding.sum().item()

            total_loss += loss.item() * num_tokens
            total_tokens += num_tokens

    avg_loss = total_loss / total_tokens
    perplexity = math.exp(avg_loss)

    return perplexity
```

1. **Checkpointing**: Saving model checkpoints regularly is crucial, especially for long training runs:

```python
def save_checkpoint(model, optimizer, scheduler, epoch, loss, checkpoint_path):
    """Save a training checkpoint."""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'loss': loss,
    }, checkpoint_path)

def load_checkpoint(model, optimizer, scheduler, checkpoint_path):
    """Load a training checkpoint."""
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']

    return model, optimizer, scheduler, epoch, loss
```

1. **Data preparation**: Efficient data loading is essential for training transformers. We need to tokenize the text,
   create attention masks, and prepare labels:

```python
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length):
        self.encodings = tokenizer(
            texts,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        # For causal language modeling, labels are the same as inputs
        self.labels = self.encodings['input_ids'].clone()

    def __len__(self):
        return len(self.encodings['input_ids'])

    def __getitem__(self, idx):
        return {
            'input_ids': self.encodings['input_ids'][idx],
            'attention_mask': self.encodings['attention_mask'][idx],
            'labels': self.labels[idx]
        }

# Example usage
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('gpt2')
train_texts = ["This is an example text", "Another example text"]
train_dataset = TextDataset(train_texts, tokenizer, max_length=512)
train_dataloader = DataLoader(train_dataset, batch_size=4)
```

1. **Generation and inference**: Once the model is trained, we can use it to generate text:

```python
def generate_text(model, tokenizer, prompt, max_length=100, temperature=1.0, top_k=50, top_p=0.95):
    """Generate text using a trained transformer model.

    Args:
        model: The transformer model
        tokenizer: The tokenizer
        prompt: The prompt text to start generation
        max_length: Maximum length of generated text
        temperature: Sampling temperature (higher = more creative, lower = more focused)
        top_k: Number of highest probability tokens to consider for each step
        top_p: Probability threshold for nucleus sampling

    Returns:
        generated_text: The generated text including the prompt
    """
    model.eval()

    # Encode the prompt
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    input_ids = input_ids.to(next(model.parameters()).device)

    # Create attention mask
    attention_mask = torch.ones_like(input_ids)

    # Initialize generation
    generated = input_ids

    # Create causal mask for future positions
    seq_length = input_ids.size(1)
    causal_mask = create_causal_mask(seq_length, input_ids.device)

    with torch.no_grad():
        for _ in range(max_length):
            # Get model output for the current sequence
            _, logits = model(generated, attention_mask=attention_mask)

            # Get the logits for the next token (last token in the sequence)
            next_token_logits = logits[:, -1, :]

            # Apply temperature
            next_token_logits = next_token_logits / temperature

            # Apply top-k filtering
            if top_k > 0:
                # Keep only the top-k tokens with highest probability
                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
                # Create a new tensor with -inf everywhere except for top-k tokens
                next_token_logits = torch.full_like(next_token_logits, float('-inf'))
                next_token_logits.scatter_(1, top_k_indices, top_k_logits)

            # Apply top-p (nucleus) filtering
            if top_p < 1.0:
                # Sort logits and corresponding indices
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                # Calculate cumulative probabilities
                sorted_probs = torch.softmax(sorted_logits, dim=-1)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

                # Remove tokens with cumulative probability above the threshold
                sorted_indices_to_remove = cumulative_probs > top_p
                # Shift the indices to the right to keep the first token above threshold
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0

                # Create a scatter index tensor for removal
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_token_logits[:, indices_to_remove] = float('-inf')

            # Convert logits to probabilities
            probs = torch.softmax(next_token_logits, dim=-1)

            # Sample the next token
            next_token = torch.multinomial(probs, num_samples=1)

            # Append the new token to the sequence
            generated = torch.cat((generated, next_token), dim=1)

            # Update attention mask
            attention_mask = torch.cat((attention_mask, torch.ones_like(next_token)), dim=1)

            # Update causal mask
            new_seq_length = generated.size(1)
            causal_mask = create_causal_mask(new_seq_length, generated.device)

            # Check if EOS token was generated
            if next_token.item() == tokenizer.eos_token_id:
                break

    # Decode the generated text
    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)

    return generated_text
```

This text generation function implements several important techniques for controlling the output of transformer language
models:

1. **Temperature Sampling**: By dividing the logits by a temperature parameter, we control the randomness of the
   predictions. Lower temperatures (e.g., 0.7) make the model more confident, focusing on high-probability tokens,
   resulting in more predictable text. Higher temperatures (e.g., 1.2) make the distribution more uniform, increasing
   diversity but potentially reducing coherence.

   <div align="center">
   <img src="images/temp.png" width="600" height="auto">
   <p style="color: #555;">Figure: Temperature Parameter</p>
   </div>

2. **Top-K Sampling**: This technique restricts sampling to the K most likely next tokens instead of the entire
   vocabulary. This prevents the model from selecting extremely unlikely tokens while still maintaining diversity.
   Common values range from 40 to 100.

3. **Top-P (Nucleus) Sampling**: Rather than selecting from a fixed number of tokens, nucleus sampling dynamically
   selects the smallest set of tokens whose cumulative probability exceeds a threshold (top_p). This adapts better to
   different contexts where the probability distribution might be concentrated or spread out.

Combining these sampling methods provides better control over the generation process than simple greedy or beam search
approaches, especially for creative text generation.

For a complete transformer implementation, let's integrate everything into a training pipeline:

```python
def train_transformer_model(
    train_texts,
    eval_texts=None,
    model_config=None,
    tokenizer_name='gpt2',
    max_length=512,
    batch_size=8,
    num_epochs=3,
    learning_rate=5e-5,
    warmup_steps=1000,
    output_dir='./transformer_model'
):
    """Train a transformer model from scratch.

    Args:
        train_texts: List of training texts
        eval_texts: Optional list of evaluation texts
        model_config: Configuration for the transformer model
        tokenizer_name: Name of the pre-trained tokenizer to use
        max_length: Maximum sequence length
        batch_size: Batch size for training
        num_epochs: Number of training epochs
        learning_rate: Learning rate for the optimizer
        warmup_steps: Number of warmup steps for learning rate scheduling
        output_dir: Directory to save the model

    Returns:
        model: The trained transformer model
    """
    # Set up device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Set up tokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    # Use default config if none provided
    if model_config is None:
        model_config = TransformerConfig(
            vocab_size=len(tokenizer),
            max_position_embeddings=max_length,
            hidden_size=768,
            num_hidden_layers=12,
            num_attention_heads=12,
            intermediate_size=3072
        )

    # Initialize model
    model = TransformerForLanguageModeling(model_config)
    model.to(device)

    # Prepare datasets
    train_dataset = TextDataset(train_texts, tokenizer, max_length)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    if eval_texts is not None:
        eval_dataset = TextDataset(eval_texts, tokenizer, max_length)
        eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)
    else:
        eval_dataloader = None

    # Set up optimizer and scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)

    num_training_steps = len(train_dataloader) * num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=num_training_steps
    )

    # Set up gradient scaler for mixed precision training
    scaler = GradScaler()

    # Training loop
    global_step = 0
    best_eval_loss = float('inf')

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0

        for batch in tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Zero gradients
            optimizer.zero_grad()

            # Forward pass with mixed precision
            with autocast():
                loss, _ = model(input_ids, attention_mask=attention_mask, labels=labels)

            # Backward pass with gradient scaling
            scaler.scale(loss).backward()

            # Clip gradients
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters and learning rate
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            global_step += 1
            epoch_loss += loss.item()

            # Log progress every 100 steps
            if global_step % 100 == 0:
                print(f"Step {global_step}, Loss: {loss.item():.4f}")

        avg_epoch_loss = epoch_loss / len(train_dataloader)
        print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}")

        # Evaluate
        if eval_dataloader is not None:
            eval_loss = evaluate_transformer(model, eval_dataloader, device)
            print(f"Evaluation Loss: {eval_loss:.4f}")

            # Save best model
            if eval_loss < best_eval_loss:
                best_eval_loss = eval_loss
                print(f"New best evaluation loss: {best_eval_loss:.4f}")

                # Save the model
                os.makedirs(output_dir, exist_ok=True)
                model_path = os.path.join(output_dir, 'best_model.pt')
                torch.save(model.state_dict(), model_path)
                print(f"Model saved to {model_path}")

        # Save checkpoint at the end of each epoch
        checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch+1}.pt')
        save_checkpoint(model, optimizer, scheduler, epoch, avg_epoch_loss, checkpoint_path)

    # Save final model
    final_model_path = os.path.join(output_dir, 'final_model.pt')
    torch.save(model.state_dict(), final_model_path)
    print(f"Final model saved to {final_model_path}")

    return model
```

This comprehensive training pipeline brings together all the components we've developed and adds several important
features:

1. **Automatic Device Selection**: The code automatically selects GPU if available, falling back to CPU otherwise.
2. **Mixed Precision Training**: Using PyTorch's automatic mixed precision (AMP) with the GradScaler, we can train
   larger models more efficiently by performing some computations in lower precision (typically float16).
3. **Progress Tracking**: Using tqdm for progress bars and regular loss logging helps monitor training progress.
4. **Model Checkpointing**: Saving checkpoints after each epoch and the best model based on evaluation loss helps
   recover from training interruptions and ensures we keep the best performing model.
5. **Learning Rate Scheduling**: A linear warmup followed by linear decay helps stabilize early training and prevent
   overshooting later on.

For practical deployment and experimentation, we might also want to add:

```python
def fine_tune_on_custom_data(base_model_path, custom_texts, output_dir, epochs=3):
    """Fine-tune a pre-trained transformer model on custom data.

    Args:
        base_model_path: Path to the pre-trained model
        custom_texts: List of custom training texts
        output_dir: Directory to save the fine-tuned model
        epochs: Number of fine-tuning epochs

    Returns:
        model: The fine-tuned model
    """
    # Load the pre-trained model
    config = TransformerConfig()  # Adjust as needed
    model = TransformerForLanguageModeling(config)
    model.load_state_dict(torch.load(base_model_path))

    # Use a smaller learning rate for fine-tuning
    return train_transformer_model(
        train_texts=custom_texts,
        model_config=config,
        num_epochs=epochs,
        learning_rate=2e-5,
        output_dir=output_dir
    )
```

This fine-tuning function allows us to adapt a pre-trained model to specific domains or tasks, a common practice in
modern NLP.

Building transformer models from scratch in PyTorch provides several important benefits:

1. **Deep Understanding**: By implementing each component, we gain insights into how transformers work and what makes
   them effective.
2. **Customization Flexibility**: We can easily modify the architecture, add new components, or experiment with
   alternative approaches.
3. **Optimization Control**: We have full control over the training process, allowing for specialized optimizations for
   specific hardware or requirements.
4. **Educational Value**: The implementation serves as a learning tool for understanding advanced deep learning
   concepts.

In practice, most researchers and practitioners will use established libraries like Hugging Face Transformers, which
provide optimized implementations of transformer architectures. However, understanding the underlying implementation
details is invaluable for research, debugging, and innovation in this rapidly evolving field.

The transformer architecture has become the foundation of modern NLP, revolutionizing tasks from machine translation to
text generation. By implementing it from scratch, we gain a deeper appreciation for its elegant design and the
engineering challenges involved in training these powerful models effectively.

#### Decoder-only Architecture

The evolution of transformer architectures has led to specialized designs optimized for different tasks. While the
original transformer paper introduced an encoder-decoder architecture primarily for machine translation, subsequent
research has shown that simplified architectures focusing solely on the decoder component can achieve remarkable
performance on a wide range of language tasks. These decoder-only architectures have become the backbone of some of the
most powerful language models today, including GPT (Generative Pre-trained Transformer) and its descendants.



<div align="center">
<img src="images/encoder.png" width="600" height="auto">
<p style="color: #555;">Figure: Encoder-Decoder Architecture</p>
</div>



##### GPT-style Models

GPT-style models represent a radical simplification of the original transformer architecture. Instead of using both
encoder and decoder components, these models rely exclusively on stacked decoder blocks, with one critical modification:
they remove the encoder-decoder cross-attention mechanism, retaining only self-attention layers.



<div align="center">
<img src="images/m3.png" width="600" height="auto">
<p style="color: #555;">Figure: Other Model Architectures</p>
</div>

To understand the conceptual breakthrough of GPT-style models, we should first examine their core design principles:

1. **Unidirectional Attention**: Unlike BERT-style encoder models which use bidirectional attention (looking at both
   left and right context), GPT models use unidirectional or causal attention, where each token can only attend to
   itself and previous tokens. This design enforces the autoregressive property that's essential for text generation
   tasks.
2. **Deep Transformer Stacks**: GPT models typically use many layers of transformer decoder blocks, allowing them to
   learn complex patterns and relationships in language. Each successive generation of GPT models has increased this
   depth, with GPT-3 featuring 96 layers in its largest configuration.
3. **Language Modeling Objective**: These models are trained with a simple next-token prediction objective, predicting
   the next word given all previous words. This straightforward task turns out to be remarkably powerful for learning
   general language representations.
4. **Scale**: GPT models leverage extreme scale in three dimensions:
    - Model parameters (from 117M in GPT-1 to 175B in GPT-3)
    - Training data (hundreds of billions of tokens)
    - Computational resources (thousands of GPU/TPU days)

The basic architecture of a GPT-style model can be implemented as follows:

```python
class GPTModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        # Token embeddings
        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)

        # Positional embeddings
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)

        # Dropout for regularization
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

        # Transformer layers
        self.layers = nn.ModuleList([
            TransformerDecoderLayer(config) for _ in range(config.num_hidden_layers)
        ])

        # Layer normalization
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """Initialize weights with small random values and set biases to zero."""
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
            if isinstance(module, nn.Linear) and module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def get_causal_attention_mask(self, seq_length, device):
        """Create a causal attention mask to prevent attending to future tokens."""
        # Create a mask where each position can only attend to previous positions
        mask = torch.triu(torch.ones((seq_length, seq_length), device=device), diagonal=1)
        # Convert to binary mask where 1 = masked (blocked), 0 = unmasked (allowed)
        mask = (mask == 1)
        # Expand dims for batch and attention heads
        mask = mask.unsqueeze(0).unsqueeze(0)
        return mask

    def forward(self, input_ids, attention_mask=None):
        """Forward pass through the GPT model.

        Args:
            input_ids: Tensor of shape [batch_size, seq_length] with token IDs
            attention_mask: Optional mask for padding tokens (1 for tokens to attend to, 0 for padding)

        Returns:
            hidden_states: Tensor of shape [batch_size, seq_length, hidden_size]
        """
        device = input_ids.device
        batch_size, seq_length = input_ids.size()

        # Get token embeddings
        hidden_states = self.token_embeddings(input_ids)

        # Create position IDs (0, 1, 2, ..., seq_length-1)
        position_ids = torch.arange(seq_length, dtype=torch.long, device=device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)

        # Add positional embeddings
        position_embeddings = self.position_embeddings(position_ids)
        hidden_states = hidden_states + position_embeddings

        # Apply dropout
        hidden_states = self.dropout(hidden_states)

        # Get causal attention mask
        causal_mask = self.get_causal_attention_mask(seq_length, device)

        # If padding mask is provided, combine it with the causal mask
        if attention_mask is not None:
            # Convert padding mask (batch_size, seq_length) to attention mask format
            # Shape: (batch_size, 1, 1, seq_length)
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            # Convert from 0/1 to -inf/0 for softmax
            attention_mask = attention_mask.to(dtype=hidden_states.dtype)  # same dtype as embeddings
            attention_mask = (1.0 - attention_mask) * -10000.0

            # Combine masks: a position is masked if it's a padding token or if it's a future token
            # Since causal_mask is already a boolean tensor, we convert it to the same format as attention_mask
            combined_mask = attention_mask.masked_fill(causal_mask, -10000.0)
        else:
            # If no padding mask, just use the causal mask
            combined_mask = causal_mask.to(dtype=hidden_states.dtype) * -10000.0

        # Apply transformer layers
        for layer in self.layers:
            hidden_states = layer(hidden_states, combined_mask)

        # Apply final layer normalization
        hidden_states = self.layer_norm(hidden_states)

        return hidden_states
```

The `TransformerDecoderLayer` class represents a single layer in the GPT model:

```python
class TransformerDecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Self-attention
        self.self_attention = MultiHeadAttention(config)
        self.attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

        # Feed-forward network
        self.feed_forward = nn.Sequential(
            nn.Linear(config.hidden_size, config.intermediate_size),
            nn.GELU(),  # GPT-2 and GPT-3 use GELU activation
            nn.Linear(config.intermediate_size, config.hidden_size),
            nn.Dropout(config.hidden_dropout_prob),
        )
        self.ff_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states, attention_mask=None):
        """Process hidden states through a transformer decoder layer.

        Args:
            hidden_states: Tensor of shape [batch_size, seq_length, hidden_size]
            attention_mask: Attention mask to prevent attending to certain positions

        Returns:
            hidden_states: Processed hidden states
        """
        # Self-attention with residual connection and layer normalization
        # Note: Using pre-LN (layer normalization before attention) as in GPT-2/3
        residual = hidden_states
        hidden_states = self.attention_layernorm(hidden_states)
        attention_output = self.self_attention(hidden_states, attention_mask)
        hidden_states = residual + attention_output

        # Feed-forward network with residual connection and layer normalization
        residual = hidden_states
        hidden_states = self.ff_layernorm(hidden_states)
        feed_forward_output = self.feed_forward(hidden_states)
        hidden_states = residual + feed_forward_output

        return hidden_states
```

The GPT architecture uses pre-layer normalization (Pre-LN) rather than the post-layer normalization used in the original
transformer. This subtle change helps stabilize training, especially for deep networks, by ensuring that the gradients
flowing through the residual connections have a more consistent scale.

To adapt this model for language generation, we add a language modeling head on top:

```python
class GPTForCausalLM(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.transformer = GPTModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Tie weights between the input embeddings and output embeddings
        self.lm_head.weight = self.transformer.token_embeddings.weight

    def forward(self, input_ids, attention_mask=None, labels=None):
        """Forward pass for language modeling.

        Args:
            input_ids: Tensor of shape [batch_size, seq_length] with token IDs
            attention_mask: Optional mask for padding tokens
            labels: Optional tensor of target token IDs for calculating loss

        Returns:
            tuple containing:
                - loss (if labels provided)
                - logits: Tensor of shape [batch_size, seq_length, vocab_size]
        """
        transformer_outputs = self.transformer(input_ids, attention_mask=attention_mask)
        hidden_states = transformer_outputs

        # Apply language modeling head to get logits
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )

        return (loss, logits) if loss is not None else logits
```

The GPT family has evolved through several generations, each bringing significant improvements:

1. **GPT-1** (2018): Introduced the decoder-only pre-training approach for language models. With 117M parameters, it
   demonstrated the value of unsupervised pre-training followed by task-specific fine-tuning.
2. **GPT-2** (2019): Scaled up to 1.5B parameters and focused on zero-shot learning, showing that larger models can
   perform tasks without explicit fine-tuning simply through prompt engineering.
3. **GPT-3** (2020): Made a massive leap to 175B parameters, demonstrating emergent abilities including few-shot
   learning, where the model can learn from just a few examples provided in the prompt.
4. **GPT-4** (2023): While architectural details aren't fully disclosed, GPT-4 further scales the approach and
   introduces multimodal capabilities, processing both text and images.

Each iteration has maintained the same fundamental architecture while scaling up parameters, training data, and
computation. This simple scaling approach has yielded surprisingly powerful results, suggesting that decoder-only
transformer architectures contain significant untapped potential that can be realized through scale.

##### Causal Masking Implementation

The defining characteristic of GPT-style models is their causal (unidirectional) attention mechanism, which prevents
each token from attending to future tokens. This causal constraint is essential for autoregressive generation and is
implemented using attention masking.

The causal mask ensures that when processing a sequence, each position can only attend to itself and previous positions,
simulating a left-to-right reading pattern. This is critical during both training and generation, as it prevents the
model from "seeing into the future" when making predictions.

Let's explore how causal masking is implemented in detail:

1. **Creating the Causal Mask**:

For a sequence of length n, we create an nn upper triangular matrix where entries above the diagonal are 1 (masked) and
entries on or below the diagonal are 0 (unmasked). This mask is then broadcast across the batch and head dimensions to
match the shape expected by the attention mechanism.

```python
def create_causal_mask(seq_length, device):
    """Create a causal attention mask.

    Args:
        seq_length: Length of the sequence
        device: Device to place the mask on

    Returns:
        mask: Boolean tensor of shape [1, 1, seq_length, seq_length]
              where True values indicate positions to mask (future tokens)
    """
    # Create a square matrix where each row i contains a 1 in columns j where j > i
    # This ensures position i can only attend to positions j where j  i
    mask = torch.triu(torch.ones((seq_length, seq_length), device=device), diagonal=1).bool()

    # Add batch and head dimensions for broadcasting
    mask = mask.unsqueeze(0).unsqueeze(0)

    return mask
```

The resulting mask might look like this for a sequence of length 5:

```
[[0, 1, 1, 1, 1],
 [0, 0, 1, 1, 1],
 [0, 0, 0, 1, 1],
 [0, 0, 0, 0, 1],
 [0, 0, 0, 0, 0]]
```

Where 1 indicates positions that should be masked (i.e., prevented from attending).

1. **Applying the Mask in Attention Calculations**:

During the attention calculation, we need to convert this binary mask into values that will effectively remove the
masked positions from consideration. This is typically done by adding large negative values (e.g., -10000) to the
attention scores before the softmax operation, causing those positions to receive near-zero attention weights:

```python
def apply_causal_mask(attention_scores, causal_mask):
    """Apply causal mask to attention scores.

    Args:
        attention_scores: Tensor of shape [batch_size, num_heads, seq_length, seq_length]
        causal_mask: Boolean tensor of shape [1, 1, seq_length, seq_length]

    Returns:
        masked_scores: Masked attention scores
    """
    # Convert boolean mask to float and apply large negative values to masked positions
    mask_value = torch.finfo(attention_scores.dtype).min  # Most negative value possible
    # Or use a large negative constant like -10000

    # Create a float mask with -inf at masked positions and 0 elsewhere
    float_mask = torch.zeros_like(attention_scores)
    float_mask.masked_fill_(causal_mask, mask_value)

    # Add the mask to attention scores
    masked_scores = attention_scores + float_mask

    return masked_scores
```

1. **Combining with Padding Masks**:

In practice, we often need to combine causal masking with padding masking to handle variable-length sequences within a
batch. Padding masks ensure that the model doesn't attend to padding tokens:

```python
def combine_masks(causal_mask, padding_mask):
    """Combine causal and padding masks.

    Args:
        causal_mask: Boolean tensor of shape [1, 1, seq_length, seq_length]
        padding_mask: Tensor of shape [batch_size, seq_length] where 1 means
                     valid token and 0 means padding token

    Returns:
        combined_mask: Combined attention mask
    """
    # Reshape padding mask to [batch_size, 1, 1, seq_length]
    # This allows broadcasting across the causal mask
    padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)

    # Convert padding mask to attention format (1 for tokens to attend to, 0 for padding)
    # We need to generate a mask that is 1 where we can attend and 0 where we cannot
    # So we need a mask that is 0 for padding and 1 for actual tokens

    # A position is masked (can't be attended to) if:
    # 1. It's a future token (in causal_mask), OR
    # 2. It's a padding token (padding_mask is 0)

    # Create attention_padding_mask where True means "do not attend"
    attention_padding_mask = (padding_mask == 0)

    # Combine: a position is masked if it's a future token OR a padding token
    combined_mask = causal_mask | attention_padding_mask

    return combined_mask
```

1. **Integration into the Multi-Head Attention Implementation**:

Finally, the mask is integrated into the scaled dot-product attention calculation:

```python
def scaled_dot_product_attention(query, key, value, mask=None):
    """Compute scaled dot-product attention.

    Args:
        query: Tensor of shape [batch_size, num_heads, seq_length, head_dim]
        key: Tensor of shape [batch_size, num_heads, seq_length, head_dim]
        value: Tensor of shape [batch_size, num_heads, seq_length, head_dim]
        mask: Optional mask tensor of shape [batch_size, 1, seq_length, seq_length]
              or [1, 1, seq_length, seq_length]

    Returns:
        attention_output: Tensor of shape [batch_size, num_heads, seq_length, head_dim]
        attention_weights: Tensor of shape [batch_size, num_heads, seq_length, seq_length]
    """
    # Calculate attention scores
    head_dim = query.size(-1)

    # [batch_size, num_heads, seq_length, seq_length]
    attention_scores = torch.matmul(query, key.transpose(-1, -2))
    attention_scores = attention_scores / math.sqrt(head_dim)

    # Apply mask if provided
    if mask is not None:
        # If mask is boolean, convert to appropriate values for softmax
        if mask.dtype == torch.bool:
            attention_scores.masked_fill_(mask, -10000.0)
        else:
            attention_scores = attention_scores + mask

    # Apply softmax to get attention weights
    attention_weights = F.softmax(attention_scores, dim=-1)

    # Apply attention weights to values
    attention_output = torch.matmul(attention_weights, value)

    return attention_output, attention_weights
```

The causal masking ensures the autoregressive property essential for language modeling and text generation. During
training, it forces the model to predict the next token based only on previous tokens, creating a natural left-to-right
language modeling objective. During inference, it ensures that generated text doesn't depend on future tokens that
haven't been generated yet.

The implementation of causal masking in transformers has a beautiful mathematical eleganceit's a simple modification
that enforces a powerful constraint, allowing these models to learn the probability distribution of language in a way
that can be directly used for generation.

##### Autoregressive Property

The autoregressive property is fundamental to GPT-style decoder-only models, governing both how they learn language
patterns during training and how they generate text during inference. Autoregression refers to modeling sequential data
where future values depend on past values, similar to how in natural language, the choice of a word depends on the words
that came before it.

Let's examine how autoregression works in decoder-only transformers:

1. **Mathematical Foundation**:

In probabilistic terms, autoregressive models factor the joint probability of a sequence into a product of conditional
probabilities:

$p(x_1, x_2, ..., x_n) = p(x_1) \cdot p(x_2|x_1) \cdot p(x_3|x_1, x_2) \cdot ... \cdot p(x_n|x_1, x_2, ..., x_{n-1})$

For language models, this means the probability of a sequence of tokens is calculated as the product of the probability
of each token given all previous tokens.

1. **Training Process**:

During training, the model learns to predict the next token given all previous tokens in the sequence. For a sequence of
tokens $x = (x_1, x_2, ..., x_n)$, the training objective is to maximize the log-likelihood:

$\mathcal{L}(\theta) = \sum_{i=1}^{n} \log p_\theta(x_i|x_{<i})$

Where $p_\theta(x_i|x_{<i})$ is the probability the model assigns to token $x_i$ given all previous tokens $x_{<i}$.

The causal mask ensures this autoregressive property by preventing the model from seeing future tokens when predicting
the current token. This forces the model to rely solely on past context, precisely mimicking the prediction task it will
face during text generation.

1. **Inference Process**:

During inference (text generation), the autoregressive property determines how new tokens are generated:

```python
def generate_text_autoregressive(model, tokenizer, prompt, max_length=100):
    """Generate text using autoregressive sampling.

    Args:
        model: The language model
        tokenizer: Tokenizer for encoding/decoding
        prompt: Initial text prompt
        max_length: Maximum number of tokens to generate

    Returns:
        generated_text: The complete generated text
    """
    # Encode the prompt
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    # Generate tokens one by one
    for _ in range(max_length):
        # Get model predictions
        with torch.no_grad():
            outputs = model(input_ids)
            next_token_logits = outputs[:, -1, :]  # Get logits for last token

        # Sample the next token
        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)

        # Append the new token to the sequence
        input_ids = torch.cat([input_ids, next_token_id], dim=-1)

        # Check if we've generated an end-of-sequence token
        if next_token_id.item() == tokenizer.eos_token_id:
            break

    # Decode the generated tokens
    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)

    return generated_text
```

In this process:

1. We start with an initial prompt.
2. The model predicts the probability distribution for the next token.
3. We sample from this distribution to select the next token.
4. We append this token to the existing sequence.
5. We repeat steps 2-4 until we reach a stopping condition (e.g., max length or EOS token).

Each new token becomes part of the context for predicting subsequent tokens, creating a recursive process that can
generate coherent text of arbitrary length.

1. **Benefits of Autoregression**:

The autoregressive property provides several advantages:

- **Natural Text Generation**: Since natural language itself is produced sequentially (one word after another), the
  autoregressive modeling approach aligns well with how humans produce and process language.
- **Coherent Long-Form Content**: Because each prediction is conditioned on all previous tokens, the model can maintain
  consistent themes, topics, and narrative flow over long passages.
- **Controllable Generation**: The autoregressive process allows for various controlled text generation techniques, such
  as prompt engineering, where the initial prompt can steer the generation in specific directions.
- **Adaptability**: The model can adapt its generation based on what it has already produced, maintaining consistency
  with prior choices.

1. **Limitations and Challenges**:

Despite its strengths, autoregressive generation also faces some challenges:

- **Error Accumulation**: Mistakes or inaccuracies introduced early in the generation can propagate and amplify through
  the sequence, potentially leading to text that drifts off-topic or becomes nonsensical.
- **Exposure Bias**: During training, the model is exposed to gold-standard previous tokens from the training data, but
  during generation, it must rely on its own potentially imperfect predictions, creating a mismatch between training and
  inference conditions.
- **Repetition and Loops**: Autoregressive models can sometimes get stuck in repetitive patterns where they repeatedly
  generate similar phrases or sentences.
- **Computational Efficiency**: Generating each token requires a full forward pass through the model, making generation
  relatively slow compared to non-autoregressive approaches.

The autoregressive property, enforced by causal masking, is what gives GPT-style models their ability to function as
powerful text generators. While there are alternative approaches to text generation (such as non-autoregressive models
that predict multiple tokens in parallel), autoregressive models remain the dominant paradigm for high-quality natural
language generation due to their ability to maintain coherence and contextual relevance throughout the generated text.

##### Generation Strategies

The power of decoder-only transformer models lies not just in their architecture, but also in how we guide their
generation process. Different generation strategies can produce dramatically different outputs from the same model,
influencing factors like creativity, coherence, and factual accuracy. Let's explore the key strategies used to control
text generation in GPT-style models.

1. **Greedy Decoding**:

The simplest approach is to always select the token with the highest probability at each step:

```python
def greedy_decoding(model, input_ids, max_length):
    """Generate text by selecting the most probable token at each step."""
    generated = input_ids.clone()

    for _ in range(max_length):
        # Get predictions from model
        outputs = model(generated)
        next_token_logits = outputs[:, -1, :]

        # Select token with highest probability
        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

        # Append to sequence
        generated = torch.cat([generated, next_token], dim=-1)

        # Check for EOS token
        if next_token.item() == model.config.eos_token_id:
            break

    return generated
```

While greedy decoding is efficient, it often leads to repetitive and deterministic text that lacks diversity. It can't
explore alternatives that might ultimately lead to better overall sequences.

1. **Beam Search**:

Beam search expands upon greedy decoding by keeping track of multiple most likely sequence candidates at each step:



<div align="center">
<img src="images/beam.png" width="600" height="auto">
<p style="color: #555;">Figure: Beam Search</p>
</div>





```python
def beam_search(model, input_ids, beam_width=5, max_length=20):
    """Generate text using beam search to explore multiple candidate sequences."""
    # Initialize with the input sequence
    generated = input_ids.clone()

    # Start with a single beam containing the input sequence
    # Each beam contains (sequence, score)
    beams = [(generated, 0.0)]
    finished_beams = []

    for _ in range(max_length):
        new_beams = []

        # Expand each current beam
        for sequence, score in beams:
            # Skip if this beam has finished (reached EOS token)
            if sequence[0, -1].item() == model.config.eos_token_id:
                finished_beams.append((sequence, score))
                continue

            # Get predictions from model
            outputs = model(sequence)
            next_token_logits = outputs[:, -1, :]

            # Get probabilities
            next_token_probs = F.softmax(next_token_logits, dim=-1)

            # Get top-k tokens and their probabilities
            topk_probs, topk_indices = torch.topk(next_token_probs, beam_width, dim=-1)

            # Create new candidate beams
            for i in range(beam_width):
                token_id = topk_indices[0, i].unsqueeze(0).unsqueeze(0)
                token_prob = torch.log(topk_probs[0, i]).item()

                # Create new sequence by appending token
                new_sequence = torch.cat([sequence, token_id], dim=1)
                new_score = score + token_prob  # Log probabilities sum (instead of multiplying)

                new_beams.append((new_sequence, new_score))

        # Keep only the top beam_width beams
        all_beams = new_beams + finished_beams
        all_beams.sort(key=lambda x: x[1], reverse=True)  # Sort by score (descending)
        beams = [b for b in all_beams[:beam_width] if b[0][0, -1].item() != model.config.eos_token_id]
        finished_beams = [b for b in all_beams[:beam_width] if b[0][0, -1].item() == model.config.eos_token_id]

        # If all beams have finished or we have enough finished beams
        if not beams or len(finished_beams) >= beam_width:
            break

    # Return the highest-scoring completed sequence, or the highest-scoring incomplete sequence if none completed
    all_beams = finished_beams + beams
    all_beams.sort(key=lambda x: x[1], reverse=True)
    return all_beams[0][0]  # Return the highest-scoring sequence
```



Beam search aims to find a sequence with high overall probability by exploring multiple paths simultaneously. While it
often produces more fluent text than greedy decoding, it tends to favor safe, common phrases and can still generate
repetitive content. It's most useful for tasks where correctness and fluency are more important than creativity, such as
translation or summarization.

1. **Sampling-Based Approaches**:

Instead of deterministically selecting the most probable tokens, sampling-based approaches introduce randomness by
sampling from the probability distribution:

a. **Pure Sampling**:

```python
def temperature_sampling(model, input_ids, temperature=1.0, max_length=20):
    """Generate text by sampling from the probability distribution with temperature control."""
    generated = input_ids.clone()

    for _ in range(max_length):
        # Get predictions
        outputs = model(generated)
        next_token_logits = outputs[:, -1, :]

        # Apply temperature
        if temperature != 1.0:
            next_token_logits = next_token_logits / temperature

        # Convert to probabilities
        next_token_probs = F.softmax(next_token_logits, dim=-1)

        # Sample from the distribution
        next_token = torch.multinomial(next_token_probs, num_samples=1)

        # Append to sequence
        generated = torch.cat([generated, next_token], dim=-1)

        # Check for EOS token
        if next_token.item() == model.config.eos_token_id:
            break

    return generated
```

The temperature parameter controls randomnesshigher values (e.g., 1.2) make the distribution more uniform, increasing
diversity but potentially reducing coherence, while lower values (e.g., 0.7) make the distribution more peaked,
producing more focused but potentially repetitive text.

b. **Top-k Sampling**:

```python
def top_k_sampling(model, input_ids, k=40, temperature=1.0, max_length=20):
    """Generate text using top-k sampling to limit token selection to most likely options."""
    generated = input_ids.clone()

    for _ in range(max_length):
        # Get predictions
        outputs = model(generated)
        next_token_logits = outputs[:, -1, :]

        # Apply temperature
        if temperature != 1.0:
            next_token_logits = next_token_logits / temperature

        # Zero out all logits below the top k
        top_k_logits, top_k_indices = torch.topk(next_token_logits, k)

        # Create a mask of logits to keep
        next_token_logits = torch.full_like(next_token_logits, float('-inf'))
        next_token_logits.scatter_(1, top_k_indices, top_k_logits)

        # Convert to probabilities
        next_token_probs = F.softmax(next_token_logits, dim=-1)

        # Sample from the filtered distribution
        next_token = torch.multinomial(next_token_probs, num_samples=1)

        # Append to sequence
        generated = torch.cat([generated, next_token], dim=-1)

        # Check for EOS token
        if next_token.item() == model.config.eos_token_id:
            break

    return generated
```

Top-k sampling restricts the sampling to only the k most likely next tokens, preventing the selection of highly
improbable tokens that might lead to incoherent text. This provides a balance between the diversity of sampling and the
coherence of more deterministic approaches.

c. **Nucleus (Top-p) Sampling**:

```python
def nucleus_sampling(model, input_ids, p=0.9, temperature=1.0, max_length=20):
    """Generate text using nucleus sampling to dynamically select the most probable tokens."""
    generated = input_ids.clone()

    for _ in range(max_length):
        # Get predictions
        outputs = model(generated)
        next_token_logits = outputs[:, -1, :]

        # Apply temperature
        if temperature != 1.0:
            next_token_logits = next_token_logits / temperature

        # Convert to probabilities
        next_token_probs = F.softmax(next_token_logits, dim=-1)

        # Sort probabilities in descending order
        sorted_probs, sorted_indices = torch.sort(next_token_probs, descending=True)

        # Calculate cumulative probabilities
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

        # Create mask based on cumulative probability threshold p
        # We want to keep tokens whose cumulative probability is <= p
        sorted_indices_to_remove = cumulative_probs > p

        # Shift indices to remove tokens after reaching threshold
        # Keep at least the token with highest probability
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0  # Always keep the highest probability token

        # Create a mask of indices to remove
        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)

        # Zero out probabilities of tokens to remove
        next_token_probs.masked_fill_(indices_to_remove, 0.0)

        # Renormalize probabilities
        next_token_probs = next_token_probs / next_token_probs.sum(dim=-1, keepdim=True)

        # Sample from the filtered distribution
        next_token = torch.multinomial(next_token_probs, num_samples=1)

        # Append to sequence
        generated = torch.cat([generated, next_token], dim=-1)

        # Check for EOS token
        if next_token.item() == model.config.eos_token_id:
            break

    return generated
```

Nucleus sampling (also called top-p sampling) dynamically selects the smallest set of tokens whose cumulative
probability exceeds a threshold p. This adaptive approach is often more effective than top-k sampling because it adjusts
the number of candidate tokens based on the confidence of the model's predictions. When the model is very confident, it
might select from just a few tokens; when uncertain, it might consider many more options.

1. **Combined Strategies**:

In practice, modern text generation systems often combine these approaches. For example, many implementations use
nucleus sampling with temperature adjustment, optionally enhanced with additional techniques:

```python
def generate_with_combined_strategies(
    model,
    tokenizer,
    prompt,
    max_length=100,
    temperature=0.8,
    top_p=0.95,
    top_k=50,
    repetition_penalty=1.2,
    no_repeat_ngram_size=3
):
    """Generate text using a combination of strategies for improved quality."""
    # Encode prompt
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    attention_mask = torch.ones_like(input_ids)

    # Track generated tokens for repetition penalty
    generated = input_ids.clone()

    for _ in range(max_length):
        # Prepare inputs
        model_inputs = {
            'input_ids': generated,
            'attention_mask': attention_mask
        }

        # Get predictions
        with torch.no_grad():
            outputs = model(**model_inputs)
            next_token_logits = outputs[:, -1, :]

        # Apply temperature
        next_token_logits = next_token_logits / temperature

        # Apply repetition penalty
        if repetition_penalty > 1.0:
            # Clone logits before modification
            logits_to_modify = next_token_logits.clone()

            # Get list of already generated tokens
            prev_tokens = generated[0].tolist()

            # Identify unique tokens to penalize
            unique_tokens = set(prev_tokens)

            # Apply penalty - decrease logits for previously used tokens
            for token in unique_tokens:
                # Check how many times token appears in sequence
                if prev_tokens.count(token) > 0:
                    # Apply increasing penalty for repeated tokens
                    repetition_count = prev_tokens.count(token)
                    penalty = repetition_penalty ** repetition_count
                    logits_to_modify[0, token] = logits_to_modify[0, token] / penalty

            next_token_logits = logits_to_modify

        # Apply n-gram blocking for repetition
        if no_repeat_ngram_size > 0:
            # Get the list of previously generated tokens
            prev_tokens = generated[0].tolist()

            # For each n-gram length
            for n_gram_size in range(1, min(no_repeat_ngram_size + 1, len(prev_tokens))):
                # Check the most recent n-gram
                recent_ngram = prev_tokens[-(n_gram_size):]

                # Check if this n-gram has appeared before
                for i in range(len(prev_tokens) - n_gram_size + 1):
                    if prev_tokens[i:i+n_gram_size] == recent_ngram:
                        # If we found a matching n-gram, block the next token
                        if i + n_gram_size < len(prev_tokens):
                            next_token_to_block = prev_tokens[i + n_gram_size]
                            next_token_logits[0, next_token_to_block] = float('-inf')

        # Apply top-k filtering
        if top_k > 0:
            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)
            next_token_logits = torch.full_like(next_token_logits, float('-inf'))
            next_token_logits.scatter_(1, top_k_indices, top_k_logits)

        # Apply top-p (nucleus) filtering
        if top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            sorted_probs = torch.softmax(sorted_logits, dim=-1)
            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

            # Remove tokens with cumulative probability above the threshold
            sorted_indices_to_remove = cumulative_probs > top_p
            # Shift the indices to the right to keep also the first token above the threshold
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0

            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            next_token_logits.masked_fill_(indices_to_remove, float('-inf'))

        # Convert to probabilities
        probs = torch.softmax(next_token_logits, dim=-1)

        # Sample next token
        next_token = torch.multinomial(probs, num_samples=1)

        # Append to generated sequence
        generated = torch.cat([generated, next_token], dim=-1)
        attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=-1)

        # Check for EOS token
        if next_token.item() == tokenizer.eos_token_id:
            break

    # Decode the generated tokens
    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)

    return generated_text
```

This comprehensive approach combines several strategies to address common issues in text generation:

- **Temperature** controls the overall randomness of sampling.
- **Top-k filtering** limits the pool of candidate tokens to the k most likely.
- **Nucleus sampling** dynamically adjusts the candidate pool based on the probability distribution.
- **Repetition penalty** reduces the probability of repeating tokens that have appeared recently.
- **N-gram blocking** prevents the exact repetition of n-grams, helping avoid loops and repetitive patterns.

1. **Contrastive Decoding**:

A more recent innovation involves using two models of different sizes to guide generation:

```python
def contrastive_decoding(large_model, small_model, input_ids, alpha=0.1, max_length=20):
    """Generate text using contrastive decoding to leverage differences between models."""
    generated = input_ids.clone()

    for _ in range(max_length):
        # Get predictions from both models
        with torch.no_grad():
            large_outputs = large_model(generated)
            small_outputs = small_model(generated)

            large_logits = large_outputs[:, -1, :]
            small_logits = small_outputs[:, -1, :]

        # Convert to probabilities
        large_probs = F.softmax(large_logits, dim=-1)
        small_probs = F.softmax(small_logits, dim=-1)

        # Calculate contrastive distribution
        # Emphasis on tokens where large model has higher confidence than small model
        contrastive_distribution = large_probs - alpha * small_probs

        # Zero out negative values and renormalize
        contrastive_distribution = torch.max(contrastive_distribution, torch.zeros_like(contrastive_distribution))
        # Add small epsilon to ensure non-zero probabilities
        contrastive_distribution = contrastive_distribution + 1e-10
        contrastive_distribution = contrastive_distribution / contrastive_distribution.sum(dim=-1, keepdim=True)

        # Sample from the contrastive distribution
        next_token = torch.multinomial(contrastive_distribution, num_samples=1)

        # Append to sequence
        generated = torch.cat([generated, next_token], dim=-1)

        # Check for EOS token
        if next_token.item() == large_model.config.eos_token_id:
            break

    return generated
```

Contrastive decoding leverages the intuition that larger models tend to have better knowledge but also share many biases
with smaller models. By emphasizing tokens that the larger model favors more strongly than the smaller model, this
approach can produce higher quality text by filtering out common patterns that both models have learned.

Each generation strategy has its strengths and ideal use cases:

- **Greedy and beam search** work well for tasks requiring precision and determinism (e.g., translation, factual
  question answering).
- **Pure sampling with temperature** is useful for creative tasks where variety is important (e.g., story writing).
- **Top-k and nucleus sampling** strike a balance between creativity and coherence, making them popular for
  general-purpose text generation.
- **Combined approaches** with repetition penalties address common failure modes and are used in production systems.
- **Contrastive decoding** and other model-guided techniques represent the cutting edge, potentially improving quality
  by leveraging multiple models.

The optimal generation strategy depends on the specific requirements of the application, and fine-tuning these
parameters often requires experimentation to balance factors like coherence, diversity, factual accuracy, and
creativity.

##### Applications and Limitations

Decoder-only transformer models have demonstrated remarkable capabilities across a wide range of applications, but they
also face significant limitations. Understanding both their strengths and weaknesses is crucial for effectively
leveraging these powerful models while mitigating potential risks.

**Applications**

1. **Text Generation and Creative Writing**

Decoder-only models excel at generating coherent, fluent text across various styles and formats. This capability enables
applications such as:

- **Content creation**: Generating blog posts, marketing copy, product descriptions, and other commercial content.
- **Creative writing**: Assisting with fiction writing, poetry, and screenplay development.
- **Personalized communication**: Drafting emails, messages, and other communications with customized tone and style.

The creative applications of these models continue to expand as their capabilities improve. For example, some authors
now use GPT models as writing partners, generating ideas or helping to overcome writer's block.

1. **Conversational AI and Assistants**

The most visible application of decoder-only transformers is in conversational AI systems:

- **Digital assistants**: Models like GPT-4 power sophisticated AI assistants that can answer questions, provide
  recommendations, and engage in open-ended conversation.
- **Customer service**: Automated support systems that can handle complex customer inquiries and troubleshooting.
- **Therapy and counseling support**: While not replacing human therapists, some applications provide mental health
  support through conversational interfaces.

These conversational systems benefit from the models' ability to maintain context over long exchanges and generate
contextually appropriate responses.

1. **Code Generation and Programming Assistance**

Decoder-only models have shown impressive capabilities in software development:

- **Code completion**: Suggesting code snippets based on partial input or comments.
- **Bug fixing**: Identifying and proposing fixes for bugs in existing code.
- **Documentation generation**: Creating documentation from code or explanations of how code works.
- **Language translation**: Converting code between programming languages.

Tools like GitHub Copilot, built on decoder-only architectures, are transforming software development workflows and
increasing programmer productivity.

1. **Language Translation and Understanding**

While encoder-decoder architectures were initially designed for translation tasks, decoder-only models have also
demonstrated strong performance:

- **Translation**: Converting text between languages with high fluency.
- **Summarization**: Condensing long documents while preserving key information.
- **Question answering**: Extracting relevant information from context to answer specific questions.

1. **Specialized Domain Applications**

Decoder-only models are increasingly being applied to specialized domains after fine-tuning:

- **Legal document analysis and drafting**: Reviewing contracts, generating legal documents, and identifying potential
  issues.
- **Medical applications**: Helping draft clinical notes, summarize medical literature, or provide simplified
  explanations of medical concepts.
- **Scientific research**: Assisting with literature reviews, hypothesis generation, and experimental design.

**Limitations**

Despite their impressive capabilities, decoder-only transformer models face several important limitations:

1. **Factual Accuracy and Hallucinations**

One of the most significant limitations is the tendency to generate plausible-sounding but incorrect information:

- **Hallucinations**: Models can confidently generate facts that have no basis in reality or training data.
- **Outdated information**: Models only "know" information from their training data, which typically has a cutoff date.
- **Source attribution**: Models struggle to provide accurate citations or distinguish between established facts and
  speculation.

This limitation stems from the models' fundamental designthey're trained to predict likely continuations of text rather
than retrieve verified information.

```python
def answer_with_retrieval_augmentation(model, tokenizer, question, knowledge_base):
    """Generate an answer with retrieval augmentation to improve factual accuracy."""
    # Search knowledge base for relevant information
    relevant_documents = knowledge_base.search(question, top_k=3)

    # Combine retrieved information with the question
    context = "Based on the following information:\n\n"
    for i, doc in enumerate(relevant_documents):
        context += f"Document {i+1}: {doc.text}\n\n"

    prompt = context + f"Question: {question}\nAnswer:"

    # Generate answer using the model with context
    answer = generate_text(model, tokenizer, prompt)

    return answer
```

Retrieval-augmented generation, as shown above, is one approach to mitigate hallucinations by grounding generation in
verified information sources.

1. **Reasoning Limitations**

Decoder-only models often struggle with complex reasoning:

- **Multi-step reasoning**: Performance degrades on problems requiring several logical steps.
- **Mathematical reasoning**: Difficulty with complex calculations or mathematical proofs.
- **Causal reasoning**: Challenges understanding cause and effect relationships.

These limitations are being addressed through techniques like chain-of-thought prompting, which encourages the model to
break down complex problems into steps:

```python
def chain_of_thought_reasoning(model, tokenizer, problem):
    """Apply chain-of-thought prompting to improve reasoning capabilities."""
    prompt = f"""
    Problem: {problem}


    Let's think through this step by step:
    """

    solution = generate_text(model, tokenizer, prompt)
    return solution
```

1. **Contextual Limitations**

Decoder-only models face practical constraints in handling context:

- **Context window size**: Models can only process a limited number of tokens at once (e.g., 2,048 for GPT-3, 8,192 for
  GPT-4, with newer models extending further but still with finite limits).
- **Information prioritization**: Models may focus on recent context at the expense of earlier information.
- **Long-range dependencies**: Performance can degrade when reasoning requires connecting information across long
  distances in the text.

Approaches to address context limitations include:

- **Chunking and summarization**: Breaking long documents into chunks and summarizing key information.
- **Retrieval-based methods**: Dynamically retrieving relevant context based on the current state of generation.
- **Architectural innovations**: New architectures that efficiently handle longer contexts through techniques like
  sparse attention.

1. **Bias and Fairness Issues**

Models inherit biases present in their training data:

- **Social biases**: Reinforcing stereotypes related to gender, race, ethnicity, and other attributes.
- **Cultural biases**: Over-representing dominant cultural perspectives and under-representing others.
- **Toxicity**: Potentially generating harmful, offensive, or inappropriate content.

Addressing bias requires multi-faceted approaches:

- **Diverse training data**: Ensuring training data represents a wide range of perspectives and sources.
- **Alignment techniques**: Methods like RLHF (Reinforcement Learning from Human Feedback) to align model outputs with
  human values.
- **Safety filters**: Post-processing filters to detect and prevent harmful outputs.

1. **Computational and Environmental Costs**

The scale of modern decoder-only models raises sustainability concerns:

- **Training costs**: Training large models like GPT-4 requires enormous computational resources.
- **Energy consumption**: The associated energy consumption contributes to carbon emissions.
- **Deployment costs**: Running inference with large models requires significant infrastructure, limiting accessibility.

Efforts to address these costs include:

- **Model distillation**: Creating smaller models that approximate the capabilities of larger ones.
- **Quantization**: Reducing precision requirements while maintaining performance.
- **Sparse activation**: Techniques that only activate relevant parts of the model for a given input.

1. **Lack of Interpretability**

Understanding why models generate specific outputs remains challenging:

- **Black-box nature**: The decision-making process is distributed across billions of parameters.
- **Attribution difficulty**: Hard to determine which parts of the training data influenced particular outputs.
- **Confidence assessment**: Models don't reliably indicate their confidence in generated content.

Research in model interpretability aims to address these issues, but progress has been limited relative to the rapid
scaling of model capabilities.

**Future Directions**

The field continues to evolve rapidly, with several promising directions to address current limitations:

1. **Multimodal Models**: Extending decoder-only architectures to handle multiple modalities beyond text, such as
   images, audio, and video.
2. **Tool Use and Action Taking**: Enabling models to interact with external tools, APIs, and databases to overcome
   their inherent knowledge limitations.
3. **Continuous Learning**: Developing methods for models to update their knowledge without complete retraining.
4. **Alignment and Safety**: Advancing techniques to ensure models behave in ways that align with human values and
   ethical principles.
5. **Efficiency Improvements**: Creating models that maintain capabilities while reducing computational requirements.

The decoder-only transformer architecture has proven remarkably versatile and powerful, but realizing its full potential
while addressing its limitations remains an active area of research and development. As these models continue to
improve, their impact across industries and society will likely grow accordingly, highlighting the importance of
responsible development and deployment practices.

#### Using Pre-trained Models with Hugging Face

The development of transformer models represented a significant breakthrough in natural language processing, but the
computational resources and expertise required to train these models from scratch put them out of reach for many
practitioners. The Hugging Face ecosystem has democratized access to state-of-the-art models by providing a
comprehensive platform for accessing, fine-tuning, and deploying pre-trained transformer models. This framework has
become the de facto standard for working with transformer models in practice.


<div align="center">
<img src="images/ranked.png" width="600" height="auto">
<p style="color: #555;">Figure: Using Pre-trained Transformers</p>
</div>

##### Pipeline API Usage

The Pipeline API is Hugging Face's most user-friendly entry point, allowing you to perform complex NLP tasks with just a
few lines of code. This abstraction handles all the preprocessing, model inference, and postprocessing steps behind the
scenes, making powerful language models accessible even to those without deep technical expertise.

Let's explore how the Pipeline API works and how to use it effectively for various tasks:

**Basic Pipeline Structure**

At its core, a pipeline consists of three main components:

1. **Tokenizer**: Converts raw text into token IDs that the model can process
2. **Model**: The pre-trained transformer that performs the core computation
3. **Post-processor**: Converts the model's raw outputs into user-friendly results

Creating a pipeline is remarkably simple:

```python
from transformers import pipeline

# Create a text classification pipeline
classifier = pipeline("sentiment-analysis")

# Use the pipeline
result = classifier("I absolutely loved this movie!")
print(result)
# Output: [{'label': 'POSITIVE', 'score': 0.9998}]
```

In this example, the pipeline function automatically:

- Selected an appropriate pre-trained model for sentiment analysis
- Downloaded the model weights
- Instantiated the necessary tokenizer
- Set up all preprocessing and postprocessing steps

The beauty of this approach is that you don't need to worry about the underlying details to get started.

**Common Pipeline Types**

Hugging Face supports numerous task-specific pipelines:

```python
# Text generation
generator = pipeline("text-generation")
generator("Once upon a time", max_length=50)

# Question answering
qa_pipeline = pipeline("question-answering")
qa_pipeline({
    'question': 'What is the capital of France?',
    'context': 'Paris is the capital and most populous city of France.'
})

# Named entity recognition
ner_pipeline = pipeline("ner")
ner_pipeline("Hugging Face was founded in Paris.")

# Translation
translator = pipeline("translation_en_to_fr")
translator("Hugging Face is amazing!")

# Summarization
summarizer = pipeline("summarization")
summarizer("""
    America has changed dramatically during recent years. Not only has the number
    of graduates in traditional engineering disciplines such as mechanical, civil,
    electrical, chemical, and aeronautical engineering declined, but in most of the
    premier American universities engineering curricula now concentrate on and encourage
    largely the study of engineering science.
""")

# Feature extraction (embedding sentences)
feature_extractor = pipeline("feature-extraction")
features = feature_extractor("Hugging Face is a company that provides NLP tools.")
```

Each pipeline automatically selects an appropriate default model, but you can specify a particular model:

```python
# Using a specific model for sentiment analysis
classifier = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)
```

**Advanced Pipeline Usage**

Pipelines can be customized to meet specific requirements:

```python
# Batch processing for efficiency
texts = ["Hello world", "How are you?", "I love NLP"]
generator = pipeline("text-generation", batch_size=2)  # Process 2 texts at a time
results = generator(texts, max_length=20)

# Using GPU acceleration
classifier = pipeline("sentiment-analysis", device=0)  # Use first GPU

# Controlling tokenization behavior
summarizer = pipeline(
    "summarization",
    tokenizer_kwargs={"truncation": True, "max_length": 512}
)
```

The Pipeline API also allows you to handle specific tokenization requirements and control how inputs are processed:

```python
# Passing preprocessing parameters
ner = pipeline("ner", aggregation_strategy="simple")  # Merge entities
result = ner("Angela Merkel is the Chancellor of Germany")

# Controlling generation parameters
generator = pipeline("text-generation")
generator(
    "The meaning of life is",
    do_sample=True,        # Use sampling instead of greedy decoding
    temperature=0.7,       # Control randomness
    top_k=50,              # Limit to top 50 tokens
    top_p=0.95,            # Nucleus sampling threshold
    num_return_sequences=3 # Generate multiple outputs
)
```

**Creating Custom Pipelines**

For specialized tasks, you can create custom pipelines by combining Hugging Face components:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# Load specific model and tokenizer
model_name = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Create a custom pipeline
custom_classifier = pipeline(
    "sentiment-analysis",
    model=model,
    tokenizer=tokenizer
)

# Use the custom pipeline
result = custom_classifier("I absolutely loved the new Spider-Man movie!")
```

This approach gives you full control over which models and tokenizers to use while still benefiting from the pipeline's
convenient abstraction for preprocessing and postprocessing.

<div align="center">
<img src="images/hugging.png" width="600" height="auto">
<p style="color: #555;">Figure: Hugging Face Library</p>
</div>


**Building Multi-step Pipelines**

Complex workflows often require chaining multiple pipelines together:

```python
def analyze_customer_feedback(feedback):
    """Process customer feedback through multiple pipelines."""
    # First, determine the language
    language_detector = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")
    language = language_detector(feedback)[0]['label']

    # Then choose appropriate sentiment model based on language
    if language == 'en':
        sentiment_model = "distilbert-base-uncased-finetuned-sst-2-english"
    elif language == 'fr':
        sentiment_model = "nlptown/bert-base-multilingual-uncased-sentiment"
    else:
        sentiment_model = "nlptown/bert-base-multilingual-uncased-sentiment"

    # Analyze sentiment
    sentiment_analyzer = pipeline("sentiment-analysis", model=sentiment_model)
    sentiment = sentiment_analyzer(feedback)

    # Extract key topics
    summarizer = pipeline("summarization")
    topics = summarizer(feedback, max_length=30, min_length=5)

    return {
        'language': language,
        'sentiment': sentiment,
        'key_topics': topics[0]['summary_text']
    }

# Example usage
result = analyze_customer_feedback(
    "I waited over an hour for my food. The staff was apologetic but the manager was nowhere to be found."
)
```

This multi-step approach allows you to build sophisticated NLP workflows by composing simpler pipeline components.

**Pipeline Limitations and Best Practices**

While the Pipeline API is powerful and convenient, it's important to understand its limitations:

1. **Memory usage**: Pipelines load the entire model into memory, which can be significant for large models.
2. **Optimization**: For high-throughput applications, working directly with the model and tokenizer can enable more
   optimizations.
3. **Flexibility**: Some advanced use cases require more control than the pipeline abstraction provides.

Best practices for effective pipeline usage include:

- **Batch processing**: When processing multiple texts, use batching to improve throughput.
- **Right-sizing models**: Choose the smallest model that meets your accuracy requirements.
- **GPU acceleration**: For larger models, use GPU acceleration to improve performance.
- **Caching**: Consider caching results for repeated queries or implementing database lookups for common inputs.

The Pipeline API provides a perfect balance between ease of use and power, making it an ideal entry point for working
with transformer models. As your needs evolve, you can gradually move to more customized implementations while still
leveraging the Hugging Face ecosystem.

##### Model Selection Guidelines

With thousands of pre-trained models available in the Hugging Face Model Hub, selecting the right one for your specific
use case can be overwhelming. Making an informed choice requires understanding the tradeoffs between different model
architectures, sizes, training datasets, and specializations.

Let's explore a systematic approach to model selection:

**Understanding Model Architectures**

Different model architectures have distinct strengths and weaknesses:

1. **BERT and RoBERTa (encoder-only)**:
    - Strengths: Strong for classification, named entity recognition, and understanding tasks
    - Weaknesses: Not designed for text generation
    - Use when: You need to analyze or extract information from existing text
2. **GPT and OPT (decoder-only)**:
    - Strengths: Excellent for text generation, completion, and creative writing
    - Weaknesses: May struggle with tasks requiring bidirectional context
    - Use when: Your primary goal is generating coherent text or completions
3. **T5 and BART (encoder-decoder)**:
    - Strengths: Versatile for translation, summarization, and question answering
    - Weaknesses: Larger memory footprint due to separate encoder and decoder
    - Use when: You need to transform text from one form to another
4. **DistilBERT, TinyBERT, MobileBERT (distilled models)**:
    - Strengths: Faster inference, lower memory requirements
    - Weaknesses: Slightly reduced accuracy compared to larger counterparts
    - Use when: Speed and resource efficiency are priorities over maximum accuracy

**Model Size and Performance Tradeoffs**

Model size dramatically impacts both performance and resource requirements:

```python
# Comparing different sizes of BERT models
from transformers import pipeline
import time

texts = ["Your text example here"] * 100  # Create 100 examples

# Small model
small_classifier = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)
start = time.time()
small_results = small_classifier(texts)
small_time = time.time() - start

# Medium model
medium_classifier = pipeline(
    "sentiment-analysis",
    model="bert-base-uncased"
)
start = time.time()
medium_results = medium_classifier(texts)
medium_time = time.time() - start

# Large model
large_classifier = pipeline(
    "sentiment-analysis",
    model="bert-large-uncased"
)
start = time.time()
large_results = large_classifier(texts)
large_time = time.time() - start

print(f"Small model: {small_time:.2f}s")
print(f"Medium model: {medium_time:.2f}s")
print(f"Large model: {large_time:.2f}s")
```

General guidelines for model size selection:

- **Small models (< 100M parameters)**: Good for simple tasks, resource-constrained environments, or real-time
  applications
- **Medium models (100M-1B parameters)**: Good balance of performance and efficiency for most common NLP tasks
- **Large models (1B-10B parameters)**: High performance on complex or nuanced tasks
- **Very large models (>10B parameters)**: State-of-the-art performance but require significant resources

**Domain and Language Considerations**

Models trained on domain-specific data typically outperform general-purpose models on tasks within that domain:

```python
from transformers import pipeline

# General-purpose sentiment model
general_classifier = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

# Financial domain-specific model
finance_classifier = pipeline(
    "sentiment-analysis",
    model="ProsusAI/finbert"
)

# Medical domain-specific model
medical_classifier = pipeline(
    "ner",
    model="samrawal/bert-base-uncased_clinical-ner"
)

# Compare results on domain-specific text
financial_text = "The company announced a 15% increase in quarterly earnings, exceeding analyst expectations."
medical_text = "The patient presented with severe abdominal pain and elevated white blood cell count."

print("General model on financial text:", general_classifier(financial_text))
print("Finance model on financial text:", finance_classifier(financial_text))
print("Medical model on medical text:", medical_classifier(medical_text))
```

For multilingual applications, consider specialized multilingual models:

```python
# Multilingual sentiment analysis
multilingual_classifier = pipeline(
    "sentiment-analysis",
    model="nlptown/bert-base-multilingual-uncased-sentiment"
)

texts = [
    "I really enjoyed this product. Highly recommended!", # English
    "Me gust mucho este producto. Muy recomendable!",   # Spanish
    "J'ai vraiment aim ce produit. Hautement recommand!", # French
    ""                          # Chinese
]

for text in texts:
    result = multilingual_classifier(text)
    print(f"Text: {text}\nSentiment: {result}")
```

**Evaluation and Selection Process**

A systematic approach to model selection involves:

1. **Defining requirements**: Clarify task, languages, domains, and constraints (speed, memory, accuracy)
2. **Creating a candidate shortlist**: Filter Hugging Face models based on:
    - Task compatibility (via tags and model cards)
    - Architecture suitability
    - Language support
    - Domain relevance
    - Size constraints
3. **Quantitative evaluation**: Test performance on a relevant dataset:

```python
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset
import evaluate
import numpy as np

# Load evaluation dataset
dataset = load_dataset("glue", "sst2", split="validation[:100]")

# Define candidate models
candidate_models = [
    "distilbert-base-uncased-finetuned-sst-2-english",
    "textattack/roberta-base-SST-2",
    "cardiffnlp/twitter-roberta-base-sentiment"
]

# Evaluation metric
accuracy_metric = evaluate.load("accuracy")

# Evaluate each model
results = {}
for model_name in candidate_models:
    print(f"Evaluating {model_name}...")

    # Create pipeline
    classifier = pipeline("sentiment-analysis", model=model_name)

    # Tokenizer and model for getting raw logits if needed
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)

    # Map model outputs to expected labels
    id2label = model.config.id2label

    # Process dataset
    predictions = []
    for example in dataset:
        result = classifier(example["sentence"])[0]

        # Map the output label to dataset format (0 for negative, 1 for positive)
        # This mapping may differ depending on the model's output format
        pred_label = 1 if result["label"] in ["POSITIVE", "LABEL_1", "4 stars", "5 stars"] else 0
        predictions.append(pred_label)

    # Calculate accuracy
    accuracy = accuracy_metric.compute(predictions=predictions, references=dataset["label"])

    # Measure inference time
    import time
    start_time = time.time()
    _ = classifier(dataset["sentence"][:10])
    inference_time = (time.time() - start_time) / 10

    results[model_name] = {
        "accuracy": accuracy["accuracy"],
        "inference_time": inference_time
    }

# Display results
for model_name, metrics in results.items():
    print(f"\nModel: {model_name}")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"Avg. inference time: {metrics['inference_time']:.4f} seconds")
```

1. **Qualitative evaluation**: Manually review model outputs for quality, biases, and edge cases:

```python
# Qualitative evaluation with challenging examples
challenging_examples = [
    "The movie wasn't bad at all.",  # Negation
    "The acting was good, but the plot was terrible.",  # Mixed sentiment
    "This film is a masterpiece of awfulness.",  # Sarcasm
    "It's not the best I've seen, but it's decent enough."  # Nuanced
]

for model_name in candidate_models:
    classifier = pipeline("sentiment-analysis", model=model_name)
    print(f"\nModel: {model_name}")
    for example in challenging_examples:
        result = classifier(example)
        print(f"Text: '{example}'")
        print(f"Prediction: {result}\n")
```

1. **Resource measurement**: Evaluate memory usage and computational requirements:

```python
import torch
import psutil
import os
from transformers import AutoModelForSequenceClassification

def measure_model_resources(model_name):
    """Measure memory and compute resources for a model."""
    # Record starting memory
    process = psutil.Process(os.getpid())
    start_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB

    # Load model
    model = AutoModelForSequenceClassification.from_pretrained(model_name)

    # Record memory after loading
    end_memory = process.memory_info().rss / 1024 / 1024
    memory_used = end_memory - start_memory

    # Count parameters
    param_count = sum(p.numel() for p in model.parameters())

    # Estimate disk space
    torch.save(model.state_dict(), "temp_model.pt")
    disk_space = os.path.getsize("temp_model.pt") / 1024 / 1024  # MB
    os.remove("temp_model.pt")

    return {
        "memory_mb": memory_used,
        "parameters": param_count,
        "disk_space_mb": disk_space
    }

# Measure resources for candidate models
for model_name in candidate_models:
    print(f"\nMeasuring resources for {model_name}")
    resources = measure_model_resources(model_name)
    print(f"Memory usage: {resources['memory_mb']:.2f} MB")
    print(f"Parameter count: {resources['parameters']:,}")
    print(f"Model size on disk: {resources['disk_space_mb']:.2f} MB")
```

**Practical Model Selection Examples**

Let's walk through practical examples of model selection for common use cases:

1. **Customer Service Chatbot**:
    - Requirements: Multilingual, fast response time, general domain knowledge
    - Good choice: distilbert-base-multilingual-cased for understanding, GPT-2 small for generation
    - Rationale: Small, efficient models for real-time response, multilingual support for diverse customers
2. **Medical Document Classification**:
    - Requirements: High accuracy, domain-specific understanding, English only
    - Good choice: BioBERT or ClinicalBERT
    - Rationale: Domain-specific pre-training on biomedical literature, higher accuracy for specialized terminology
3. **Content Moderation**:
    - Requirements: High recall for toxic content, multiple languages, fast processing
    - Good choice: Detoxify or ToxicBERT models
    - Rationale: Specifically trained on toxic content detection, optimized for high recall

The ideal model selection process balances:

- Task performance (accuracy, F1 score, etc.)
- Inference speed requirements
- Memory and computational constraints
- Specific domain and language needs
- Ethical considerations and bias mitigation

By systematically evaluating these factors, you can select models that provide the optimal balance of performance and
efficiency for your specific use case.

##### Text Generation Parameters

Text generation with transformer models involves much more than simply calling a model with a prompt. The generation
process is highly configurable, with numerous parameters that can significantly impact the quality, diversity, and
characteristics of the generated text. Understanding these parameters is essential for fine-tuning the generation
process to suit specific applications.

Let's explore the key parameters and how they affect text generation:

**Core Generation Parameters**

1. **Temperature** controls the randomness of predictions by scaling the logits before applying softmax:

```python
from transformers import pipeline

generator = pipeline('text-generation', model='gpt2')

# Low temperature (more focused/deterministic)
focused_text = generator(
    "The meaning of life is",
    max_length=50,
    temperature=0.3
)

# Medium temperature (balanced)
balanced_text = generator(
    "The meaning of life is",
    max_length=50,
    temperature=0.7
)

# High temperature (more random/creative)
creative_text = generator(
    "The meaning of life is",
    max_length=50,
    temperature=1.2
)

print("Focused (temp=0.3):", focused_text[0]['generated_text'])
print("\nBalanced (temp=0.7):", balanced_text[0]['generated_text'])
print("\nCreative (temp=1.2):", creative_text[0]['generated_text'])
```

Temperature values typically range from 0.1 to 1.5:

- Low values (0.1-0.5): More focused, deterministic outputs; good for factual generation
- Medium values (0.6-0.8): Balanced creativity and coherence; good for most general use cases
- High values (0.9-1.5): More diverse, unexpected outputs; good for creative applications

1. **Top-k sampling** limits token selection to the k most likely next tokens:

```python
# Very restrictive top-k
restrictive = generator(
    "Once upon a time, there was a",
    max_length=50,
    do_sample=True,
    top_k=5
)

# Moderate top-k
moderate = generator(
    "Once upon a time, there was a",
    max_length=50,
    do_sample=True,
    top_k=40
)

# No top-k filtering
unrestricted = generator(
    "Once upon a time, there was a",
    max_length=50,
    do_sample=True,
    top_k=0  # Disable top-k filtering
)

print("Restrictive (top_k=5):", restrictive[0]['generated_text'])
print("\nModerate (top_k=40):", moderate[0]['generated_text'])
print("\nUnrestricted (top_k=0):", unrestricted[0]['generated_text'])
```

The top_k parameter is typically set between 20-50, with:

- Lower values creating more focused but potentially repetitive text
- Higher values allowing more diversity but risking incoherence
- A value of 0 disables top-k filtering entirely

1. **Top-p (nucleus) sampling** dynamically limits token selection to the smallest set of tokens whose cumulative
   probability exceeds p:

```python
# Conservative top-p
conservative = generator(
    "The scientist discovered a new",
    max_length=50,
    do_sample=True,
    top_p=0.5  # Only consider tokens in the top 50% of probability mass
)

# Balanced top-p
balanced = generator(
    "The scientist discovered a new",
    max_length=50,
    do_sample=True,
    top_p=0.9  # Consider tokens in the top 90% of probability mass
)

# No top-p filtering
unrestricted = generator(
    "The scientist discovered a new",
    max_length=50,
    do_sample=True,
    top_p=1.0  # Consider all tokens
)

print("Conservative (top_p=0.5):", conservative[0]['generated_text'])
print("\nBalanced (top_p=0.9):", balanced[0]['generated_text'])
print("\nUnrestricted (top_p=1.0):", unrestricted[0]['generated_text'])
```

Common top_p values range from 0.7 to 0.95:

- Lower values (0.5-0.7): More focused on highly probable tokens; good for factual generation
- Medium values (0.8-0.9): Good balance for general text generation
- Higher values (0.95-1.0): Includes more low-probability tokens; increases diversity

1. **Max length** and **min length** control the generation length:

```python
# Short generation
short = generator(
    "Summarize climate change:",
    max_length=30,
    min_length=20
)

# Medium generation
medium = generator(
    "Summarize climate change:",
    max_length=100,
    min_length=50
)

# Long generation
long = generator(
    "Summarize climate change:",
    max_length=200,
    min_length=150
)

print("Short:", short[0]['generated_text'])
print("\nMedium:", medium[0]['generated_text'])
print("\nLong:", long[0]['generated_text'])
```

Length parameters are measured in tokens (not words or characters), which typically correspond to subword units.

**Repetition Control Parameters**

Controlling repetition is crucial for natural-sounding text generation:

1. **Repetition penalty** reduces the probability of tokens that have already appeared in the generated text:

```python
# No repetition penalty
no_penalty = generator(
    "The cat sat on the",
    max_length=100,
    repetition_penalty=1.0  # No penalty (default)
)

# Moderate repetition penalty
moderate_penalty = generator(
    "The cat sat on the",
    max_length=100,
    repetition_penalty=1.2  # Moderate penalty
)

# Strong repetition penalty
strong_penalty = generator(
    "The cat sat on the",
    max_length=100,
    repetition_penalty=1.5  # Strong penalty
)

print("No penalty:", no_penalty[0]['generated_text'])
print("\nModerate penalty:", moderate_penalty[0]['generated_text'])
print("\nStrong penalty:", strong_penalty[0]['generated_text'])
```

Repetition penalty values typically range from 1.0 (no penalty) to 2.0 (severe penalty):

- Values around 1.1-1.3 provide a subtle improvement in text quality
- Values above 1.5 strongly discourage repetition but may impact fluency

1. **No-repeat n-gram size** prevents any n-gram from appearing twice in the generated text:

```python
# Allow all repetitions
allow_rep = generator(
    "The issue with this approach is that it can lead to",
    max_length=100,
    no_repeat_ngram_size=0  # Disable n-gram blocking
)

# Prevent duplicate 2-grams
block_2grams = generator(
    "The issue with this approach is that it can lead to",
    max_length=100,
    no_repeat_ngram_size=2  # Block repeated 2-grams
)

# Prevent duplicate 3-grams
block_3grams = generator(
    "The issue with this approach is that it can lead to",
    max_length=100,
    no_repeat_ngram_size=3  # Block repeated 3-grams
)

print("Allow all repetitions:", allow_rep[0]['generated_text'])
print("\nBlock 2-grams:", block_2grams[0]['generated_text'])
print("\nBlock 3-grams:", block_3grams[0]['generated_text'])
```

Typical values for no_repeat_ngram_size:

- 0: Disables n-gram blocking
- 2-3: Good for most text generation tasks
- 4-5: Strict prevention of phrase repetition; may affect fluency

**Decoding Strategies**

Different decoding strategies determine the fundamental approach to token selection:

1. **Greedy decoding** always selects the most probable next token:

```python
greedy_text = generator(
    "The future of artificial intelligence is",
    max_length=50,
    do_sample=False  # Use greedy decoding
)
```

1. **Sampling** selects tokens probabilistically based on their predicted probabilities:

```python
sampled_text = generator(
    "The future of artificial intelligence is",
    max_length=50,
    do_sample=True,  # Use sampling
    temperature=0.7  # with temperature control
)
```

1. **Beam search** maintains multiple candidate sequences and selects the one with highest overall probability:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Encode input
input_text = "The solution to climate change requires"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Generate with beam search
beam_outputs = model.generate(
    input_ids,
    max_length=50,
    num_beams=5,           # Number of beams
    no_repeat_ngram_size=2,
    early_stopping=True    # Stop when all beams reach EOS
)

# Decode
beam_text = tokenizer.decode(beam_outputs[0], skip_special_tokens=True)
print("Beam search output:", beam_text)
```

**Advanced Generation Parameters**

For more control over the generation process:

1. **Number of return sequences** generates multiple alternative completions:

```python
multiple_outputs = generator(
    "The key to happiness is",
    max_length=50,
    do_sample=True,
    num_return_sequences=3  # Generate 3 different completions
)

for i, output in enumerate(multiple_outputs):
    print(f"Completion {i+1}: {output['generated_text']}")
```

1. **Diversification** in beam search encourages diversity among different beams:

```python
diverse_outputs = model.generate(
    input_ids,
    max_length=50,
    num_beams=5,
    num_beam_groups=5,      # Group beams for diversity penalty
    diversity_penalty=1.0,  # Penalty for similar tokens across groups
    num_return_sequences=5
)

for i, output in enumerate(diverse_outputs):
    diverse_text = tokenizer.decode(output, skip_special_tokens=True)
    print(f"Diverse output {i+1}: {diverse_text}")
```

1. **Length penalty** influences the model's preference for shorter or longer outputs in beam search:

```python
# Prefer shorter completions
short_outputs = model.generate(
    input_ids,
    max_length=100,
    num_beams=4,
    length_penalty=0.5  # Values < 1.0 prefer shorter sequences
)

# Prefer longer completions
long_outputs = model.generate(
    input_ids,
    max_length=100,
    num_beams=4,
    length_penalty=2.0  # Values > 1.0 prefer longer sequences
)
```

**Parameter Selection Guidelines**

Selecting the right combination of parameters depends on your specific task:

1. **Creative writing or storytelling**:
    - Higher temperature (0.8-1.0)
    - Moderate top-p (0.9-0.95)
    - Moderate repetition penalty (1.1-1.2)
    - Use sampling rather than beam search
2. **Factual or technical content**:
    - Lower temperature (0.3-0.7)
    - Lower top-p (0.7-0.85)
    - Higher repetition penalty (1.2-1.5)
    - Consider beam search with 4-5 beams
3. **Dialogue or conversational generation**:
    - Balanced temperature (0.7-0.8)
    - Moderate top-p (0.9)
    - Moderate repetition control (no_repeat_ngram_size=3)
    - Consider sampling for more natural responses
4. **Code generation**:
    - Lower temperature (0.2-0.5)
    - Conservative top-p (0.8)
    - Minimal repetition penalty (1.0-1.1)
    - Beam search works well for structured output

**Practical Example: Optimizing for Different Goals**

Let's implement a systematic approach to find optimal generation parameters:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "gpt2-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

prompt = "The researchers discovered that the new treatment"

# Function to generate text with specific parameters
def generate_with_params(params):
    """Generate text using specified parameters and return the result."""
    inputs = tokenizer(prompt, return_tensors="pt")

    generation_config = {
        "max_length": 100,
        "do_sample": params.get("do_sample", True),
        **{k: v for k, v in params.items() if k != "do_sample"}
    }

    with torch.no_grad():
        output_sequences = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            **generation_config
        )

    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)
    return generated_text

# Test different parameter configurations for different goals

# 1. Factual, consistent text
factual_params = {
    "temperature": 0.4,
    "top_p": 0.80,
    "top_k": 40,
    "repetition_penalty": 1.2,
    "no_repeat_ngram_size": 3,
    "do_sample": True
}

# 2. Creative, diverse text
creative_params = {
    "temperature": 1.0,
    "top_p": 0.95,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "no_repeat_ngram_size": 2,
    "do_sample": True
}

# 3. Balanced, general-purpose text
balanced_params = {
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "no_repeat_ngram_size": 3,
    "do_sample": True
}

# 4. Deterministic, structured text
deterministic_params = {
    "num_beams": 5,
    "early_stopping": True,
    "no_repeat_ngram_size": 2,
    "do_sample": False  # Use beam search
}

# Generate and compare outputs
factual_text = generate_with_params(factual_params)
creative_text = generate_with_params(creative_params)
balanced_text = generate_with_params(balanced_params)
deterministic_text = generate_with_params(deterministic_params)

print(f"--- PROMPT: {prompt} ---\n")
print(f"FACTUAL:\n{factual_text}\n\n")
print(f"CREATIVE:\n{creative_text}\n\n")
print(f"BALANCED:\n{balanced_text}\n\n")
print(f"DETERMINISTIC:\n{deterministic_text}\n\n")
```

Understanding these parameters and their interactions allows you to fine-tune text generation for specific applications,
finding the optimal balance between coherence, diversity, and quality for your particular use case. Experimentation is
key, as the ideal settings often vary depending on the model, domain, and specific requirements of your application.

##### Fine-tuning Pre-trained Models

Pre-trained transformer models provide an excellent starting point for most NLP tasks, but fine-tuning them on
domain-specific data can substantially improve their performance for particular applications. Fine-tuning adapts the
general knowledge captured in pre-training to the specific patterns, vocabulary, and style of your target domain.

Let's explore the process of fine-tuning pre-trained models with Hugging Face, from data preparation to deployment:

**Understanding Fine-tuning Fundamentals**

Fine-tuning builds upon a pre-trained model by training it for additional steps on a target dataset, typically with a
task-specific objective. This process preserves the general knowledge from pre-training while adapting the model to new
domains or tasks.

The key advantages of fine-tuning include:

1. **Efficiency**: Requires far less data and computational resources than training from scratch
2. **Performance**: Typically achieves better results than using either pre-trained models directly or training
   task-specific models from scratch
3. **Adaptability**: Allows customization of models for specific domains, languages, or applications

**Data Preparation for Fine-tuning**

Preparing high-quality data is crucial for successful fine-tuning:

```python
from datasets import load_dataset, Dataset
import pandas as pd

# Option 1: Load an existing dataset from Hugging Face Hub
squad_dataset = load_dataset("squad_v2", split="train[:1000]")

# Option 2: Create a dataset from your own data (CSV example)
df = pd.read_csv("your_custom_data.csv")
custom_dataset = Dataset.from_pandas(df)

# Option 3: Create a dataset from text files
from datasets import load_dataset
text_dataset = load_dataset("text", data_files={"train": "train.txt", "validation": "validation.txt"})

# Examine the dataset structure
print(custom_dataset[0])
```

Most fine-tuning tasks require task-specific data formatting. For example, text classification:

```python
import pandas as pd
from datasets import Dataset

# Sample sentiment analysis data
data = {
    "text": [
        "I absolutely loved this product! Best purchase ever.",
        "Decent quality but overpriced for what you get.",
        "Terrible experience, broke after one week. Avoid!",
        "Works as expected, good value for the price."
    ],
    "label": [1, 0, 0, 1]  # 1 for positive, 0 for negative
}

# Create DataFrame and convert to Hugging Face Dataset
df = pd.DataFrame(data)
dataset = Dataset.from_pandas(df)

# Split into train and validation
dataset = dataset.train_test_split(test_size=0.2)
```

For more complex tasks like language modeling, the dataset needs further processing:

```python
from transformers import AutoTokenizer

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Define tokenization function
def tokenize_function(examples):
    """Tokenize text examples for causal language modeling."""
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length",
        return_tensors="pt"
    )

# Apply tokenization to dataset
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]  # Remove original text field
)

# Format for language modeling (for GPT-style models)
def prepare_for_language_modeling(examples):
    """Prepare inputs and labels for causal language modeling."""
    examples["labels"] = examples["input_ids"].clone()
    return examples

lm_dataset = tokenized_dataset.map(
    prepare_for_language_modeling,
    batched=True
)
```

**Fine-tuning Process**

With your data prepared, you can fine-tune a pre-trained model using the Trainer API:

```python
from transformers import (
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
import evaluate
import numpy as np

# Load pre-trained model
model_checkpoint = "distilbert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(
    model_checkpoint,
    num_labels=2  # Binary classification
)

# Define evaluation metric
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    """Calculate accuracy for evaluation."""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",            # Output directory
    learning_rate=5e-5,                # Learning rate
    per_device_train_batch_size=16,    # Batch size per GPU/CPU for training
    per_device_eval_batch_size=64,     # Batch size for evaluation
    num_train_epochs=3,                # Number of training epochs
    weight_decay=0.01,                 # Strength of weight decay
    evaluation_strategy="epoch",       # Evaluate after each epoch
    save_strategy="epoch",             # Save after each epoch
    load_best_model_at_end=True,       # Load the best model at the end of training
    push_to_hub=False,                 # Whether to upload the model to the Hub
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    compute_metrics=compute_metrics,
)

# Fine-tune the model
trainer.train()

# Save the model
model_path = "./fine-tuned-sentiment-model"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)
```

For language model fine-tuning, you need a slightly different approach:

```python
from transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling

# Load pre-trained model
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Prepare data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Not using masked language modeling (for GPT-style models)
)

# Training arguments for language model fine-tuning
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    warmup_steps=500,
    learning_rate=5e-5,
    fp16=True,  # Use mixed precision training
    logging_dir="./logs",
    logging_steps=100,
    num_train_epochs=1,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
)

# Fine-tune the model
trainer.train()
```

**Efficient Fine-tuning Techniques**

For larger models or resource-constrained environments, several techniques can make fine-tuning more efficient:

1. **Parameter-Efficient Fine-tuning (PEFT)** methods update only a small subset of parameters:

```python
from transformers import AutoModelForSequenceClassification
from peft import get_peft_model, LoraConfig, TaskType

# Load base model
model_name = "roberta-base"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Configure LoRA
peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=16,                          # Rank of LoRA matrices
    lora_alpha=32,                 # LoRA scaling factor
    lora_dropout=0.1,              # Dropout for LoRA layers
    target_modules=["query", "key", "value"]  # Which modules to apply LoRA to
)

# Create PEFT model
peft_model = get_peft_model(model, peft_config)

# Count trainable parameters
trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in peft_model.parameters())
print(f"Trainable parameters: {trainable_params} ({trainable_params/total_params:.2%} of total)")

# Fine-tune as normal using the Trainer with peft_model
```

2. **Gradient Accumulation** allows training with larger effective batch sizes:

```python
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,  # Accumulate gradients across 4 batches
    # This gives an effective batch size of 16
    # Other arguments as before...
)
```

3. **Mixed Precision Training** reduces memory usage and speeds up training:

```python
training_args = TrainingArguments(
    output_dir="./results",
    fp16=True,  # Enable mixed precision training
    # Other arguments as before...
)
```

**Fine-tuning for Specific Tasks**

Different tasks require different model types and fine-tuning approaches:

1. **Text Classification**:

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-cased",
    num_labels=num_classes
)

# Fine-tune as shown earlier
```

2. **Token Classification (NER)**:

```python
from transformers import AutoModelForTokenClassification
import torch

# Prepare NER dataset (using IOB format)
ner_data = {
    "tokens": [
        ["John", "Smith", "lives", "in", "New", "York", "."],
        ["Apple", "is", "headquartered", "in", "Cupertino", "."]
    ],
    "tags": [
        ["B-PER", "I-PER", "O", "O", "B-LOC", "I-LOC", "O"],
        ["B-ORG", "O", "O", "O", "B-LOC", "O"]
    ]
}

# Convert tags to IDs
tag2id = {"O": 0, "B-PER": 1, "I-PER": 2, "B-ORG": 3, "I-ORG": 4, "B-LOC": 5, "I-LOC": 6}
id2tag = {id: tag for tag, id in tag2id.items()}

# Load and fine-tune model
model = AutoModelForTokenClassification.from_pretrained(
    "bert-base-cased",
    num_labels=len(tag2id)
)
```

3. **Question Answering**:

```python
from transformers import AutoModelForQuestionAnswering

# Prepare SQuAD-like dataset
qa_data = {
    "context": ["Paris is the capital of France.", "Berlin is the capital of Germany."],
    "question": ["What is the capital of France?", "What is the capital of Germany?"],
    "answer_text": ["Paris", "Berlin"],
    "answer_start": [0, 0]  # Character position where answer starts in context
}

# Load and fine-tune model
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")
```

4. **Summarization**:

```python
from transformers import AutoModelForSeq2SeqLM

# Prepare summarization dataset
summarization_data = {
    "document": ["Long document text here...", "Another long document..."],
    "summary": ["Short summary here.", "Another summary."]
}

# Load and fine-tune model
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
```

**Evaluating Fine-tuned Models**

Thorough evaluation is essential to ensure your fine-tuned model meets performance requirements:

```python
from transformers import pipeline
import evaluate

# Load metrics
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")
precision = evaluate.load("precision")
recall = evaluate.load("recall")

# Create pipeline with fine-tuned model
classifier = pipeline("sentiment-analysis", model=model_path, tokenizer=model_path)

# Prepare test data
test_data = [
    "This product exceeded all my expectations!",
    "Not worth the money, very disappointed.",
    "Average performance, nothing special."
]
test_labels = [1, 0, 0]  # Ground truth

# Make predictions
predictions = classifier(test_data)
predicted_labels = [1 if pred["label"] == "POSITIVE" else 0 for pred in predictions]

# Calculate metrics
results = {
    "accuracy": accuracy.compute(predictions=predicted_labels, references=test_labels),
    "f1": f1.compute(predictions=predicted_labels, references=test_labels),
    "precision": precision.compute(predictions=predicted_labels, references=test_labels),
    "recall": recall.compute(predictions=predicted_labels, references=test_labels)
}

print("Evaluation results:", results)

# Analyze errors
for text, true_label, pred_label, prediction in zip(test_data, test_labels, predicted_labels, predictions):
    if true_label != pred_label:
        print(f"ERROR ON: {text}")
        print(f"True: {true_label}, Predicted: {pred_label} ({prediction['score']:.4f})")
```

**Sharing and Deploying Fine-tuned Models**

After fine-tuning, you can share or deploy your model in several ways:

1. **Save locally**:

```python
model.save_pretrained("./my-fine-tuned-model")
tokenizer.save_pretrained("./my-fine-tuned-model")
```

2. **Push to Hugging Face Hub**:

```python
from huggingface_hub import notebook_login

# Login to Hugging Face
notebook_login()

# Push model to Hub
model.push_to_hub("username/model-name")
tokenizer.push_to_hub("username/model-name")

# Or use the Trainer's push_to_hub method
trainer.push_to_hub()
```

3. **Export for production**:

```python
# Convert to ONNX format for faster inference
from transformers import export

# Export ONNX model
export(
    tokenizer=tokenizer,
    model=model,
    opset=13,
    output=Path("./model.onnx")
)
```

Fine-tuning pre-trained models allows you to leverage the power of state-of-the-art transformers while adapting them to
your specific domain and task. Whether you're improving sentiment analysis for customer feedback, creating a specialized
question-answering system, or adapting a language model to generate domain-specific text, fine-tuning provides a
practical and efficient path to high-performance NLP systems.

##### Multi-modal Applications

While transformers were initially designed for text processing, their architecture has proven remarkably adaptable to
other modalities such as images, audio, and video. Multi-modal applications that combine multiple types of data
represent one of the most exciting frontiers in AI development, enabling systems that can understand and generate
content across different sensory domains.



<div align="center">
<img src="images/multinormal.png" width="600" height="auto">
<p style="color: #555;">Figure: Multinomial Distribution</p>
</div>

Let's explore how to leverage Hugging Face's tools for building multi-modal applications:

**Vision-Language Models**

Vision-language models like CLIP (Contrastive Language-Image Pre-training) learn joint representations of images and
text, enabling powerful cross-modal applications:

```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import requests

# Load CLIP model and processor
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Load image
image_url = "https://farm3.staticflickr.com/2674/4058976312_b5d9b33dfc_z.jpg"
image = Image.open(requests.get(image_url, stream=True).raw)

# Prepare text candidates
texts = ["a photo of a dog", "a photo of a cat", "a photo of a building", "a photo of a landscape"]

# Process inputs
inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)

# Get similarity scores
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)

# Print results
for text, prob in zip(texts, probs[0].tolist()):
    print(f"{text}: {prob:.4f}")
```

CLIP can be used for a wide range of vision-language tasks:

1. **Zero-shot image classification**:

```python
from transformers import pipeline

# Create zero-shot image classification pipeline
classifier = pipeline("zero-shot-image-classification", model="openai/clip-vit-large-patch14")

# Define image and candidate labels
image_path = "path/to/image.jpg"
candidate_labels = ["a photograph of a forest", "a photograph of a beach", "a photograph of a city"]

# Classify image
results = classifier(image_path, candidate_labels)
for result in results:
    print(f"{result['label']}: {result['score']:.4f}")
```

1. **Image search by text query**:

```python
import torch
from PIL import Image
import os

def create_image_search_engine(model, processor, image_folder):
    """Create a simple image search engine using CLIP."""
    # Load and process all images
    images = []
    image_paths = []
    image_features = []

    # Process all images in the folder
    for filename in os.listdir(image_folder):
        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            image_path = os.path.join(image_folder, filename)
            image = Image.open(image_path)
            images.append(image)
            image_paths.append(image_path)

    # Batch process images to get features
    for i in range(0, len(images), 32):  # Process in batches of 32
        batch = images[i:i+32]
        inputs = processor(images=batch, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = model.get_image_features(**inputs)

        # Normalize features
        outputs = outputs / outputs.norm(dim=1, keepdim=True)
        image_features.append(outputs)

    # Concatenate all features
    if len(image_features) > 1:
        image_features = torch.cat(image_features)
    else:
        image_features = image_features[0]

    # Return search function
    def search(query, top_k=5):
        # Process query
        inputs = processor(text=[query], return_tensors="pt", padding=True)
        with torch.no_grad():
            text_features = model.get_text_features(**inputs)

        # Normalize features
        text_features = text_features / text_features.norm(dim=1, keepdim=True)

        # Calculate similarities
        similarities = (100.0 * image_features @ text_features.T).squeeze(1)

        # Get top matches
        values, indices = similarities.topk(min(top_k, len(images)))

        # Return results
        results = []
        for value, idx in zip(values, indices):
            results.append({
                "image_path": image_paths[idx],
                "score": value.item(),
                "image": images[idx]
            })

        return results

    return search

# Create search engine
search_engine = create_image_search_engine(model, processor, "path/to/image/folder")

# Search for images
results = search_engine("a cat playing with a ball")
for res in results:
    print(f"Score: {res['score']:.2f}, Path: {res['image_path']}")
    # Display image
    res['image'].show()
```

**Image-to-Text Generation Models**

Image captioning models like ViT-GPT2 combine a vision transformer encoder with a language model decoder to generate
textual descriptions of images:

```python
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import torch
from PIL import Image

# Load model and processors
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
image_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# Load image
image = Image.open("path/to/image.jpg")

# Prepare image
pixel_values = image_processor(images=image, return_tensors="pt").pixel_values

# Generate caption
output_ids = model.generate(
    pixel_values,
    max_length=16,
    num_beams=4,
    return_dict_in_generate=True,
    early_stopping=True
)

# Decode caption
caption = tokenizer.decode(output_ids.sequences[0], skip_special_tokens=True)
print(f"Generated caption: {caption}")
```

**Audio-Text Models**

Hugging Face also supports audio models for speech recognition, classification, and more:

```python
from transformers import pipeline
import librosa

# Load Automatic Speech Recognition pipeline
asr = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")

# Process audio file
audio_file = "path/to/audio.wav"
waveform, rate = librosa.load(audio_file, sr=16000)
transcription = asr(waveform)

print(f"Transcription: {transcription['text']}")
```

For more advanced audio processing:

```python
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch
import librosa
import numpy as np

# Load model and processor
model_name = "facebook/wav2vec2-large-960h-lv60-self"
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name)

# Process audio
audio_file = "path/to/audio.wav"
speech_array, sampling_rate = librosa.load(audio_file, sr=16000)
inputs = processor(speech_array, sampling_rate=16000, return_tensors="pt", padding=True)

with torch.no_grad():
    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits

# Get predicted IDs
predicted_ids = torch.argmax(logits, dim=-1)

# Decode to text
transcription = processor.batch_decode(predicted_ids)
print(f"Transcription: {transcription[0]}")
```

**Building Multi-modal Applications**

Let's combine multiple modalities to create more sophisticated applications:

1. **Visual Question Answering (VQA)**:

```python
from transformers import ViltProcessor, ViltForQuestionAnswering
from PIL import Image

# Load model and processor
model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")

# Prepare inputs
image = Image.open("path/to/image.jpg")
question = "What is shown in the image?"

# Process inputs
inputs = processor(image, question, return_tensors="pt")

# Get answer
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# Get predicted answers
predicted_idx = logits.argmax(-1).item()
answer = model.config.id2label[predicted_idx]
print(f"Question: {question}")
print(f"Answer: {answer}")
```

2. **Image-guided Text Generation**:

```python
from transformers import BlipProcessor, BlipForConditionalGeneration

# Load model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

# Load image
image = Image.open("path/to/image.jpg")

# Unconditional image captioning
inputs = processor(image, return_tensors="pt")
output = model.generate(**inputs, max_length=30)
caption = processor.decode(output[0], skip_special_tokens=True)
print(f"Caption: {caption}")

# Conditional image captioning (with a prompt)
text = "a picture of"
inputs = processor(image, text, return_tensors="pt")
output = model.generate(**inputs, max_length=30)
conditional_caption = processor.decode(output[0], skip_special_tokens=True)
print(f"Conditional caption: {conditional_caption}")
```

3. **Multi-modal Search Engine**:

```python
def multimodal_search(text_query, image=None, top_k=5):
    """Search a database using both text and image inputs."""
    # Get text embedding
    text_inputs = text_processor(text_query, return_tensors="pt", padding=True)
    with torch.no_grad():
        text_features = text_model(**text_inputs).pooler_output

    # If image is provided, get image embedding and combine
    if image is not None:
        image_inputs = image_processor(images=image, return_tensors="pt")
        with torch.no_grad():
            image_features = image_model(**image_inputs).pooler_output

        # Combine text and image features (e.g., average them)
        combined_features = (text_features + image_features) / 2
    else:
        combined_features = text_features

    # Normalize features
    combined_features = combined_features / combined_features.norm(dim=1, keepdim=True)

    # Calculate similarity with database items
    # (Assuming database_features are precomputed and normalized)
    similarities = (100.0 * database_features @ combined_features.T).squeeze(1)

    # Get top matches
    values, indices = similarities.topk(min(top_k, len(database_features)))

    # Return results
    results = []
    for value, idx in zip(values, indices):
        results.append({
            "item_id": database_ids[idx],
            "score": value.item()
        })

    return results
```

4. **Speech-to-Image Generation**:

```python
import torch
from transformers import pipeline, AutoProcessor, AutoModelForCausalLM
from diffusers import StableDiffusionPipeline
import librosa

# Load speech recognition pipeline
asr = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")

# Load text-to-image pipeline
text_to_image = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)
text_to_image.to("cuda")

def speech_to_image(audio_file, num_inference_steps=50):
    """Generate an image from spoken description."""
    # Transcribe speech to text
    waveform, rate = librosa.load(audio_file, sr=16000)
    transcription = asr(waveform)
    text = transcription["text"]
    print(f"Transcribed text: {text}")

    # Generate image from text
    image = text_to_image(text, num_inference_steps=num_inference_steps).images[0]

    return image, text

# Generate image from speech
image, text = speech_to_image("path/to/audio_description.wav")
image.show()
```

**Challenges and Best Practices in Multi-modal Applications**

Working with multi-modal data introduces specific challenges:

1. **Alignment between modalities**: Different modalities can have semantic gaps that need to be bridged.
2. **Processing efficiency**: Multi-modal models often require significant computational resources. Consider:
    - Using smaller specialized models when possible
    - Implementing batching and caching strategies
    - Pre-computing embeddings for static content
3. **Evaluation complexity**: Multi-modal systems require evaluation metrics that account for all modalities.
4. **Bias and fairness**: Multi-modal models can inherit and amplify biases from pre-training data across modalities.

Best practices for multi-modal applications include:

1. **Start simple**: Begin with pre-trained models and simple pipelines before building complex systems.
2. **Leverage specialized models**: Use models specifically trained for multi-modal tasks rather than combining separate
   models when possible.
3. **Focus on the user experience**: Design interactions that feel natural for the specific modalities involved.
4. **Test extensively**: Multi-modal systems can fail in unexpected ways when modalities interact.
5. **Consider accessibility**: Ensure your application is usable by people with different sensory abilities.

The field of multi-modal AI is advancing rapidly, with new models continually emerging that further bridge the gap
between different forms of data. By combining Hugging Face's accessible implementation of these models with thoughtful
application design, developers can create rich, intuitive experiences that leverage the full spectrum of human
communication modalities.













