# C-2: Activation Functions and Classification

1. Sigmoid Functions and Activation Functions
    - Sigmoid Function Properties and Derivatives
    - ReLU and its Variants
    - Hyperbolic Tangent (tanh)
    - Softmax for Multi-Class Problems
    - Activation Function Selection Criteria
2. Multi-Class Classification
    - One-Hot Encoding Implementation
    - Softmax Function Application
    - Multi-Class Loss Functions
    - Decision Boundaries in Higher Dimensions
    - Performance Evaluation Metrics
3. Logistic Regression
    - Mathematical Formulation
    - Probabilistic Interpretation
    - Binary Classification Implementation
    - Multi-Class Extension
    - Gradient Calculation Process

#### Sigmoid Functions and Activation Functions

Activation functions are crucial components in neural networks that introduce non-linearity, allowing these networks to
learn complex patterns and relationships in data. Without activation functions, even multi-layer neural networks would
only be capable of learning linear relationships, severely limiting their expressive power and practical utility.

##### Sigmoid Function Properties and Derivatives

The sigmoid function, also known as the logistic function, is one of the earliest and most historically significant
activation functions in neural networks. It maps any input value to an output between 0 and 1, creating an S-shaped
curve.

Mathematically, the sigmoid function is defined as:

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

The sigmoid function possesses several important properties:

1. **Bounded output**: The function outputs values strictly between 0 and 1, making it useful for representing
   probabilities.
2. **Smoothness**: The sigmoid is continuously differentiable, which is essential for gradient-based optimization.
3. **Monotonicity**: The function is strictly increasing, meaning larger inputs always produce larger outputs.
4. **Symmetry**: The sigmoid function has point symmetry around the point (0, 0.5).

The derivative of the sigmoid function has a particularly elegant form, which can be derived using the quotient rule:

$\sigma'(x) = \frac{d}{dx}\left(\frac{1}{1+e^{-x}}\right)$

$= \frac{e^{-x}}{(1+e^{-x})^2}$

$= \frac{1}{1+e^{-x}} \cdot \frac{e^{-x}}{1+e^{-x}}$

$= \frac{1}{1+e^{-x}} \cdot \left(1-\frac{1}{1+e^{-x}}\right)$

$= \sigma(x)(1-\sigma(x))$

This elegant form of the derivative makes sigmoid computationally efficient during backpropagation. However, it also
reveals a significant limitation: when $x$ has a large absolute value, $\sigma(x)$ approaches either 0 or 1, causing the
derivative $\sigma'(x)$ to approach 0. This leads to the vanishing gradient problem, where the error signal diminishes
as it propagates backward through the network, making training deep networks challenging.

Despite its limitations, the sigmoid function is still used in specific contexts, such as the output layer of binary
classification models or in certain recurrent neural network architectures.

##### ReLU and its Variants

The Rectified Linear Unit (ReLU) has become the most widely used activation function in modern deep learning, largely
replacing sigmoid and tanh functions in most architectures. It addresses many of the problems associated with sigmoid
and tanh while being computationally efficient.

The ReLU function is defined simply as:

$$\text{ReLU}(x) = \max(0, x)$$

This piecewise linear function returns the input directly if it's positive, and zero otherwise. Despite its simplicity,
ReLU has profound benefits:

1. **Computational efficiency**: ReLU involves only a simple threshold operation, making it much faster to compute than
   sigmoid or tanh.
2. **Sparsity promotion**: By mapping negative values to zero, ReLU creates sparse activations (approximately 50% of
   neurons are not activated), which can be beneficial for representational efficiency.
3. **Reduced vanishing gradient problem**: For positive inputs, the derivative of ReLU is 1, allowing gradients to flow
   without attenuation.

The derivative of ReLU is:

$$\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \ 0 & \text{if } x < 0 \ \text{undefined} & \text{if } x = 0 \end{cases}$$

In practice, the derivative at $x = 0$ is typically defined as either 0 or 1.

Despite its advantages, ReLU has a significant limitation known as the "dying ReLU problem." If a neuron's weights are
updated such that the weighted sum becomes consistently negative, the neuron will always output zero, and its weights
will no longer be updated during backpropagation. This effectively "kills" the neuron, reducing model capacity.

To address this issue, several ReLU variants have been developed:

1. **Leaky ReLU**: Allows a small gradient when the input is negative.
   $$\text{LeakyReLU}(x) = \begin{cases} x & \text{if } x > 0 \ \alpha x & \text{if } x \leq 0 \end{cases}$$ where
   $\alpha$ is a small constant, typically 0.01.
2. **Parametric ReLU (PReLU)**: Similar to Leaky ReLU, but the negative slope $\alpha$ is a learnable parameter.
3. **Exponential Linear Unit (ELU)**:
   $$\text{ELU}(x) = \begin{cases} x & \text{if } x > 0 \ \alpha(e^x - 1) & \text{if } x \leq 0 \end{cases}$$ ELU can
   produce negative outputs but has a bounded value for negative inputs, reducing the impact of saturating neurons.
4. **Scaled Exponential Linear Unit (SELU)**: A self-normalizing variant designed to maintain a mean of 0 and variance
   of 1 across layers.
5. **Gaussian Error Linear Unit (GELU)**: Approximates $x \cdot \Phi(x)$ where $\Phi(x)$ is the standard normal
   cumulative distribution function, combining properties of ReLU and dropout.

These variants aim to preserve the benefits of ReLU while mitigating its limitations, with the choice among them often
depending on the specific application and network architecture.

##### Hyperbolic Tangent (tanh)

The hyperbolic tangent (tanh) function is another sigmoidal activation function that maps inputs to outputs in the range
(-1, 1). It's defined as:

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

The tanh function shares several properties with the sigmoid function:

1. **Bounded output**: tanh outputs values strictly between -1 and 1.
2. **Smoothness**: Like sigmoid, tanh is continuously differentiable.
3. **Monotonicity**: tanh is strictly increasing.
4. **Symmetry**: Unlike sigmoid, tanh is odd-symmetric around the origin, meaning $\tanh(-x) = -\tanh(x)$.

The derivative of tanh can be expressed in terms of the function itself:

$$\tanh'(x) = 1 - \tanh^2(x)$$

This derivative also suffers from the vanishing gradient problem for inputs with large absolute values, but tanh has two
advantages over the sigmoid function:

1. **Zero-centered outputs**: The range (-1, 1) means that the average activation can be closer to zero, which helps
   with the learning dynamics of the network.
2. **Stronger gradients**: The derivative of tanh reaches a maximum value of 1 (at x = 0), whereas the maximum
   derivative of the sigmoid is 0.25. This generally allows for faster learning.

Historically, tanh was often preferred over sigmoid in hidden layers due to these advantages. In modern deep learning,
however, ReLU and its variants have largely supplanted tanh, except in specific architectures like certain recurrent
neural networks (e.g., LSTMs and GRUs) where tanh is still commonly used for internal state transitions.

The relationship between sigmoid and tanh is direct: $\tanh(x) = 2\sigma(2x) - 1$. This shows that tanh is essentially a
rescaled version of the sigmoid, stretched to range from -1 to 1 instead of 0 to 1.

##### Softmax for Multi-Class Problems

While sigmoid and tanh are useful for binary problems or individual neuron activations, multi-class classification
requires a different approach. The softmax function extends the concept of the sigmoid to multiple classes by converting
a vector of real numbers into a probability distribution.

For an input vector $\mathbf{z} = (z_1, z_2, ..., z_K)$, the softmax function is defined as:

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

The softmax function has several important properties:

1. **Normalization**: The outputs sum to 1, forming a valid probability distribution.
2. **Positive outputs**: All outputs are strictly positive, regardless of the input values.
3. **Preserve ordering**: Larger input values correspond to larger output probabilities.
4. **Differentiability**: Softmax is differentiable, making it suitable for gradient-based optimization.

The derivative of the softmax function with respect to its inputs is:

$$\frac{\partial \text{softmax}(z_i)}{\partial z_j} = \begin{cases} \text{softmax}(z_i)(1 - \text{softmax}(z_i)) & \text{if } i = j \ -\text{softmax}(z_i)\text{softmax}(z_j) & \text{if } i \neq j \end{cases}$$

This cross-dependency in the derivatives means that changing one input affects all outputs, reflecting the constraint
that the outputs must sum to 1.

Softmax is typically used in the output layer of neural networks for multi-class classification problems. When combined
with cross-entropy loss, it provides a natural framework for learning class probabilities:

$$L = -\sum_{i=1}^{n} \sum_{j=1}^{K} y_{ij} \log(\text{softmax}(z_{ij}))$$

where $y_{ij}$ is 1 if example $i$ belongs to class $j$ and 0 otherwise.

An important computational consideration with softmax is numerical stability. Direct computation can lead to overflow
when exponentiating large values. A common trick is to subtract the maximum value from all elements before
exponentiation:

$$\text{softmax}(z_i) = \frac{e^{z_i - \max_j z_j}}{\sum_{j=1}^{K} e^{z_j - \max_j z_j}}$$

This produces identical results but avoids numerical issues.

##### Activation Function Selection Criteria

Choosing the appropriate activation function is a critical decision in neural network design that impacts training
dynamics, model performance, and computational efficiency. Several criteria guide this selection:

1. **Nature of the problem**:
    - Binary classification output layers often use sigmoid
    - Multi-class classification output layers typically use softmax
    - Regression output layers generally use linear (no activation)
    - Hidden layers in modern networks predominantly use ReLU or variants
2. **Gradient flow considerations**:
    - Avoid functions susceptible to vanishing gradients (like sigmoid/tanh) in deep networks
    - Functions with strong, consistent gradients (like ReLU) facilitate training in deep architectures
    - Consider whether zero-centered outputs are beneficial (favoring tanh over sigmoid)
3. **Computational efficiency**:
    - ReLU requires fewer computational resources than sigmoid or tanh
    - Hardware-optimized implementations may favor certain functions
    - Memory constraints might influence the choice (e.g., ELU requires storing exponential computations)
4. **Network depth**:
    - Very deep networks typically perform better with ReLU and its variants
    - Residual networks often use ReLU successfully
    - Self-normalizing networks may benefit from SELU
5. **Regularization effects**:
    - ReLU's sparsity can have a regularizing effect
    - Leaky variants reduce sparsity but improve gradient flow
    - Some activation functions (like swish) have slight regularization properties
6. **Domain-specific considerations**:
    - Recurrent architectures often use tanh and sigmoid for gating mechanisms
    - CNNs typically perform well with ReLU
    - Transformers often use GELU or similar functions
7. **Empirical performance**:
    - Research suggests that ReLU variants like Leaky ReLU, PReLU, and ELU can outperform standard ReLU in some tasks
    - GELU has shown strong performance in transformer architectures
    - Ultimately, empirical testing on validation data often determines the best function for a specific problem

Modern research continues to develop and refine activation functions. Recent innovations include:

- **Swish**: $f(x) = x \cdot \sigma(\beta x)$ where $\beta$ is a trainable parameter or fixed constant
- **Mish**: $f(x) = x \cdot \tanh(\ln(1 + e^x))$
- **GELU**: $f(x) = x \cdot \Phi(x)$ where $\Phi$ is the standard normal CDF

These newer functions often aim to combine the benefits of existing activations while addressing their limitations. The
field continues to evolve as researchers develop activation functions with better theoretical properties and empirical
performance.

In practice, a good default approach is to start with ReLU for hidden layers due to its simplicity and effectiveness,
then experiment with variants if issues arise during training. For output layers, the activation function should match
the task requirements: linear for regression, sigmoid for binary classification, and softmax for multi-class
classification.

#### Multi-Class Classification

Multi-class classification extends beyond binary problems to scenarios where we need to categorize data into three or
more distinct classes. This fundamental task appears in numerous applications, from image recognition (identifying
different objects) to natural language processing (categorizing text documents) to medical diagnostics (classifying
diseases based on symptoms).

##### One-Hot Encoding Implementation

One-hot encoding transforms categorical variables into a format suitable for machine learning algorithms by representing
each category as a binary vector. This representation is crucial for multi-class classification tasks because it creates
a standardized way to represent categorical outcomes.

The process of one-hot encoding involves creating a vector of length equal to the number of categories, where only one
element is "hot" (set to 1) corresponding to the category, and all other elements are "cold" (set to 0).

For example, if we have categories [A, B, C], the one-hot encoding would be:

- A → [1, 0, 0]
- B → [0, 1, 0]
- C → [0, 0, 1]

Mathematically, for a category $c$ from a set of $K$ possible categories, the one-hot encoded vector $y$ is defined as:

$$y_i = \begin{cases} 1 & \text{if } i = c \ 0 & \text{otherwise} \end{cases}$$

for $i = 1, 2, ..., K$.

One-hot encoding has several important properties that make it valuable for multi-class classification:

1. **Equidistance**: Each encoding is equidistant from all others in Euclidean space, ensuring no implicit ordering or
   hierarchy among categories.
2. **Orthogonality**: One-hot vectors are orthogonal to each other, meaning their dot product is zero, which helps
   learning algorithms distinguish between classes.
3. **Compatibility**: This representation works naturally with the softmax activation function and cross-entropy loss
   used in neural networks.

Implementation approaches include:

1. **Manual implementation**:

```python
def one_hot_encode(category, num_categories):
    encoding = [0] * num_categories
    encoding[category] = 1
    return encoding
```

1. **Using libraries like scikit-learn**:

```python
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform([[0], [1], [2]])
```

1. **Using NumPy or TensorFlow functions**:

```python
import numpy as np
np.eye(3)[category_indices]
```

Despite its advantages, one-hot encoding can lead to high-dimensional, sparse vectors when dealing with many categories.
This dimensionality issue is sometimes addressed using techniques like feature hashing or embedding layers in neural
networks.

##### Softmax Function Application

The softmax function serves as the cornerstone of multi-class classification in neural networks by transforming raw
model outputs (logits) into a probability distribution over multiple classes. This allows us to interpret the network's
outputs as class probabilities, which is essential for classification tasks.

The softmax function takes a vector of real numbers $\mathbf{z} = (z_1, z_2, ..., z_K)$ and transforms it into a
probability distribution $\mathbf{p} = (p_1, p_2, ..., p_K)$ where:

$$p_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

The function ensures that:

- All outputs are positive: $p_i > 0$ for all $i$
- The sum of all outputs equals 1: $\sum_{i=1}^{K} p_i = 1$
- Larger input values receive higher probabilities

In a neural network context, softmax is typically applied to the output layer for multi-class classification. The
network produces logits (unnormalized scores) for each class, and softmax converts these into probabilities.

The application process in a neural network follows these steps:

1. **Forward pass through network layers**: The network processes input data through its layers, producing a vector of
   logits $\mathbf{z}$.
2. **Apply softmax transformation**: The logits are passed through the softmax function to obtain class probabilities
   $\mathbf{p}$.
3. **Prediction**: The class with the highest probability is selected as the prediction:
   $\hat{y} = \text{argmax}_i(p_i)$.
4. **Loss calculation**: During training, these probabilities are compared with the true labels (typically one-hot
   encoded) using a loss function like cross-entropy.

A key implementation detail for softmax is ensuring numerical stability. Direct computation can lead to numerical
overflow due to the exponential function. A common stabilization technique is to subtract the maximum logit value before
exponentiation:

$$p_i = \frac{e^{z_i - \max_j z_j}}{\sum_{j=1}^{K} e^{z_j - \max_j z_j}}$$

This produces identical probability distributions while avoiding numerical issues.

In practice, many deep learning frameworks combine softmax with cross-entropy loss in a single operation for both
efficiency and numerical stability. For example, in PyTorch:

```python
# Separate softmax and cross-entropy
probabilities = torch.softmax(logits, dim=1)
loss = nn.CrossEntropyLoss()(probabilities, labels)

# Combined operation (more efficient and stable)
loss = nn.CrossEntropyLoss()(logits, labels)  # Applies softmax internally
```

##### Multi-Class Loss Functions

Loss functions for multi-class classification measure the discrepancy between predicted probabilities and actual class
labels, providing the optimization signal that guides the learning process. Several loss functions are commonly used for
multi-class problems, with categorical cross-entropy being the most prevalent.

**Categorical Cross-Entropy (Multi-class Cross-Entropy)**

This is the standard loss function for multi-class classification with softmax outputs. It measures the dissimilarity
between the predicted probability distribution and the true distribution (represented as one-hot encoded vectors).

For a single sample, categorical cross-entropy is defined as:

$$L_{CE} = -\sum_{i=1}^{K} y_i \log(p_i)$$

Where:

- $K$ is the number of classes
- $y_i$ is the true label (1 for the correct class, 0 for others)
- $p_i$ is the predicted probability for class $i$

For a dataset of $N$ samples, the loss is averaged:

$$L_{CE} = -\frac{1}{N} \sum_{j=1}^{N} \sum_{i=1}^{K} y_{ji} \log(p_{ji})$$

Since $y$ is typically one-hot encoded, only one term in the inner sum is non-zero, simplifying to:

$$L_{CE} = -\frac{1}{N} \sum_{j=1}^{N} \log(p_{j,c_j})$$

where $c_j$ is the correct class for sample $j$.

The gradient of this loss with respect to the logits has a simple form:

$$\frac{\partial L_{CE}}{\partial z_i} = p_i - y_i$$

This elegant gradient makes categorical cross-entropy particularly effective for training neural networks.

**Sparse Categorical Cross-Entropy**

This is functionally equivalent to categorical cross-entropy but takes class indices rather than one-hot encoded vectors
as labels. This saves memory when dealing with many classes:

$$L_{SCE} = -\frac{1}{N} \sum_{j=1}^{N} \log(p_{j,c_j})$$

where $c_j$ is the index of the correct class for sample $j$.

**Kullback-Leibler Divergence**

KL divergence measures how one probability distribution diverges from a second, expected probability distribution:

$$L_{KL} = \sum_{i=1}^{K} y_i \log\left(\frac{y_i}{p_i}\right)$$

When $y$ is one-hot encoded, KL divergence is equivalent to categorical cross-entropy plus a constant.

**Focal Loss**

Focal loss addresses class imbalance by down-weighting easy examples and focusing training on hard examples:

$$L_{FL} = -\sum_{i=1}^{K} \alpha_i y_i (1-p_i)^\gamma \log(p_i)$$

where $\alpha_i$ is a weighting factor for class $i$ and $\gamma$ is a focusing parameter that reduces the relative loss
for well-classified examples.

**Label Smoothing**

This technique regularizes the model by replacing hard 0/1 labels with soft targets:

$$y'_i = \begin{cases} 1-\epsilon+\epsilon/K & \text{if } i \text{ is the correct class} \ \epsilon/K & \text{otherwise} \end{cases}$$

where $\epsilon$ is the smoothing parameter. This prevents the model from becoming too confident and improves
generalization.

##### Decision Boundaries in Higher Dimensions

Decision boundaries in multi-class classification define the regions in feature space where the predicted class changes.
In binary classification, this boundary is a single hypersurface, but multi-class classification involves multiple
boundaries that partition the feature space into regions corresponding to different classes.

For a neural network with softmax output, the decision boundary between classes $i$ and $j$ is defined by the points
where their probabilities are equal:

$$p_i = p_j$$

Substituting the softmax formula:

$$\frac{e^{z_i}}{\sum_{k=1}^{K} e^{z_k}} = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}$$

Which simplifies to:

$$z_i = z_j$$

For a neural network, each logit $z_i$ is typically computed as a linear function of the final hidden layer output
$\mathbf{h}$:

$$z_i = \mathbf{w}_i \cdot \mathbf{h} + b_i$$

Thus, the decision boundary between classes $i$ and $j$ is given by:

$$\mathbf{w}_i \cdot \mathbf{h} + b_i = \mathbf{w}_j \cdot \mathbf{h} + b_j$$
$$(\mathbf{w}_i - \mathbf{w}_j) \cdot \mathbf{h} + (b_i - b_j) = 0$$

This is the equation of a hyperplane in the space of the final hidden layer outputs. The orientation of this hyperplane
is determined by the difference in weight vectors $(\mathbf{w}_i - \mathbf{w}_j)$, and its position is determined by the
difference in biases $(b_i - b_j)$.

For a $K$-class problem, there are potentially $\binom{K}{2} = \frac{K(K-1)}{2}$ such hyperplanes, although not all may
be relevant depending on the geometry of the problem.

The complexity of these decision boundaries depends on the network architecture:

1. **Linear model**: Produces linear decision boundaries directly in the input space.
2. **Single hidden layer**: Can create non-linear decision boundaries but with limited complexity.
3. **Deep networks**: Can form highly complex, non-linear decision boundaries that adapt to the data distribution.

The "decision regions" are the areas in feature space where a particular class has the highest probability. These
regions meet at the decision boundaries and collectively cover the entire feature space. Visualizing these boundaries
helps understand the model's behavior, though direct visualization becomes challenging beyond three dimensions.

Techniques for visualizing high-dimensional decision boundaries include:

1. **Dimension reduction**: Using methods like PCA or t-SNE to project data to 2D or 3D for visualization.
2. **Pairwise boundary plots**: Showing decision boundaries between pairs of features while holding others constant.
3. **Decision boundary meshes**: Creating a grid in feature space and coloring points according to predicted class.

Understanding these decision boundaries is crucial for interpreting model behavior, identifying potential biases, and
diagnosing classification errors.

##### Performance Evaluation Metrics

Proper evaluation metrics are essential for assessing the performance of multi-class classification models, especially
when class distributions are imbalanced or when different types of errors have varying consequences. Several metrics are
commonly used, each providing different insights into model performance.

**Accuracy**

Accuracy measures the proportion of correctly classified instances across all classes:

$$\text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}$$

While intuitive, accuracy can be misleading with imbalanced classes. A model that always predicts the majority class
might achieve high accuracy despite failing completely on minority classes.

**Confusion Matrix**

The confusion matrix provides a comprehensive view of classification performance by showing:

- The number of true positives (TP)
- False positives (FP)
- True negatives (TN)
- False negatives (FN) for each class

For a $K$-class problem, the confusion matrix is a $K \times K$ table where element $(i,j)$ represents the count of
instances from true class $i$ predicted as class $j$.

**Precision, Recall, and F1 Score**

These metrics can be calculated for each class in a one-vs-rest manner:

For class $i$:

- **Precision**: The proportion of positive predictions that are correct.
  $$\text{Precision}_i = \frac{\text{TP}_i}{\text{TP}_i + \text{FP}_i}$$
- **Recall (Sensitivity)**: The proportion of actual positives that are correctly identified.
  $$\text{Recall}_i = \frac{\text{TP}_i}{\text{TP}_i + \text{FN}_i}$$
- **F1 Score**: The harmonic mean of precision and recall.
  $$\text{F1}_i = 2 \cdot \frac{\text{Precision}_i \cdot \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}$$

These per-class metrics can be aggregated in several ways:

1. **Macro-averaging**: Calculate metrics for each class independently and then average them, giving equal weight to
   each class regardless of its frequency.
2. **Weighted averaging**: Calculate metrics for each class and average them weighted by the number of true instances
   for each class.
3. **Micro-averaging**: Aggregate the contributions of all classes to compute the average metric, effectively giving
   more weight to classes with more instances.

**Cohen's Kappa**

This metric measures agreement between the classifier and the true labels, corrected for agreement by chance:

$$\kappa = \frac{p_o - p_e}{1 - p_e}$$

where $p_o$ is the observed agreement ratio (accuracy) and $p_e$ is the expected agreement by chance.

**Area Under the ROC Curve (AUC-ROC)**

For multi-class problems, AUC-ROC can be calculated in a one-vs-rest manner for each class. It measures the ability of
the classifier to rank positive instances higher than negative ones.

**Log-Loss (Cross-Entropy)**

Log-loss evaluates the uncertainty of the model's predictions by heavily penalizing confident incorrect predictions:

$$\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{K} y_{ij} \log(p_{ij})$$

This metric is particularly useful when probabilistic outputs are important.

**Matthews Correlation Coefficient (MCC)**

MCC can be generalized to multi-class problems and provides a balanced measure even with imbalanced classes:

$$\text{MCC} = \frac{c \cdot s - \sum_{k} p_k \cdot t_k}{\sqrt{(s^2 - \sum_{k} p_k^2) \cdot (s^2 - \sum_{k} t_k^2)}}$$

where $c$ is the total number of correct predictions, $s$ is the total number of samples, $p_k$ is the number of times
class $k$ was predicted, and $t_k$ is the number of times class $k$ truly occurs.

**When to Use Different Metrics**

The choice of evaluation metric should depend on the specific problem context:

1. Use **accuracy** when classes are balanced and all types of errors are equally costly.
2. Use **precision** when false positives are more costly (e.g., spam detection).
3. Use **recall** when false negatives are more costly (e.g., disease detection).
4. Use **F1 score** when seeking a balance between precision and recall.
5. Use **log-loss** when the quality of probability estimates is important.
6. Use **MCC or Cohen's Kappa** when dealing with imbalanced datasets where a balanced measure is needed.

The best practice is to consider multiple metrics together to gain a comprehensive understanding of model performance
across different aspects of the classification task.

#### Logistic Regression

Logistic regression stands as one of the fundamental algorithms in machine learning, serving as a bridge between linear
models and more complex neural networks. Despite its name suggesting a regression technique, logistic regression is
primarily used for classification tasks, making it an essential tool in a data scientist's repertoire.

##### Mathematical Formulation

At its core, logistic regression extends linear regression by applying a non-linear transformation to its output,
constraining the result to represent a probability. The model begins with a linear combination of input features:

$$z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n = \mathbf{w}^T\mathbf{x} + b$$

Where:

- $\mathbf{w} = (w_1, w_2, ..., w_n)$ is the weight vector
- $\mathbf{x} = (x_1, x_2, ..., x_n)$ is the input feature vector
- $b = w_0$ is the bias term (often incorporated into $\mathbf{w}$ by adding a constant feature $x_0 = 1$)

This linear combination $z$ (sometimes called the "logit") is then transformed using the sigmoid function to produce a
value between 0 and 1:

$$\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(w^T\mathbf{x} + b)}}$$

The sigmoid function serves as the crucial component that transforms the unbounded linear prediction into a probability
value. The S-shaped curve of the sigmoid function asymptotically approaches 0 for large negative inputs and 1 for large
positive inputs, with a smooth transition in between.

The logistic regression model can be interpreted as modeling the log-odds (logit) of the positive class:

$$\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \log\left(\frac{P(y=1|\mathbf{x})}{1-P(y=1|\mathbf{x})}\right) = \mathbf{w}^T\mathbf{x} + b$$

This formulation reveals that logistic regression assumes a linear relationship not between the inputs and the
probability directly, but between the inputs and the log-odds of the positive class.

Another important aspect of the mathematical formulation is the decision boundary. For binary classification, the
decision boundary is the hypersurface where the predicted probability equals 0.5:

$$\sigma(z) = 0.5$$

Solving for $z$, we get:

$$z = 0$$

Which means:

$$\mathbf{w}^T\mathbf{x} + b = 0$$

This is the equation of a hyperplane in the feature space, indicating that logistic regression creates linear decision
boundaries between classes—a fundamental limitation of the model compared to more complex classifiers.

##### Probabilistic Interpretation

Logistic regression has a natural probabilistic interpretation that gives it strong theoretical foundations. The model
directly outputs the probability that an instance belongs to the positive class:

$$P(y=1|\mathbf{x}; \mathbf{w}, b) = \sigma(\mathbf{w}^T\mathbf{x} + b)$$

Consequently, the probability of the negative class is:

$$P(y=0|\mathbf{x}; \mathbf{w}, b) = 1 - P(y=1|\mathbf{x}; \mathbf{w}, b) = 1 - \sigma(\mathbf{w}^T\mathbf{x} + b)$$

These probabilities can be concisely written as:

$$P(y|\mathbf{x}; \mathbf{w}, b) = \sigma(\mathbf{w}^T\mathbf{x} + b)^y \cdot (1 - \sigma(\mathbf{w}^T\mathbf{x} + b))^{(1-y)}$$

This is recognizable as a Bernoulli distribution, where the probability parameter depends on the input features.

From a generative perspective, logistic regression can be derived by assuming that the data for each class comes from a
Gaussian distribution with the same covariance matrix but different means. Under this assumption, the posterior
probability follows the logistic form.

The probabilistic nature of logistic regression connects it to information theory and maximum likelihood estimation.
Training a logistic regression model typically involves finding the parameters $\mathbf{w}$ and $b$ that maximize the
likelihood of the observed data:

$$L(\mathbf{w}, b) = \prod_{i=1}^{m} P(y^{(i)}|\mathbf{x}^{(i)}; \mathbf{w}, b)$$

Taking the logarithm (which is monotonically increasing, thus preserving the maximum) gives the log-likelihood:

$$\ell(\mathbf{w}, b) = \sum_{i=1}^{m} \log P(y^{(i)}|\mathbf{x}^{(i)}; \mathbf{w}, b)$$

Substituting the Bernoulli probability:

$$\ell(\mathbf{w}, b) = \sum_{i=1}^{m} \left[ y^{(i)} \log \sigma(z^{(i)}) + (1 - y^{(i)}) \log (1 - \sigma(z^{(i)})) \right]$$

where $z^{(i)} = \mathbf{w}^T\mathbf{x}^{(i)} + b$.

For optimization purposes, we typically minimize the negative log-likelihood, which is equivalent to minimizing the
cross-entropy loss between the predicted probabilities and the true labels:

$$J(\mathbf{w}, b) = -\frac{1}{m}\ell(\mathbf{w}, b) = -\frac{1}{m}\sum_{i=1}^{m} \left[ y^{(i)} \log \sigma(z^{(i)}) + (1 - y^{(i)}) \log (1 - \sigma(z^{(i)})) \right]$$

This probabilistic interpretation not only provides a solid theoretical foundation but also connects logistic regression
to neural networks, where the same cross-entropy loss function is commonly used for classification tasks.

##### Binary Classification Implementation

Implementing logistic regression for binary classification involves several key steps: data preparation, model
initialization, training via optimization, and prediction. Let's examine each of these steps in detail.

**Data Preparation**:

1. **Feature scaling**: Logistic regression is sensitive to the scale of input features. Standardization (subtracting
   mean and dividing by standard deviation) or normalization (scaling to a fixed range) is often applied.
2. **Handling missing values**: Imputation strategies, such as replacing with mean, median, or most frequent values, are
   necessary as logistic regression doesn't handle missing values inherently.
3. **Feature engineering**: Creating interaction terms, polynomial features, or other transformations can improve model
   performance, especially when the relationship between features and the log-odds is not strictly linear.

**Model Initialization**:

The parameters $\mathbf{w}$ and $b$ are typically initialized to small random values or zeros. Initialization is less
critical for logistic regression than for deep neural networks since logistic regression has a convex cost function with
a single global minimum.

**Training Process**:

Logistic regression is trained by finding the parameters that minimize the cost function (negative log-likelihood or
cross-entropy). Several optimization algorithms can be used:

1. **Gradient Descent**: The most straightforward approach, updating parameters iteratively in the direction of the
   negative gradient:

    $$\mathbf{w} := \mathbf{w} - \alpha \nabla_{\mathbf{w}} J(\mathbf{w}, b)$$
    $$b := b - \alpha \nabla_{b} J(\mathbf{w}, b)$$

    Where $\alpha$ is the learning rate controlling the step size.

2. **Stochastic Gradient Descent (SGD)**: Updates parameters based on gradients computed from a single example at a
   time, providing faster but noisier updates.

3. **Mini-batch Gradient Descent**: A compromise between batch and stochastic approaches, using small random subsets of
   data for each update.

4. **Advanced Optimizers**: Methods like Newton's method, BFGS, or L-BFGS provide faster convergence by incorporating
   second-order information, though they require more computation per iteration.

5. **Regularized Training**: To prevent overfitting, regularization terms are often added to the cost function:

    - L1 regularization (Lasso): $J_{L1}(\mathbf{w}, b) = J(\mathbf{w}, b) + \lambda \sum_{j=1}^{n} |w_j|$
    - L2 regularization (Ridge): $J_{L2}(\mathbf{w}, b) = J(\mathbf{w}, b) + \lambda \sum_{j=1}^{n} w_j^2$
    - Elastic Net: A combination of L1 and L2

    These penalize large weights, with L1 promoting sparsity (feature selection) and L2 preventing any single feature
    from dominating.

**Making Predictions**:

Once trained, the model makes predictions by:

1. Computing the logit: $z = \mathbf{w}^T\mathbf{x} + b$

2. Applying the sigmoid function: $\hat{p} = \sigma(z)$

3. Converting to class labels based on a threshold (typically 0.5):

    $$\hat{y} = \begin{cases} 1 & \text{if } \hat{p} \geq threshold \ 0 & \text{otherwise} \end{cases}$$

The threshold can be adjusted based on the relative importance of precision versus recall for the specific application.

**Evaluation Metrics**:

Common metrics for evaluating binary classifiers include:

1. **Accuracy**: The proportion of correct predictions.
2. **Precision**: The ratio of true positives to all positive predictions.
3. **Recall**: The ratio of true positives to all actual positives.
4. **F1 Score**: The harmonic mean of precision and recall.
5. **ROC Curve and AUC**: Plots the true positive rate against the false positive rate at various thresholds, with AUC
   measuring the overall performance.
6. **Log-loss**: Directly measures how well the predicted probabilities match the true labels.

A practical implementation in Python using scikit-learn might look like:

```python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Prepare data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train model
model = LogisticRegression(C=1.0, penalty='l2', solver='lbfgs', max_iter=1000)
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)
report = classification_report(y_test, y_pred)
```

##### Multi-Class Extension

Logistic regression can be extended from binary classification to multi-class problems through several approaches. The
two most common methods are One-vs-Rest (OvR) and Multinomial Logistic Regression.

**1. One-vs-Rest (OvR) Approach**:

In the One-vs-Rest approach (also known as One-vs-All), we train $K$ separate binary logistic regression models, where
$K$ is the number of classes. Each model predicts whether an instance belongs to a specific class or not.

For each class $k \in {1, 2, ..., K}$, we train a binary logistic regression model:

$$P(y=k|\mathbf{x}; \mathbf{w}_k, b_k) = \sigma(\mathbf{w}_k^T\mathbf{x} + b_k)$$

Where $\mathbf{w}_k$ and $b_k$ are the parameters for the $k$-th model.

During prediction, we apply all $K$ models to the input and select the class with the highest probability:

$$\hat{y} = \arg\max_{k \in {1,2,...,K}} P(y=k|\mathbf{x}; \mathbf{w}_k, b_k)$$

Advantages of OvR include:

- Simplicity and direct extension from binary logistic regression
- Each classifier can be trained and updated independently
- Works well even when classes are imbalanced

Limitations include:

- Training $K$ separate models can be computationally expensive
- Does not directly model the joint probability distribution
- Class probabilities may not be directly comparable

**2. Multinomial Logistic Regression**:

Multinomial logistic regression (also known as Softmax Regression) is a direct extension that models the probability
distribution over all classes simultaneously. Instead of using the sigmoid function, it uses the softmax function to
convert logits into probabilities:

$$P(y=k|\mathbf{x}; \mathbf{W}, \mathbf{b}) = \frac{e^{\mathbf{w}*k^T\mathbf{x} + b_k}}{\sum*{j=1}^{K} e^{\mathbf{w}_j^T\mathbf{x} + b_j}}$$

Where:

- $\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, ..., \mathbf{w}_K]$ is the weight matrix
- $\mathbf{b} = [b_1, b_2, ..., b_K]$ is the bias vector

The model predicts the class with the highest probability:

$$\hat{y} = \arg\max_{k \in {1,2,...,K}} P(y=k|\mathbf{x}; \mathbf{W}, \mathbf{b})$$

Training involves minimizing the negative log-likelihood (cross-entropy loss):

$$J(\mathbf{W}, \mathbf{b}) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \mathbb{1}{y^{(i)} = k} \log P(y^{(i)}=k|\mathbf{x}^{(i)}; \mathbf{W}, \mathbf{b})$$

Where $\mathbb{1}{y^{(i)} = k}$ is an indicator function that equals 1 when $y^{(i)} = k$ and 0 otherwise.

Advantages of multinomial logistic regression include:

- Directly models the probability distribution over all classes
- Often provides better calibrated probabilities
- More efficient than training multiple binary classifiers

Limitations include:

- More complex to implement from scratch
- Training all parameters simultaneously can be computationally intensive
- May struggle with highly imbalanced classes

**Implementation Considerations**:

For both approaches, several practical considerations are important:

1. **Parameter sharing**: In multinomial logistic regression, the same features are used for all classes, but with
   different weights. This creates a more coherent model than OvR.
2. **Regularization**: L1 or L2 regularization can be applied to both approaches to prevent overfitting.
3. **Computational efficiency**: For large numbers of classes, OvR might be more efficient since each model can be
   trained independently and in parallel.
4. **Probability calibration**: Multinomial logistic regression typically provides better calibrated probabilities than
   OvR.

In practical implementations, scikit-learn provides both approaches:

```python
# One-vs-Rest
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

ovr_model = OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=1000))
ovr_model.fit(X_train, y_train)

# Multinomial
multi_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)
multi_model.fit(X_train, y_train)
```

##### Gradient Calculation Process

Understanding the gradient calculation process is essential for implementing logistic regression using gradient-based
optimization methods. This process involves deriving the gradients of the cost function with respect to the model
parameters.

Starting with the binary logistic regression cost function (cross-entropy loss):

$$J(\mathbf{w}, b) = -\frac{1}{m}\sum_{i=1}^{m} \left[ y^{(i)} \log \sigma(z^{(i)}) + (1 - y^{(i)}) \log (1 - \sigma(z^{(i)})) \right]$$

where $z^{(i)} = \mathbf{w}^T\mathbf{x}^{(i)} + b$ and $\sigma(z^{(i)}) = \frac{1}{1 + e^{-z^{(i)}}}$.

To calculate the gradient, we need to find $\nabla_{\mathbf{w}} J(\mathbf{w}, b)$ and $\nabla_{b} J(\mathbf{w}, b)$.

**Step 1: Calculate the Derivative of the Sigmoid Function**

The sigmoid function has an elegant derivative:

$$\sigma'(z) = \frac{d}{dz}\sigma(z) = \frac{d}{dz}\left(\frac{1}{1+e^{-z}}\right)$$

Using the quotient rule:

$$\sigma'(z) = \frac{e^{-z}}{(1+e^{-z})^2} = \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} = \sigma(z)(1 - \sigma(z))$$

This elegant form of the derivative will simplify our subsequent calculations.

**Step 2: Calculate the Partial Derivative with Respect to $z^{(i)}$**

Let's define $\hat{y}^{(i)} = \sigma(z^{(i)})$ for brevity. The partial derivative of the cost function with respect to
$z^{(i)}$ is:

$\frac{\partial}{\partial z^{(i)}} J(\mathbf{w}, b) = -\frac{1}{m}\frac{\partial}{\partial z^{(i)}} \left[ y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)}) \right]$

$= -\frac{1}{m}\left[ \frac{y^{(i)}}{\hat{y}^{(i)}}\frac{\partial \hat{y}^{(i)}}{\partial z^{(i)}} + \frac{1 - y^{(i)}}{1 - \hat{y}^{(i)}}\frac{\partial (1 - \hat{y}^{(i)})}{\partial z^{(i)}} \right]$

$= -\frac{1}{m}\left[ \frac{y^{(i)}}{\hat{y}^{(i)}}\hat{y}^{(i)}(1 - \hat{y}^{(i)}) - \frac{1 - y^{(i)}}{1 - \hat{y}^{(i)}}\hat{y}^{(i)}(1 - \hat{y}^{(i)}) \right]$

$= -\frac{1}{m}\left[ y^{(i)}(1 - \hat{y}^{(i)}) - (1 - y^{(i)})\hat{y}^{(i)} \right]$

$= -\frac{1}{m}\left[ y^{(i)} - y^{(i)}\hat{y}^{(i)} - \hat{y}^{(i)} + y^{(i)}\hat{y}^{(i)} \right]$

$= -\frac{1}{m}\left[ y^{(i)} - \hat{y}^{(i)} \right]$

$= \frac{1}{m}(\hat{y}^{(i)} - y^{(i)})$

This is a remarkably simple form: the partial derivative with respect to $z^{(i)}$ is proportional to the prediction
error ($\hat{y}^{(i)} - y^{(i)}$).

**Step 3: Calculate the Gradients with Respect to Parameters**

Using the chain rule, we can now calculate the gradients with respect to each weight $w_j$ and the bias $b$:

$$\begin{align*} \frac{\partial J}{\partial w_j} &= \sum_{i=1}^{m} \frac{\partial J}{\partial z^{(i)}} \frac{\partial z^{(i)}}{\partial w_j} \ &= \sum_{i=1}^{m} \frac{1}{m}(\hat{y}^{(i)} - y^{(i)}) \cdot x_j^{(i)} \ &= \frac{1}{m}\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})x_j^{(i)} \end{align*}$$

Similarly for the bias:

$$\begin{align*} \frac{\partial J}{\partial b} &= \sum_{i=1}^{m} \frac{\partial J}{\partial z^{(i)}} \frac{\partial z^{(i)}}{\partial b} \ &= \sum_{i=1}^{m} \frac{1}{m}(\hat{y}^{(i)} - y^{(i)}) \cdot 1 \ &= \frac{1}{m}\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) \end{align*}$$

These gradient expressions have intuitive interpretations:

- The gradient for each weight $w_j$ is the average of the prediction errors, weighted by the corresponding feature
  value $x_j^{(i)}$.
- The gradient for the bias $b$ is simply the average of the prediction errors.

In vector form, the gradient with respect to the weight vector can be written as:

$$\nabla_{\mathbf{w}} J(\mathbf{w}, b) = \frac{1}{m}\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})\mathbf{x}^{(i)}$$

**Gradient Descent Update Rules**:

Using these gradients, the parameter update rules for gradient descent are:

$$\mathbf{w} := \mathbf{w} - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})\mathbf{x}^{(i)}$$

$$b := b - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})$$

Where $\alpha$ is the learning rate.

For stochastic gradient descent, we update using a single example at a time:

$$\mathbf{w} := \mathbf{w} - \alpha \cdot (\hat{y}^{(i)} - y^{(i)})\mathbf{x}^{(i)}$$

$$b := b - \alpha \cdot (\hat{y}^{(i)} - y^{(i)})$$

For mini-batch gradient descent, we average over a mini-batch of $B$ examples.

**Regularized Gradient Calculation**:

When using regularization, the gradient calculation needs to include the derivative of the regularization term. For L2
regularization:

$$J_{L2}(\mathbf{w}, b) = J(\mathbf{w}, b) + \frac{\lambda}{2m}\sum_{j=1}^{n} w_j^2$$

The gradient becomes:

$$\nabla_{w_j} J_{L2}(\mathbf{w}, b) = \frac{1}{m}\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}w_j$$

For L1 regularization:

$$J_{L1}(\mathbf{w}, b) = J(\mathbf{w}, b) + \frac{\lambda}{m}\sum_{j=1}^{n} |w_j|$$

The gradient includes the subgradient of the absolute value function:

$$\nabla_{w_j} J_{L1}(\mathbf{w}, b) = \frac{1}{m}\sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\text{sign}(w_j)$$

Note that the bias term $b$ is typically not regularized.

**Multi-class Extension**:

For multinomial logistic regression, the gradients are calculated similarly but using the softmax function. The partial
derivative with respect to the logit for class $k$ becomes:

$$\frac{\partial J}{\partial z_k^{(i)}} = \frac{1}{m}(p_k^{(i)} - \mathbb{1}{y^{(i)} = k})$$

Where $p_k^{(i)}$ is the predicted probability for class $k$ and $\mathbb{1}{y^{(i)} = k}$ is an indicator function.

This efficient gradient calculation process enables the effective training of logistic regression models using various
optimization algorithms, forming the foundation for more complex neural network architectures that build upon these same
principles.
