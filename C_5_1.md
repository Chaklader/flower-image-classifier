# C-5: Implementation and Applications

1. **Building Transformer Models in PyTorch**
    - Model Components Implementation
    - Attention Block Coding
    - Feedforward Networks
    - Layer Normalization
    - Training and Evaluation Process
2. **Decoder-only Architecture**
    - GPT-style Models
    - Causal Masking Implementation
    - Autoregressive Property
    - Generation Strategies
    - Applications and Limitations
3. **Using Pre-trained Models with Hugging Face**
    - Pipeline API Usage
    - Model Selection Guidelines
    - Text Generation Parameters
    - Fine-tuning Pre-trained Models
    - Multi-modal Applications

---

#### Building Transformer Models

##### Understanding the Transformer Revolution

Imagine you're trying to understand a conversation where multiple people are speaking simultaneously, and you need to
figure out not just what each person is saying, but how their words relate to everyone else's. This is essentially the
challenge that transformers solve in natural language processing. Unlike previous approaches that processed text
sequentially (like reading word by word), transformers can look at all words simultaneously and understand their
relationships, much like how you might grasp the overall meaning of a sentence at a glance rather than parsing it word
by word.

The transformer architecture represents one of the most significant breakthroughs in deep learning, fundamentally
changing how machines process and understand language. To truly appreciate this innovation, let's build our
understanding from the ground up, starting with the key insight: **attention is all you need**.

##### Model Components Implementation

###### The Architecture Blueprint

A transformer model is like a sophisticated assembly line for processing language. At its core, it consists of several
key components that work in harmony:

1. **Input Embeddings**: Convert discrete tokens (words or subwords) into continuous vector representations
2. **Positional Encodings**: Add information about word order (since transformers process all positions simultaneously)
3. **Attention Mechanisms**: Allow the model to focus on relevant parts of the input
4. **Feed-forward Networks**: Process information at each position independently
5. **Layer Normalization**: Stabilize the learning process
6. **Output Projections**: Convert internal representations back to predictions

Let's understand each component in detail:

###### Configuration Parameters

Before building a transformer, we need to define its architecture through configuration parameters. Think of these as
the blueprint specifications:

- **Vocabulary Size**: The total number of unique tokens the model can recognize (typically 30,000-50,000 for most
  languages)
- **Hidden Size (d_model)**: The dimensionality of the model's internal representations (commonly 768 for BERT-base,
  1024 for BERT-large)
- **Number of Attention Heads**: How many different "perspectives" the model uses to look at the input (typically 12
  or 16)
- **Number of Layers**: The depth of the network (12 for BERT-base, 24 for BERT-large, up to 96 for GPT-3)
- **Intermediate Size**: The dimensionality of the feed-forward network's hidden layer (typically 4× the hidden size)

These parameters create a delicate balance: larger values generally improve performance but increase computational
requirements exponentially.

###### The Embedding Layer: From Words to Vectors

The journey of understanding begins with embeddings. Consider the word "cat" – to a computer, this is just a sequence of
characters. The embedding layer transforms it into a dense vector of numbers that captures its meaning in a
high-dimensional space.

**Word Embeddings**: Each token in our vocabulary is assigned a unique vector of size `hidden_size`. Initially random,
these vectors are learned during training to capture semantic relationships. For example:

- Similar words (cat, kitten) will have similar vectors
- The vector arithmetic can capture analogies: king - man + woman ≈ queen

**Positional Embeddings**: Since transformers process all positions simultaneously, they lose the natural ordering of
words. Positional embeddings solve this by adding position-specific patterns to each word embedding. For a sequence of
length $n$, we create position embeddings for positions $0, 1, 2, ..., n-1$.

The final embedding for a token at position $i$ is:
$$\text{embedding}_i = \text{word embedding}_i + \text{position embedding}_i$$

**Numerical Example**: Let's say we have a simple 3-word sentence "The cat sleeps" with a hidden size of 4:

- Word embedding for "cat": $[0.2, -0.5, 0.8, 0.1]$
- Position embedding for position 1: $[0.1, 0.0, -0.1, 0.2]$
- Combined embedding: $[0.3, -0.5, 0.7, 0.3]$

This combined representation now contains both semantic information (what the word means) and positional information
(where it appears in the sentence).

##### Attention Block Coding

###### The Heart of the Transformer: Multi-Head Attention

Attention is the transformer's superpower – the ability to dynamically focus on relevant parts of the input when
processing each element. To understand this, imagine reading a complex sentence: "The scientist who discovered
penicillin in 1928 revolutionized medicine." When processing "revolutionized," your brain naturally connects it to
"scientist" despite the intervening words. This is what attention mechanisms achieve mathematically.

###### Scaled Dot-Product Attention

The fundamental operation in attention is computing how much each word should "attend to" every other word. This is done
through three transformations:

1. **Query (Q)**: "What am I looking for?"
2. **Key (K)**: "What do I have to offer?"
3. **Value (V)**: "What information do I contain?"

The attention formula is: $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Let's break this down step by step:

**Step 1: Compute Attention Scores** $$\text{scores} = QK^T$$

This matrix multiplication computes how well each query matches each key. Higher scores indicate stronger relationships.

**Step 2: Scale the Scores** $$\text{scaled scores} = \frac{\text{scores}}{\sqrt{d_k}}$$

We divide by $\sqrt{d_k}$ (where $d_k$ is the dimension of the keys) to prevent the scores from becoming too large,
which would cause the softmax to saturate.

**Step 3: Apply Softmax** $$\text{attention weights} = \text{softmax}(\text{scaled scores})$$

This converts scores to probabilities that sum to 1 for each query.

**Step 4: Weighted Sum of Values** $$\text{output} = \text{attention weights} \cdot V$$

The final output is a weighted combination of all values, where weights represent the attention each position pays to
others.

**Numerical Example**: Consider a simplified 3-token sequence with dimension 2:

Queries $Q$: $$Q = \begin{bmatrix} 1 & 0 \ 0 & 1 \ 1 & 1 \end{bmatrix}$$

Keys $K$: $$K = \begin{bmatrix} 1 & 0 \ 0 & 1 \ 1 & 1 \end{bmatrix}$$

Values $V$: $$V = \begin{bmatrix} 1 & 2 \ 3 & 4 \ 5 & 6 \end{bmatrix}$$

Step 1: $QK^T = \begin{bmatrix} 1 & 0 & 1 \ 0 & 1 & 1 \ 1 & 1 & 2 \end{bmatrix}$

Step 2: Scale by $\sqrt{2}$: $\begin{bmatrix} 0.71 & 0 & 0.71 \ 0 & 0.71 & 0.71 \ 0.71 & 0.71 & 1.41 \end{bmatrix}$

Step 3: Apply softmax (row-wise):
$\begin{bmatrix} 0.38 & 0.24 & 0.38 \ 0.24 & 0.38 & 0.38 \ 0.24 & 0.24 & 0.52 \end{bmatrix}$

Step 4: Multiply by values to get final attention output.

###### Multi-Head Attention: Multiple Perspectives

Instead of using a single attention function, transformers use multiple "attention heads" that can focus on different
types of relationships. Think of it like having multiple experts, each looking for different patterns:

- One head might focus on syntactic relationships (subject-verb agreement)
- Another might capture semantic dependencies (word meanings)
- Yet another might track coreference (pronouns and their antecedents)

For $h$ heads with model dimension $d_{model}$:

- Each head has dimension $d_k = d_{model} / h$
- Separate $Q$, $K$, $V$ projections for each head
- Outputs are concatenated and projected back to $d_{model}$

The multi-head attention formula: $$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

Where each head is: $$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

###### Residual Connections and Layer Normalization

After the attention computation, two crucial operations ensure stable training:

1. **Residual Connection**: Add the input back to the output $$\text{output} = \text{input} + \text{AttentionOutput}$$

    This helps with gradient flow and allows the network to learn incremental changes.

2. **Layer Normalization**: Normalize across the feature dimension
   $$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

    Where $\mu$ and $\sigma$ are the mean and standard deviation computed across the hidden dimension for each position.

##### Feedforward Networks

###### The Processing Power

After attention determines what information to focus on, the feed-forward network (FFN) processes this information at
each position independently. Think of it as the "thinking" step after the "looking" step of attention.

The FFN consists of two linear transformations with a non-linear activation in between:

$$\text{FFN}(x) = W_2 \cdot \text{activation}(W_1 \cdot x + b_1) + b_2$$

Where:

- $W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}$ expands the dimension (typically $d_{ff} = 4 \cdot d_{model}$)
- $W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$ projects back to model dimension
- The activation function is typically GELU or ReLU

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_01.png" width="600" height="auto"> <p style="color: #555;">Figure: Feedforward Block</p> </div>

###### Why the Expansion?

The expansion to a larger intermediate dimension ($d_{ff} = 4 \cdot d_{model}$) gives the network more capacity to learn
complex patterns. It's like temporarily giving the model more "workspace" to process information before compressing it
back to the original size.

**Numerical Example**: With $d_{model} = 768$ and $d_{ff} = 3072$:

- Input: vector of size 768
- After $W_1$: expanded to size 3072
- After activation: non-linear transformation applied
- After $W_2$: compressed back to size 768

This expansion-compression pattern allows the model to learn more complex transformations than a single linear layer
would permit.

##### Layer Normalization

###### Stabilizing the Learning Process

Layer normalization is crucial for training deep networks effectively. Unlike batch normalization (which normalizes
across the batch dimension), layer normalization operates on each example independently, normalizing across the feature
dimension.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_02.png" width="600" height="auto"> <p style="color: #555;">Figure: Layer Normalization</p> </div>

The formula: $$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

Where for each position in the sequence:

- $\mu = \frac{1}{d} \sum_{i=1}^{d} x_i$ (mean across hidden dimension)
- $\sigma^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2$ (variance across hidden dimension)
- $\gamma$ and $\beta$ are learned parameters for scaling and shifting
- $\epsilon$ is a small constant (typically $10^{-12}$) for numerical stability

**Numerical Example**: Consider a hidden state vector: $x = [2, 4, 6, 8]$

- Mean: $\mu = (2+4+6+8)/4 = 5$
- Variance: $\sigma^2 = ((2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2)/4 = 5$
- Standard deviation: $\sigma = \sqrt{5} \approx 2.24$
- Normalized: $x_{norm} = [(2-5)/2.24, (4-5)/2.24, (6-5)/2.24, (8-5)/2.24]$
- Result: $x_{norm} \approx [-1.34, -0.45, 0.45, 1.34]$

With learned $\gamma = 1$ and $\beta = 0$, the output equals the normalized values.

###### Pre-Norm vs Post-Norm

There are two common patterns for applying layer normalization:

**Post-Norm** (Original Transformer): $$\text{output} = \text{LayerNorm}(\text{input} + \text{Sublayer}(\text{input}))$$

**Pre-Norm** (Modern Transformers): $$\text{output} = \text{input} + \text{Sublayer}(\text{LayerNorm}(\text{input}))$$

Pre-norm has become more popular because:

- It provides better gradient flow in very deep networks
- It stabilizes training, especially for large models
- It often converges faster

##### Training and Evaluation Process

###### The Learning Journey

Training a transformer involves several key considerations that go beyond standard neural network training:

**1. Learning Rate Scheduling**

Transformers benefit from a specific learning rate schedule with warmup:

- Start with a very low learning rate
- Linearly increase for the first few thousand steps (warmup)
- Then decrease, often following an inverse square root schedule

The formula for the original transformer schedule:
$$lr = d_{model}^{-0.5} \cdot \min(\text{step}^{-0.5}, \text{step} \cdot \text{warmup steps}^{-1.5})$$

This schedule helps because:

- Initial warmup prevents large gradient updates when parameters are random
- Gradual decay helps fine-tune the model as training progresses

**2. Gradient Clipping**

Due to the complex interactions in transformers, gradients can occasionally explode. Gradient clipping prevents this by
scaling down gradients that exceed a threshold:

$$g' = \begin{cases} g & \text{if } ||g|| \leq \text{threshold} \\ \frac{\text{threshold}}{||g||} \cdot g & \text{otherwise} \end{cases}$$

Typical threshold values range from 1.0 to 5.0.

**3. Loss Calculation for Language Modeling**

For autoregressive language modeling, we use cross-entropy loss: $$\mathcal{L} = -\sum_{t=1}^{T} \log P(x_t | x_{<t})$$

Where $P(x_t | x_{<t})$ is the model's predicted probability for the correct token at position $t$.

**4. Perplexity as an Evaluation Metric**

Perplexity measures how well a model predicts text:
$$\text{Perplexity} = \exp\left(\frac{1}{N} \sum_{i=1}^{N} -\log P(x_i | x_{<i})\right)$$

Lower perplexity indicates better performance. A perplexity of 20 means the model is as confused as if it were choosing
uniformly among 20 alternatives at each step.

**5. Computational Considerations**

Training transformers requires careful resource management:

- **Memory**: Attention has $O(n^2)$ memory complexity for sequence length $n$
- **Computation**: Each layer requires $O(n^2 \cdot d + n \cdot d^2)$ operations
- **Gradient Accumulation**: For large models that don't fit in memory, accumulate gradients over multiple smaller
  batches

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_04.png" width="600" height="auto"> <p style="color: #555;">Figure: Encoder-Decoder Architecture</p> </div>

#### Decoder-only Architecture

##### The Evolution to Simplicity

While the original transformer used both encoder and decoder components, researchers discovered that using only the
decoder portion could achieve remarkable results for many language tasks. This insight led to the development of GPT
(Generative Pre-trained Transformer) and similar models that have revolutionized natural language processing.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_05.png" width="600" height="auto"> <p style="color: #555;">Figure: Encoder-Decoder Architecture</p> </div>

The key insight is elegant: by training a model to predict the next word given all previous words, we can create a
system that naturally learns to understand and generate language. This autoregressive approach mirrors how humans
produce text – one word at a time, with each choice influenced by what came before.

##### GPT-style Models

###### The Architectural Philosophy

GPT-style models embrace radical simplification while maintaining the power of the transformer architecture. Instead of
separate encoding and decoding stages, these models use a unified stack of transformer decoder blocks, modified to work
without encoder input.

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_06.png" width="600" height="auto"> <p style="color: #555;">Figure: Other Model Architectures</p> </div>

The core design principles that make GPT models powerful:

**1. Unidirectional (Causal) Attention**

Unlike bidirectional models (like BERT) that can look at the entire context, GPT models use causal attention where each
position can only attend to previous positions. This constraint is essential for generation – you can't look at words
that haven't been generated yet!

The attention mask ensures position $i$ can only attend to positions $j \leq i$:
$$\text{Mask}_{ij} = \begin{cases} 0 & \text{if } j \leq i \ -\infty & \text{if } j > i \end{cases}$$

**2. Deep Architecture**

GPT models stack many transformer layers to build increasingly abstract representations:

- GPT-1: 12 layers
- GPT-2: Up to 48 layers
- GPT-3: Up to 96 layers
- GPT-4: Estimated 100+ layers

Each layer refines the representation, with lower layers capturing syntax and grammar, middle layers understanding
semantics, and upper layers handling complex reasoning.

**3. The Language Modeling Objective**

The training objective is deceptively simple – predict the next token:
$$\mathcal{L}(\theta) = -\sum_{i=1}^{n} \log P_\theta(x_i | x_1, x_2, ..., x_{i-1})$$

This objective forces the model to learn:

- Syntax (grammatical structure)
- Semantics (word meanings and relationships)
- World knowledge (facts and common sense)
- Reasoning patterns (logical connections)

**4. Scale as a Key Ingredient**

The GPT family demonstrates that scale dramatically improves capabilities:

| Model | Parameters | Training Tokens | Emergent Capabilities                        |
| ----- | ---------- | --------------- | -------------------------------------------- |
| GPT-1 | 117M       | ~5B             | Basic text completion                        |
| GPT-2 | 1.5B       | ~10B            | Coherent paragraph generation                |
| GPT-3 | 175B       | ~300B           | Few-shot learning, complex reasoning         |
| GPT-4 | ~1T+       | ~10T+           | Multimodal understanding, advanced reasoning |

The relationship between scale and capability appears to follow power laws, with new abilities emerging at certain scale
thresholds.

##### Causal Masking Implementation

###### The Mathematics of Looking Backward

Causal masking is the mechanism that enforces the autoregressive property in decoder-only transformers. It prevents each
position from "seeing into the future" – a critical constraint for valid text generation.

Consider a sequence of 5 tokens. The causal mask looks like:

$$\text{Mask} = \begin{bmatrix} 0 & -\infty & -\infty & -\infty & -\infty \ 0 & 0 & -\infty & -\infty & -\infty \ 0 & 0 & 0 & -\infty & -\infty \ 0 & 0 & 0 & 0 & -\infty \ 0 & 0 & 0 & 0 & 0 \end{bmatrix}$$

When added to attention scores before softmax, the $-\infty$ values become 0 after softmax, effectively blocking
attention to future positions.

**How It Works in Practice**:

1. **During Training**: All positions are processed in parallel, but each position only attends to previous positions
2. **During Generation**: Tokens are generated one at a time, naturally respecting causality

**Numerical Example**: For the sentence "The cat sits", when processing "sits":

- Can attend to: "The" (position 0), "cat" (position 1), "sits" (position 2)
- Cannot attend to: Any future positions (which don't exist yet during generation)

The attention scores before masking might be: $$\text{Scores} = \begin{bmatrix} 2.1 & 1.5 & 3.2 & 0.8 \end{bmatrix}$$

After applying causal mask for position 2:
$$\text{Masked Scores} = \begin{bmatrix} 2.1 & 1.5 & 3.2 & -\infty \end{bmatrix}$$

After softmax: $$\text{Attention Weights} = \begin{bmatrix} 0.25 & 0.15 & 0.60 & 0.00 \end{bmatrix}$$

##### Autoregressive Property

###### The Chain Rule of Language

The autoregressive property is the mathematical foundation that allows decoder-only models to generate coherent text.
It's based on the chain rule of probability, decomposing the joint probability of a sequence into a product of
conditional probabilities.

For a sequence $x = (x_1, x_2, ..., x_n)$:
$$P(x) = P(x_1) \cdot P(x_2|x_1) \cdot P(x_3|x_1, x_2) \cdot ... \cdot P(x_n|x_1, ..., x_{n-1})$$

This can be written more compactly as: $$P(x) = \prod_{i=1}^{n} P(x_i | x_{<i})$$

**Why This Matters**:

1. **Natural Language Structure**: Language is inherently sequential – each word depends on previous context
2. **Generation Capability**: By modeling conditional probabilities, we can generate text by sampling from these
   distributions
3. **Flexibility**: The same model can be used for various tasks by framing them as conditional generation

**The Generation Process**:

Starting with a prompt $x_{1:k}$, generate the next token by:

1. Compute $P(x_{k+1} | x_{1:k})$ for all possible tokens
2. Sample from this distribution (or select the maximum)
3. Append the selected token to the sequence
4. Repeat until reaching a stop condition

**Mathematical Properties**:

The log-likelihood of a sequence under the model: $$\log P(x) = \sum_{i=1}^{n} \log P(x_i | x_{<i})$$

This additive form is convenient for:

- Gradient computation during training
- Comparing different sequences (higher log-likelihood = more probable)
- Computing perplexity: $\text{PPL} = \exp(-\frac{1}{n} \log P(x))$

**Challenges of Autoregression**:

1. **Error Accumulation**: Mistakes compound as generation proceeds
2. **Exposure Bias**: Training sees perfect context, but generation uses model predictions
3. **Length Bias**: Tendency toward shorter or longer sequences depending on training

##### Generation Strategies

###### The Art of Sampling from Language Models

Once trained, a decoder-only model provides probability distributions over the vocabulary at each step. How we sample
from these distributions dramatically affects the generated text's quality, creativity, and coherence.

**1. Greedy Decoding**

The simplest approach: always select the highest probability token.

$$x_{t+1} = \arg\max_{x} P(x | x_{1:t})$$

Advantages:

- Deterministic and reproducible
- Fast computation
- Good for tasks requiring accuracy

Disadvantages:

- Repetitive and predictable output
- Can get stuck in loops
- Misses potentially better sequences

**2. Beam Search**

Maintain $k$ most probable sequences (beams) at each step:

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_07.png" width="600" height="auto"> <p style="color: #555;">Figure: Beam Search</p> </div>

At each step:

1. Expand each beam with top-$k$ next tokens
2. Keep only the $k$ highest-scoring sequences overall
3. Continue until all beams reach end tokens

The score of a sequence: $$\text{Score}(x_{1:n}) = \sum_{i=1}^{n} \log P(x_i | x_{<i})$$

Often normalized by length to avoid bias toward shorter sequences:
$$\text{Normalized Score} = \frac{1}{n^\alpha} \sum_{i=1}^{n} \log P(x_i | x_{<i})$$

Where $\alpha \in [0.6, 0.7]$ is the length penalty factor.

**3. Temperature Sampling**

Modify the probability distribution's sharpness before sampling:

$$P'(x_i) = \frac{\exp(\log P(x_i) / T)}{\sum_j \exp(\log P(x_j) / T)}$$

<div align="center"> <img src="images/AI_Programming_with_Python_ND_P2_C_5_03.png" width="600" height="auto"> <p style="color: #555;">Figure: Temperature Parameter</p> </div>

Where $T$ is the temperature:

- $T < 1$: Sharper distribution (more focused)
- $T = 1$: Original distribution
- $T > 1$: Flatter distribution (more random)

**Numerical Example**: Original probabilities: [0.7, 0.2, 0.1]

With $T = 0.5$ (focused):

- Logits: [-0.357, -1.609, -2.303]
- Scaled: [-0.714, -3.218, -4.606]
- New probabilities: [0.89, 0.08, 0.02]

With $T = 2.0$ (creative):

- Scaled: [-0.179, -0.805, -1.152]
- New probabilities: [0.48, 0.31, 0.21]

**4. Top-k Sampling**

Restrict sampling to the $k$ most likely tokens:

1. Sort tokens by probability
2. Keep only top $k$
3. Renormalize probabilities
4. Sample from this reduced distribution

This prevents sampling very unlikely tokens while maintaining diversity.

**5. Nucleus (Top-p) Sampling**

Dynamically select the smallest set of tokens whose cumulative probability exceeds threshold $p$:

1. Sort tokens by probability (descending)
2. Calculate cumulative sum
3. Keep tokens until cumulative probability > $p$
4. Renormalize and sample

**Example with $p = 0.9$**:

- Token probabilities: [0.4, 0.3, 0.15, 0.08, 0.04, 0.03]
- Cumulative: [0.4, 0.7, 0.85, 0.93, 0.97, 1.0]
- Keep first 4 tokens (cumulative = 0.93 > 0.9)
- Renormalize: [0.43, 0.32, 0.16, 0.09]

**6. Repetition Control**

Several techniques prevent repetitive text:

**Repetition Penalty**: Reduce probability of previously used tokens by factor $\rho$:
$$P'(x_i) = \begin{cases} P(x_i) / \rho & \text{if } x_i \in \text{previous tokens} \ P(x_i) & \text{otherwise} \end{cases}$$

**N-gram Blocking**: Forbid exact repetition of n-grams:

- Track all n-grams in generated text
- Set probability to 0 for tokens that would create repeated n-grams

**Comparison of Strategies**:

| Strategy        | Coherence | Diversity | Use Case          |
| --------------- | --------- | --------- | ----------------- |
| Greedy          | High      | Low       | Factual Q&A       |
| Beam Search     | High      | Low       | Translation       |
| Temperature=0.7 | Good      | Moderate  | General text      |
| Top-k=40        | Good      | Good      | Creative writing  |
| Top-p=0.9       | Good      | Adaptive  | Most applications |

##### Applications and Limitations

###### Where Decoder-Only Models Excel

**Applications**:

1. **Text Generation**: From creative writing to technical documentation
2. **Code Generation**: Understanding programming patterns and generating functional code
3. **Conversational AI**: Maintaining context over long dialogues
4. **Few-Shot Learning**: Adapting to new tasks with just a few examples
5. **Reasoning Tasks**: Chain-of-thought prompting for complex problem-solving

The versatility comes from the models' ability to:

- Capture long-range dependencies
- Generate coherent text at multiple scales
- Adapt to various domains through prompting

**Fundamental Limitations**:

1. **Hallucination Problem**

Models can generate plausible-sounding but false information. This occurs because:

- Training objective rewards likely text, not truthful text
- Models interpolate between training examples
- No explicit fact-checking mechanism

Mitigation strategies:

- Retrieval-augmented generation (RAG)
- Fine-tuning on curated factual data
- Post-generation fact-checking

1. **Context Length Constraints**

All transformers have finite context windows:

- Attention complexity: $O(n^2)$ for sequence length $n$
- Memory requirements grow quadratically
- Information at the beginning of long contexts may be "forgotten"

Recent innovations (sparse attention, linear attention) partially address this.

1. **Reasoning Limitations**

While capable of impressive reasoning, models struggle with:

- Multi-step mathematical proofs
- Consistent logical deduction
- Causal reasoning
- Temporal reasoning

Chain-of-thought prompting helps but doesn't fully solve these issues.

1. **Computational Requirements**

Large models require substantial resources:

- GPT-3 (175B parameters): ~350GB of memory for inference
- Training costs: Millions of dollars for large models
- Inference latency: Proportional to model size and sequence length

This limits accessibility and raises environmental concerns.

1. **Bias and Safety Concerns**

Models inherit biases from training data:

- Social biases (gender, race, culture)
- Geographical biases (overrepresentation of certain regions)
- Linguistic biases (better performance on
